{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 3.2.4. Multiple Outputs\n",
    "\n",
    "Suppose we have\n",
    "* multiple outputs $Y_1,Y_2,\\cdots,Y_K$\n",
    "* inputs $X_0,X_1,\\cdots,X_p$\n",
    "* a linear model for each output  \n",
    "\\begin{align}\n",
    "Y_k &= \\beta_{0k} + \\sum_{j=1}^p X_j\\beta_{jk} + \\epsilon_k \\\\\n",
    "&= f_k(X) + \\epsilon_k\n",
    "\\end{align}\n",
    "\n",
    "> the coefficients for the $k$th outcome are just the least squares estimates in the regression of $y_k$ on $x_0,x_1,\\cdots,x_p$ . Multiple outputs do not affect one anotherâ€™s least squares estimates.\n",
    "\n",
    "With $N$ training cases we can write the model in matrix notation\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Y}=\\mathbf{XB}+\\mathbf{E},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "* $\\mathbf{Y}$ is $N\\times K$ with $ik$ entry $y_{ik}$,\n",
    "* $\\mathbf{X}$ is $N\\times(p+1)$ input matrix,\n",
    "* $\\mathbf{B}$ is $(p+1)\\times K$ parameter matrix,\n",
    "* $\\mathbf{E}$ is $N\\times K$ error matrix.\n",
    "\n",
    "A straightforward generalization of the univariate loss function is\n",
    "\n",
    "\\begin{align}\n",
    "\\text{RSS}(\\mathbf{B}) &= \\sum_{k=1}^K \\sum_{i=1}^N \\left( y_{ik} - f_k(x_i) \\right)^2 \\\\\n",
    "&= \\text{trace}\\left( (\\mathbf{Y}-\\mathbf{XB})^T(\\mathbf{Y}-\\mathbf{XB}) \\right)\n",
    "\\end{align}\n",
    "\n",
    "The least squares estimates have exactly the same form as before\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{B}} = \\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{Y}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlated errors\n",
    "\n",
    "If the errors $\\epsilon = (\\epsilon_1,\\cdots,\\epsilon_K)$ are correlated with $\\text{Cov}(\\epsilon)=\\mathbf{\\Sigma}$, then the multivariate weighted criterion\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{RSS}(\\mathbf{B};\\mathbf{\\Sigma}) = \\sum_{i=1}^N (y_i-f(x_i))^T \\mathbf{\\Sigma}^{-1} (y_i-f(x_i))\n",
    "\\end{equation}\n",
    "\n",
    "arises naturally from multivariate Gaussian theory. Here\n",
    "* $f(x) = \\left(f_1(x),\\cdots,f_K(x)\\right)^T$ is the vector function,\n",
    "* $y_i$ the vector of $K$ responses for observation $i$.\n",
    "However, the solution is again the same with ignoring the correlations as\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{B}} = \\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{Y}.\n",
    "\\end{equation}\n",
    "\n",
    "In Section 3.7 we pursue the multiple output problem and consider situations where it does pay to combine the regressions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
