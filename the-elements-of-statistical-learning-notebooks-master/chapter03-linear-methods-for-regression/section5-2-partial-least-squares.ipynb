{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 3.5.2. Partial Least Squares\n",
    "\n",
    "Unlike PCR, partial least squares (PLS) uses $\\mathbf{y}$ (in addition to $\\mathbf{X}$) for the construction for a set of linear combinations of the inputs.\n",
    "\n",
    "PLS is not scale invariant like PCR, so we assume that each $\\mathbf{x}_j$ is standardized to have mean $0$ and variance $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm 3.3. Partial least squares.\n",
    "\n",
    "1. Standardized each $\\mathbf{x}_j$ to have mean $0$ and variance $1$.  \n",
    "Set\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{y}}^{(0)} &= \\bar{y}\\mathbf{1} \\\\\n",
    "\\mathbf{x}_j^{(0)} &= \\mathbf{x}_j, \\text{ for } j=1,\\cdots,p.\n",
    "\\end{align}\n",
    "\n",
    "2. For $m = 1,2,\\cdots,p$\n",
    "  * $\\mathbf{z}_m = \\sum_{j=1}^p \\hat\\rho_{mj}\\mathbf{x}_j^{(m-1)}$, where $\\hat\\rho_{mj} = \\langle \\mathbf{x}_j^{(m-1)},\\mathbf{y}\\rangle$.\n",
    "  * $\\hat\\theta_m = \\langle\\mathbf{z}_m,\\mathbf{y}\\rangle \\big/ \\langle\\mathbf{z}_m,\\mathbf{z}_m\\rangle$.\n",
    "  * $\\hat{\\mathbf{y}}^{(m)} = \\hat{\\mathbf{y}}^{(m-1)} + \\hat\\theta_m \\mathbf{z}_m$.\n",
    "  * Orthogonalize each $\\mathbf{x}_j^{(m-1)}$ w.r.t. $\\mathbf{z}_m$:  \n",
    "  $\\mathbf{x}_j^{(m)} = \\mathbf{x}_j^{(m-1)} - \\frac{\\langle\\mathbf{z}_m,\\mathbf{x}_j^{(m-1)}\\rangle}{\\langle\\mathbf{z}_m,\\mathbf{y}\\rangle}\\mathbf{z}_m, \\text{ for } j=1,2,\\cdots,p$.\n",
    "\n",
    "3. Output the sequence of fitted vectors $\\left\\lbrace \\hat{\\mathbf{y}}^{(m)}\\right\\rbrace_1^p$.  \n",
    "Since the $\\left\\lbrace \\mathbf{z}_l \\right\\rbrace_1^m$ are linear in the original $\\mathbf{x}_j$, so is  \n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{y}}^{(m)} = \\mathbf{X}\\hat\\beta^{\\text{pls}}(m).\n",
    "\\end{equation}\n",
    "These linear coefficients can be recovered from the sequence of PLS transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gist of the PLS algorithm\n",
    "\n",
    "PLS begins by computing the weights\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat\\rho_{1j} = \\langle \\mathbf{x}_j,\\mathbf{y} \\rangle, \\text{ for each } j,\n",
    "\\end{equation}\n",
    "which are in fact the univariate regression coefficients, since $\\mathbf{x}_j$ are standardized (only for the first step $m=1$).\n",
    "\n",
    "From this we construct derived input\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{z}_1 = \\sum_j \\hat\\rho_{1j}\\mathbf{j},\n",
    "\\end{equation}\n",
    "\n",
    "which is the first PLS direction. Hence in the construction of each $\\mathbf{z}_m$, the inputs are weighted by the strength of their univariate effect on $\\mathbf{y}$.\n",
    "\n",
    "The outcome $\\mathbf{y}$ is regressed on $\\mathbf{z}_1$ giving coefficient $\\hat\\theta_1$, and then we orthogonalize $\\mathbf{x}_1,\\cdots,\\mathbf{x}_p$ w.r.t. $\\mathbf{z}_1$.\n",
    "\n",
    "We continue this process, until $M\\le p$ directions have been obtained. In this manner, PLS produces a sequence of derived, orthogonal inputs or directions $\\mathbf{z}_1,\\cdots,\\mathbf{z}_M$.\n",
    "\n",
    "* As with PCR, if $M=p$, then $\\hat\\beta^{\\text{pls}} = \\hat\\beta^{\\text{ls}}$.\n",
    "* Using $M<p$ directions produces a reduced regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation to the optimization problem\n",
    "\n",
    "> PLS seeks direction that have high variance *and* have high correlation with the response, in contrast to PCR with keys only on high variance (Stone and Brooks, 1990; Frank and Friedman, 1993).\n",
    "\n",
    "Since it uses the response $\\mathbf{y}$ to construct its directions, its solution path is a nonlinear function of $\\mathbf{y}$.\n",
    "\n",
    "In particular, the $m$th principal component direction $v_m$ solves:\n",
    "\n",
    "\\begin{equation}\n",
    "\\max_\\alpha \\text{Var}(\\mathbf{X}\\alpha)\\\\\n",
    "\\text{subject to } \\|\\alpha\\| = 1, \\alpha^T\\mathbf{S} v_l = 0 \\text{ for } l = 1,\\cdots, m-1,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{S}$ is the sample covariance matrix of the $\\mathbf{x}_j$. The condition $\\alpha^T\\mathbf{S} v_l= 0$ ensures that $\\mathbf{z}_m = \\mathbf{X}\\alpha$ is uncorrelated with all the previous linear combinations $\\mathbf{z}_l = \\mathbf{X} v_l$.\n",
    "\n",
    "The $m$th PLS direction $\\hat\\rho_m$ solves:\n",
    "\n",
    "\\begin{equation}\n",
    "\\max_\\alpha \\text{Corr}^2(\\mathbf{y},\\mathbf{S}\\alpha)\\text{Var}(\\mathbf{X}\\alpha)\\\\\n",
    "\\text{subject to } \\|\\alpha\\| = 1, \\alpha^T\\mathbf{S}\\hat\\rho_l = 0 \\text{ for } l=1,\\cdots, m-1.\n",
    "\\end{equation}\n",
    "\n",
    "Further analysis reveals that the variance aaspect tends to dominate, and so PLS behaves much like ridge regression and PCR. We discuss further in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the input matrix $\\mathbf{X}$ is orthogonal, then PLS finds the least squares estimates after the first $m=1$ step, and subsequent steps have no effect since the $\\hat\\rho_{mj} = 0$ for $m>1$ (Exercise 3.14).\n",
    "\n",
    "It can be also shown that the sequence of PLS coefficients for $m=1,2,\\cdots,p$ represents the conjugate gradient sequence for computing the least squares solutions (Exercise 3.18)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
