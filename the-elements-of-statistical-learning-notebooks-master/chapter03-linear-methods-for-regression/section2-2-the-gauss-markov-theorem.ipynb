{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 3.2.2. The Gauss-Markov Theorem\n",
    "\n",
    "One of the most famous results in statistics asserts that\n",
    "\n",
    "> the least squares estimates of the parameter $\\beta$ have the smallest variance among all linear unbiased estimates.\n",
    "\n",
    "We will make this precise here, and also make clear that\n",
    "\n",
    "> the restriction to unbiased estimates is not necessarily a wise one.\n",
    "\n",
    "This observation will lead us to consider biased estimates such as ridge regression later in the chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The statement of the theorem\n",
    "\n",
    "We focus on estimation of any linear combination of the parameters $\\theta=a^T\\beta$. The least squares estimate of $a^T\\beta$ is\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat\\theta = a^T\\hat\\beta = a^T\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T\\mathbf{y}.\n",
    "\\end{equation}\n",
    "\n",
    "Considering $\\mathbf{X}$ to be fixed and the linear model is correct, $a^T\\beta$ is unbiased since\n",
    "\n",
    "\\begin{align}\n",
    "\\text{E}(a^T\\hat\\beta) &= \\text{E}\\left(a^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}\\right) \\\\\n",
    "&= a^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{X}\\beta \\\\\n",
    "&= a^T\\beta\n",
    "\\end{align}\n",
    "\n",
    "The Gauss-Markov Theorem states that if we have any other linear estimator $\\tilde\\theta = \\mathbf{c}^T\\mathbf{y}$ that is unbiased for $a^T\\beta$, that is, $\\text{E}(\\mathbf{c}^T\\mathbf{y})=a^T\\beta$, then\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Var}(a^T\\hat\\beta) \\le \\text{Var}(\\mathbf{c}^T\\mathbf{y}).\n",
    "\\end{equation}\n",
    "\n",
    "The proof (Exercise 3.3) uses the triangle inequality.\n",
    "\n",
    "For simplicity we have stated the result in terms of estimation of a single parameter $a^T \\beta$, but with a few more definitions one can state it in terms of the entire parameter vector $\\beta$ (Exercise 3.3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implications of the Gauss-Markov theorem\n",
    "\n",
    "Consider the mean squared error of an estimator $\\tilde\\theta$ of $\\theta$:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{MSE}(\\tilde\\theta) &= \\text{E}\\left(\\tilde\\theta-\\theta\\right)^2 \\\\\n",
    "&= \\text{Var}\\left(\\tilde\\theta\\right) + \\left[\\text{E}\\left(\\tilde\\theta-\\theta\\right)\\right]^2 \\\\\n",
    "&= \\text{Var} + \\text{Bias}^2\n",
    "\\end{align}\n",
    "\n",
    "The Gauss-Markov theorem implies that the least squares estimator has the smallest MSE of all linear estimators with no bias. However there may well exist a biased estimator with smaller MSE. Such an estimator would trade a little bias for a larger reduction in variance.\n",
    "\n",
    "Biased estimates are commonly used. Any method that shrinks or sets to zero some of the least squares coefficients may result in a biased estimate. We discuss many examples, including variable subset selection and ridge regression, later in this chapter.\n",
    "\n",
    "From a more pragmatic point of view, most models are distortions of the truth, and hence are biased; picking the right model amounts to creating the right balance between bias and variance. We go into these issues in more detail in Chapter 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation between prediction accuracy and MSE\n",
    "\n",
    "MSE is intimately related to prediction accuracy, as discussed in Chapter 2.\n",
    "\n",
    "Consider the prediction of the new response at input $x_0$,\n",
    "\n",
    "\\begin{equation}\n",
    "Y_0 = f(x_0) + \\epsilon.\n",
    "\\end{equation}\n",
    "\n",
    "Then the expected prediction error of an estimate $\\tilde{f}(x_0)=x_0^T\\tilde\\beta$ is\n",
    "\n",
    "\\begin{align}\n",
    "\\text{E}(Y_0 - \\tilde{f}(x_0))^2 &= \\text{E}\\left(Y_0 -f(x_0)+f(x_0) - \\tilde{f}(x_0)\\right)^2\\\\\n",
    "&= \\sigma^2 + \\text{E}\\left(x_o^T\\tilde\\beta - f(x_0)\\right)^2 \\\\\n",
    "&= \\sigma^2 + \\text{MSE}\\left(\\tilde{f}(x_0)\\right).\n",
    "\\end{align}\n",
    "\n",
    "Therefore, expected prediction error and MSE differ only by the constant $\\sigma^2$, representing the variance of the new observation $y_0$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
