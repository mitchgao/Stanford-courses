{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 3.2.3. Multiple Regression from Simple Univariate Regression\n",
    "\n",
    "The linear model with $p \\gt 1$ inputs is called _multiple linear regression model_.\n",
    "\n",
    "The least squares estimates\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat\\beta = \\left(\\mathbf{X}^T \\mathbf{X}\\right)^{-1} \\mathbf{X}^T \\mathbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "for this model are best understood in terms of the estimates for the *univariate* ($p=1$) linear model, as we indicate in this section.\n",
    "\n",
    "Suppose a univariate model with no intercept, i.e.,\n",
    "\n",
    "\\begin{equation}\n",
    "Y=X\\beta+\\epsilon.\n",
    "\\end{equation}\n",
    "\n",
    "The least squares estimate and residuals are\n",
    "\n",
    "\\begin{align}\n",
    "\\hat\\beta &= \\frac{\\sum_1^N x_i y_i}{\\sum_1^N x_i^2} = \\frac{\\langle\\mathbf{x},\\mathbf{y}\\rangle}{\\langle\\mathbf{x},\\mathbf{x}\\rangle}, \\\\\n",
    "r_i &= y_i - x_i\\hat\\beta, \\\\\n",
    "\\mathbf{r} &= \\mathbf{y} - \\mathbf{x}\\hat\\beta,\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "* $\\mathbf{y}=(y_1,\\cdots,y_N)^T$,\n",
    "* $\\mathbf{x}=(x_1,\\cdots,x_N)^T$ and\n",
    "* $\\langle\\cdot,\\cdot\\rangle$ denotes the dot product notation.\n",
    "\n",
    "As we will see,\n",
    "\n",
    "> this simple univariate regression provides the building block for multiple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building blocks for multiple linear regression\n",
    "\n",
    "Suppose next that the columns of the data matrix $\\mathbf{X} = \\left[\\mathbf{x}_1,\\cdots,\\mathbf{x}_p\\right]$ are orthogonal, i.e., \n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\mathbf{x}_j,\\mathbf{x}_k\\rangle = 0\\text{ for all }j\\neq k.\n",
    "\\end{equation}\n",
    "\n",
    "Then it is easy to check that the multiple least squares estimates are equal to the univariate estimates:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat\\beta_j = \\frac{\\langle\\mathbf{x}_j,\\mathbf{y}\\rangle}{\\langle\\mathbf{x}_j,\\mathbf{x}_j\\rangle}, \\forall j\n",
    "\\end{equation}\n",
    "\n",
    "> In other words, when the inputs are orthogonal, they have no effect on each other's parameter estimates in the model.\n",
    "\n",
    "Orthogonal inputs occur most often with balanced, designed experiments (where orthogonality is enforced), but almost never with observational data. Hence we will have to orthogonalize them in order to carry this idea further.\n",
    "\n",
    "#### Orthogonalization\n",
    "Suppose next that we have an intercept and a single input $\\mathbf{x}$. Then the least squares coefficient of $\\mathbf{x}$ has the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat\\beta_1 = \\frac{\\langle\\mathbf{x}-\\bar{x}\\mathbf{1},\\mathbf{y}\\rangle}{\\langle\\mathbf{x}-\\bar{x}\\mathbf{1},\\mathbf{x}-\\bar{x}\\mathbf{1}\\rangle},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\bar{x} = \\sum x_i /N$ and $\\mathbf{1} = \\mathbf{x}_0$. And also note that\n",
    "\n",
    "\\begin{equation}\n",
    "\\bar{x}\\mathbf{1} = \\frac{\\langle\\mathbf{1},\\mathbf{x}\\rangle}{\\langle\\mathbf{1},\\mathbf{1}\\rangle}\\mathbf{1},\n",
    "\\end{equation}\n",
    "\n",
    "which means the fitted value in the case we regress $\\mathbf{x}$ on $\\mathbf{x}_0=\\mathbf{1}$. Therefore we can view $\\hat\\beta_1$ as the result of two application of the simple regression with the following steps:\n",
    "1. Regress $\\mathbf{x}$ on $\\mathbf{1}$ to produce the residual $\\mathbf{z}=\\mathbf{x}-\\bar{x}\\mathbf{1}$;\n",
    "2. regress $\\mathbf{y}$ on the residual $\\mathbf{z}$ to give the coefficient $\\hat\\beta_1$.\n",
    "\n",
    "> In this procedure, \"regreess $\\mathbf{b}$ on $\\mathbf{a}$\" means a simple univariate regression of $\\mathbf{b}$ on $\\mathbf{a}$ with no intercept, producing coefficient $\\hat\\gamma=\\langle\\mathbf{a},\\mathbf{b}\\rangle/\\langle\\mathbf{a},\\mathbf{a}\\rangle$ and residual vector $\\mathbf{b}-\\hat\\gamma\\mathbf{a}$. We say that $\\mathbf{b}$ is adjusted for $\\mathbf{a}$, or is \"orthogonalized\" w.r.t. $\\mathbf{a}$.\n",
    "\n",
    "In other words,\n",
    "1. orthogonalize $\\mathbf{x}$ w.r.t. $\\mathbf{x}_0=\\mathbf{1}$;\n",
    "2. just a simple univariate regression, using the orthogonal predictors $\\mathbf{1}$ and $\\mathbf{z}$.\n",
    "\n",
    "FIGURE 3.4 shows this process for two general inputs $\\mathbf{x}_1$ and $\\mathbf{x}_2$. The orthogonalization does not change the subspace spanned by $\\mathbf{x}_0$ and $\\mathbf{x}_1$, it simply produces an orthogonal basis for representing it.\n",
    "\n",
    "This recipe gerenalizes to the case of $p$ inputs, as shown in ALGORITHM 3.1.\n",
    "\n",
    "#### ALGORITHM 3.1. Regression by successive orthogonalization\n",
    "1. Initialize $\\mathbf{z}_0=\\mathbf{x}_0=\\mathbf{1}$.\n",
    "2. For $j = 1, 2, \\cdots, p$,  \n",
    "  regress $\\mathbf{x}_j$ on $\\mathbf{z}_0,\\mathbf{z}_1,\\cdots,\\mathbf{z}_{j-1}$ to produce\n",
    "  * coefficients $\\hat\\gamma_{lj} = \\langle\\mathbf{z}_l,\\mathbf{x}_j\\rangle/\\langle\\mathbf{z}_l,\\mathbf{z}_l\\rangle$, for $l=0,\\cdots, j-1$ and\n",
    "  * residual vector $\\mathbf{z}_j=\\mathbf{x}_j - \\sum_{k=0}^{j-1}\\hat\\gamma_{kj}\\mathbf{z}_k$.\n",
    "3. Regress $\\mathbf{y}$ on the residual $\\mathbf{z}_p$ to give the estimate $\\hat\\beta_p$,\n",
    "\n",
    "  \\begin{equation}\n",
    "  \\hat\\beta_p = \\frac{\\langle\\mathbf{z}_p,\\mathbf{y}\\rangle}{\\langle\\mathbf{z}_p,\\mathbf{z}_p\\rangle}.\n",
    "  \\end{equation}\n",
    "\n",
    "Note that the inputs $\\mathbf{z}_0,\\cdots,\\mathbf{z}_{j-1}$ in step 2 are orthogonal, hence the simple regression coefficients computed there are in fact also the multiple regression coefficients.\n",
    "\n",
    "The result of this algorithm is\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat\\beta_p = \\frac{\\langle \\mathbf{z}_p, \\mathbf{y} \\rangle}{\\langle \\mathbf{z}_p, \\mathbf{z}_p \\rangle}.\n",
    "\\end{equation}\n",
    "\n",
    "Re-arranging the residual in step 2, we can see that each of the $\\mathbf{x}_j$ is\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x}_j = \\mathbf{z}_j + \\sum_{k=0}^{j-1} \\hat\\gamma_{kj}\\mathbf{z}_k,\n",
    "\\end{equation}\n",
    "\n",
    "which is a linear combination of the $\\mathbf{z}_k$, $k \\le j$. Since the $\\mathbf{z}_j$ are all orthogonal, they form a basis for the $\\text{col}(\\mathbf{X})$, and hence the least squares projection onto this subspace is $\\hat{\\mathbf{y}}$.\n",
    "\n",
    "Since $\\mathbf{z}_p$ alone involves $\\mathbf{x}_p$ (with coefficient 1), we see that the coefficient $\\hat\\beta_p$ is indeed the multiple regression coefficient of $\\mathbf{y}$ on $\\mathbf{x}_p$. This key result exposes the effect of correlated inputs in mutiple regression.\n",
    "\n",
    "Note also that by rearranging the $j$th multiple regression coefficient is the univariate regression coefficient of $\\mathbf{y}$ on $\\mathbf{x}_{j\\cdot 012\\cdots(j-1)(j+1)\\cdots p}$, the residual after regressing $\\mathbf{x}_j$ on $\\mathbf{x}_0,\\mathbf{x}_1,\\cdots,\\mathbf{x}_{j-1},\\mathbf{x}_{j+1},\\cdots,\\mathbf{x}_p$:\n",
    "\n",
    "> The multiple regression coefficient $\\hat\\beta_j$ represents the additional contribution of $\\mathbf{x}_j$ on $\\mathbf{y}$, after $\\mathbf{x}_j$ has been adjusted for $\\mathbf{x}_0,\\mathbf{x}_1,\\cdots,\\mathbf{x}_{j-1},\\mathbf{x}_{j+1},\\cdots,\\mathbf{x}_p$.\n",
    "\n",
    "#### Correlated variables\n",
    "If $\\mathbf{x}_p$ is highly correlated with some of the other $\\mathbf{x}_k$'s, the residual vector $\\mathbf{z}_p$ will be close to zero, then the coefficient $\\hat\\beta_p$ will be very unstable. This will be true for all the varialbes in the correlated set. In such situations, we might have all the $Z$-scores be small -- any one of the set can be deleted -- yet we cannot delete them all.\n",
    "\n",
    "We also obtain an alternative formula for the variance estimates,\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Var}(\\hat\\beta_p) = \\text{Var}\\left(\\frac{\\langle\\mathbf{z}_p,\\mathbf{y}\\rangle}{\\langle\\mathbf{z}_p,\\mathbf{z}_p\\rangle}\\right) = \\frac{\\sigma^2}{\\|\\mathbf{z}_p\\|^2}.\n",
    "\\end{equation}\n",
    "\n",
    "In other words, the precision with which we can estimate with $\\hat\\beta_p$ depends on the length of the residual vector $\\mathbf{z}_p$; this represents how much of $\\mathbf{x}_p$ is unexplained by the other $\\mathbf{x}_k$'s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gram-Schmidt procedure and QR decomposition\n",
    "\n",
    "Algorithm 3.1 is known as the _Gram-Schmidt_ procedure for multiple regression, and is also a useful numerical strategy for computing the estimates. We can obtain from it not just $\\hat\\beta_p$, but also the entire multiple least squares fit (Exercise 3.4).\n",
    "\n",
    "We can represent step 2 of Algorithm 3.1 in matrix form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \\mathbf{Z\\Gamma},\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "* $\\mathbf{Z}$ has as columns the $\\mathbf{z}_j$ (in order)\n",
    "* $\\mathbf{\\Gamma}$ is the upper triangular matrix with entries $\\hat\\gamma_{kj}$.\n",
    "\n",
    "Introducing the diagonal matrix $\\mathbf{D}$ with $D_{jj}=\\|\\mathbf{z}_j\\|$, we get\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{X} &= \\mathbf{ZD}^{-1}\\mathbf{D\\Gamma} \\\\\n",
    "&= \\mathbf{QR},\n",
    "\\end{align}\n",
    "\n",
    "the so-called _QR decomposition_ of $\\mathbf{X}$. Here\n",
    "* $\\mathbf{Q}$ is an $N\\times(p+1)$ orthogonal matrix s.t. $\\mathbf{Q}^T\\mathbf{Q}=\\mathbf{I}$,\n",
    "* $\\mathbf{R}$ is a $(p+1)\\times(p+1)$ upper triangular matrix.\n",
    "\n",
    "The QR decomposition represents a convenient orthogonal basis for the $\\text{col}(\\mathbf{X})$. It is easy to see, for example, that the least squares solution is given by\n",
    "\n",
    "\\begin{align}\n",
    "\\hat\\beta &= \\mathbf{R}^{-1}\\mathbf{Q}^T\\mathbf{y}, \\\\\n",
    "\\hat{\\mathbf{y}} &= \\mathbf{QQ}^T\\mathbf{y}.\n",
    "\\end{align}\n",
    "\n",
    "Note that the triangular matrix $\\mathbf{R}$ makes it easy to solve (Exercise 3.4)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
