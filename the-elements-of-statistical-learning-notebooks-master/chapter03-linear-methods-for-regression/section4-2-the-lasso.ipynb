{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 3.4.2. The Lasso\n",
    "\n",
    "The lasso estimate is defined by\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat\\beta^{\\text{lasso}} = \\arg\\min_\\beta \\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p x_{ij}\\beta_j \\right)^2 \\text{ subject to } \\sum_{j=1}^p |\\beta_j| \\le t,\n",
    "\\end{equation}\n",
    "\n",
    "Just as in ridge regression, we can re-parametrize the constant $\\beta_0$ by standardizing the predictors; $\\hat\\beta_0 = \\bar{y}$, and thereafter we fit a model without an intercept.\n",
    "\n",
    "In the signal processing literature, the lasso is a.k.a. *basis pursuit* (Chen et al., 1998).\n",
    "\n",
    "Also the lasso problem has the equivalent *Lagrangian form*\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat\\beta^{\\text{lasso}} = \\arg\\min_\\beta \\left\\lbrace \\frac{1}{2}\\sum_{i=1}^N \\left( y_i-\\beta_0-\\sum_{j=1}^p x_{ij}\\beta_j \\right)^2 + \\lambda\\sum_{j=1}^p |\\beta_j| \\right\\rbrace,\n",
    "\\end{equation}\n",
    "\n",
    "which is similar to the ridge problem as the $L_2$ ridge penalty is replaced by the $L_1$ lasso penalty. This lasso constraint makes the solutions nonlinear in the $y_i$, and there is no closed form expresssion as in ridge regression. And computing the above lasso solution is a quadratic programming problem, although efficient algorithms, introduced in $\\S$ 3.4.4, are available for computing the entire path of solution as $\\lambda$ varies, with the same computational cost as for ridge regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that\n",
    "\n",
    "* If $t \\gt t_0 = \\sum_1^p \\lvert\\hat\\beta_j^{\\text{ls}}\\rvert$, then $\\hat\\beta^{\\text{lasso}} = \\hat\\beta^{\\text{ls}}$.\n",
    "* Say, for $t = t_0/2$, then the least squares coefficients are shrunk by about $50\\%$ on average.  \n",
    "However, the nature of the shrinkage is not obvious, and we investigate it further in $\\S$ 3.4.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In FIGURE 3.7, for ease of interpretation, we have plotted the lasso prediction error estimates versus the standardized parameter\n",
    "\n",
    "\\begin{equation}\n",
    "s = \\frac{t}{\\sum_1^p \\lvert\\hat\\beta_j\\rvert}.\n",
    "\\end{equation}\n",
    "\n",
    "A value $\\hat s \\approx 0.36$ was chosen by 10-fold cross-validation; this caused four coefficients to be set to zero (see Table 3.3). The resulting model has the second lowest test error, slightly lower than the full least squares model, but the standard errors of the test error estimates are fairly large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIGURE 3.10 is discussed after implementing the lasso algorithm in $\\S$ 3.4.4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
