{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 3.5. Methods Using Derived Input Directions\n",
    "## $\\S$ 3.5.1. Principal Components Regression\n",
    "\n",
    "The linear combinations $Z_m$ used in principal component regression (PCR) are the principal components as defined in $\\S$ 3.4.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCR forms the derived input columns\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{z}_m = \\mathbf{X} v_m,\n",
    "\\end{equation}\n",
    "\n",
    "and then regress $\\mathbf{y}$ on $\\mathbf{z}_1,\\mathbf{z}_2,\\cdots,\\mathbf{z}_M$ for some $M\\le p$. Since the $\\mathbf{z}_m$ are orthogonal, this regression is just a sum of univariate regressions:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{y}}_{(M)}^{\\text{pcr}} = \\bar{y}\\mathbf{1} + \\sum_{m=1}^M \\hat\\theta_m \\mathbf{z}_m = \\bar{y}\\mathbf{1} + \\mathbf{X}\\mathbf{V}_M\\hat{\\mathbf{\\theta}},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat\\theta_m = \\langle\\mathbf{z}_m,\\mathbf{y}\\rangle \\big/ \\langle\\mathbf{z}_m,\\mathbf{z}_m\\rangle$. We can see from the last equality that, since the $\\mathbf{z}_m$ are each linear combinations of the original $\\mathbf{x}_j$, we can express the solution in terms of coefficients of the $\\mathbf{x}_j$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat\\beta^{\\text{pcr}}(M) = \\sum_{m=1}^M \\hat\\theta_m v_m.\n",
    "\\end{equation}\n",
    "\n",
    "As with ridge regression, PCR depends on the scaling of the inputs, so typically we first standardized them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with ridge regression\n",
    "\n",
    "If $M=p$, since the columns of $\\mathbf{Z} = \\mathbf{UD}$ span the $\\text{col}(\\mathbf{X})$,\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat\\beta^{\\text{pcr}}(p) = \\hat\\beta^{\\text{ls}}.\n",
    "\\end{equation}\n",
    "\n",
    "For $M<p$ we get a reduced regression and we see that PCR is very similar to ridge regression: both operate via the principal components of the input matrix.\n",
    "* Ridge regression shrinks the coefficients of the principal components (FIGURE 3.17), shrinking more depending on the size of the corresponding eigenvalue;\n",
    "* PCR discards the $p-M$ smallest eigenvalue components. FIGURE 3.17 illustrates this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
