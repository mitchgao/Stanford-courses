{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 2.9. Model Selection and the Bias-Variance Tradeoff\n",
    "\n",
    "### Review\n",
    "\n",
    "All the Models described so far have a *smoothing* or *complexity* parameter that has to be determined:\n",
    "* The multiplier of the penalty term;\n",
    "* the width of the kernel;\n",
    "* or the number of basis functions.\n",
    "\n",
    "In the case of the smoothing spline, the parameter $\\lambda$ indexes models ranging from a straight line fit to the interpolating model.\n",
    "\n",
    "Similarly a local degree-$m$ polynomial model ranges between a degree-$m$ global polynomial when the window size is infinitely large, to an interpolating fit when the window size shrinks to zero.\n",
    "\n",
    "This means that we cannot use residual sum-of-squares on the training data to determine these parameters as well, since we would always pick those that gave interpolating fits and hence zero residuals. Such a model is unlikely to predict future data well at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The bias-variance tradeoff for the kNN\n",
    "\n",
    "The kNN regression fit $\\hat{f}_k(x_0)$ usefully illustrates the competing forces that affect the predictive ability of such approximations.\n",
    "\n",
    "Suppose\n",
    "* the data arise from a model $Y=f(X)+\\epsilon$, with $\\text{E}(\\epsilon)=0$ and $\\text{Var}(\\epsilon)=\\sigma^2$;\n",
    "* for simplicity here the values of $x_i$ in the sample are fixed in advance (nonrandom).\n",
    "\n",
    "The expected prediction error at $x_0$, a.k.a. _test_ or _generalization_ error, can be decomposed:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{EPE}_k(x_0) &= \\text{E}\\left[(Y-\\hat{f}_k(x_0))^2|X=x_0\\right] \\\\\n",
    "&= \\text{E}\\left[(Y -f(x_0) + f(x_0) -\\hat{f}_k(x_0))^2|X=x_0\\right] \\\\\n",
    "&= \\text{E}(\\epsilon^2) + 2\\text{E}\\left[\\epsilon(f(x_0) -\\hat{f}_k(x_0))|X=x_0\\right] + \\text{E}\\left[\\left(f(x_0)-\\hat{f}_k(x_0)\\right)^2|X=x_0\\right] \\\\\n",
    "&= \\sigma^2 + 0+ \\left[\\text{Bias}^2(\\hat{f}_k(x_0))+\\text{Var}_\\mathcal{T}(\\hat{f}_k(x_0))\\right] \\\\\n",
    "&= \\sigma^2 + \\left(f(x_0) - \\frac{1}{k}\\sum_{l=1}^k f(x_{(l)})\\right)^2 + \\frac{\\sigma^2}{k}\n",
    "\\end{align},\n",
    "\n",
    "where subscripts in parentheses $(l)$ indicate the sequence of nearest neighbors to $x_0$.\n",
    "\n",
    "There are three terms in this expression.\n",
    "\n",
    "#### Irreducible error\n",
    "The first term $\\sigma^2$ is the *irreducible* error -- the variance of the new test target -- and is beyond our control, even if we know the true $f(x_0)$.\n",
    "\n",
    "The second and third terms are under our control, and make up the _mean squared error_ of $\\hat{f}_k(x_0)$ in estimateing $f(x_0)$, which is broken down into a bias component and a variance component.\n",
    "\n",
    "#### Bias\n",
    "The bias term is the squared difference between the true mean $f(x_0)$ and the expected value of the estimate, i.e.,\n",
    "\n",
    "\\begin{equation}\n",
    "\\left[ \\text{E}_\\mathcal{T} \\left( \\hat{f}_k(x_0) \\right) - f(x_0) \\right]^2,\n",
    "\\end{equation}\n",
    "\n",
    "where the expectation averages the randomness in the training data.\n",
    "\n",
    "This term will most likely increase with $k$, if the true function is reasonably smooth. For small $k$ the few closest neighbors will have values $f(x_{(l)})$ close to $f(x_0)$, so their average should be close to $f(x_0)$. As $k$ grows, the neighbors are further away, and then anything can happen.\n",
    "\n",
    "#### Variance\n",
    "The variance term is simply the variance of an average here, and decreases as the inverse of $k$.\n",
    "\n",
    "#### Finally, the tradeoff\n",
    "So as $k$ varies, there is a *bias-variance tradeoff*.\n",
    "\n",
    "More generally, as the _model complexity_ of our procedure is increased, the variance tends to increase and the squared bias tends to decrease, vice versa. For kNN, the model complexity is controlled by $k$.\n",
    "\n",
    "Typically we would like to choose our model complexity to trade bias off with variance in such a way as to minimize the test error. An obvious estimate of test error is _training error_\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{N} \\sum_i (y_i \\hat{y}_i)^2.\n",
    "\\end{equation}\n",
    "\n",
    "Unfortunately training error is not a good estimate of test error, as it does not properly account for model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation & implication\n",
    "FIGURE 2.11 shows the typical behavior of the test and training error, as model complexity is varied.\n",
    "\n",
    "> The training error tends to decrease whenever we increase the model complexity, i.e., whenever we fit the data harder.\n",
    "\n",
    "However with too much fitting, the model adapts itself too closely to the training data, and will not generalize well (i.e., have large test error). In that case the predictions $\\hat{f}(x_0)$ will have large variance, as reflected in the above EPE expression.\n",
    "\n",
    "In contrast, if the model is not complex enough, it will _underfit_ and may have large mias, again resulting in poor generalization. In Chapter 7 we discuss methods for estimating the test error of a prediction method, and hence estimating the optimal amount of model complexity fir a given prediction method and training set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
