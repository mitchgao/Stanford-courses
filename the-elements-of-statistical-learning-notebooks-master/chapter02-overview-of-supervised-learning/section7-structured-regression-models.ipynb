{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 2.7. Structured Regression Models\n",
    "\n",
    "### Review & motivation\n",
    "\n",
    "We have seen that although nearest-neighbor and other local methods focus directly on estimating the function at a point, they face problems in high dimensions. They may also be inappropriate even in low dimensions in cases where more structured approaches can make more efficient use of the data.\n",
    "\n",
    "This section introduces classes of such structured approaches. Before we proceed, though, we discuss further the need for such classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 2.7.1. Difficulty of the Problem\n",
    "\n",
    "Consider the RSS criterion for an arbitrary function $f$,\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{RSS}(f) = \\sum_{i=1}^N \\left( y_i - f(x_i) \\right)^2.\n",
    "\\end{equation}\n",
    "\n",
    "Minimizing the RSS leads to infinitely many solutions: Any function $\\hat{f}$ passing through the training points $(x_i,y_i)$ is a solution. Any particular solution chosen might be a poor predictor at test points different from the training points.\n",
    "\n",
    "If there are multiple observation pairs $(x_i,y_{il})$, $l=1,\\cdots,N_i$, at each value of $x_i$, the risk is limited. In this case, the solution pass through the average values of the $y_{il}$ at each $x_i$ (Exercise 2.6). The situation is similar to the one we have already visited in $\\S$ 2.4; indeed, the above RSS is the finite sample version of the expected prediction error\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{EPE}(f) = \\text{E}\\left( Y - f(X) \\right)^2 = \\int \\left( y - f(x) \\right)^2 \\text{Pr}(dx, dy).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessity & limit of the restriction\n",
    "\n",
    "If the sample size $N$ were sufficiently large such that repeats were guaranteed and densely arranged, it would seem that these solutions might all tend to the limiting conditional expectation.\n",
    "\n",
    "In order to obtain useful results for finite $N$, we must restrict the eligible solution to the RSS to a smaller set of functions.\n",
    "\n",
    "> How to decide on the nature of the restrictions is based on considerations outside of the data.\n",
    "\n",
    "These restrictions are somtimes\n",
    "* encoded via the parametric representation of $f_\\theta$, or\n",
    "* may be built into the learning method itself, either implicitly or explicitly.\n",
    "\n",
    "> These restricted classes of solutions are the major topic of this book.\n",
    "\n",
    "One thing should be clear, though.\n",
    "\n",
    "> Any restrictions imposed on $f$ that lead to a unique solution to RSS do not really remove the ambiguity caused by the multiplicity of solutions. There are infinitely many possible restrictions, each leading to a unique solution, so the abmiguity has simply been transferred to the choice of constraint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complexity\n",
    "\n",
    "In general the constraints imposed by most learning methods can be described as _complexity_ restrictions of one kind or another.\n",
    "\n",
    "> This usually means some kind of regular behavior in small neighborhoods of the input space.\n",
    "\n",
    "That is, for all input points $x$ sufficiently close to each other in some metric, $\\hat{f}$ exhibits some special structure such as\n",
    "* nearly constant,\n",
    "* linear or\n",
    "* low-order polynomial behavior.\n",
    "\n",
    "The estimator is then obtained by averaging or polynomial fitting in that neighborhood.\n",
    "\n",
    "The strength of the constraint is dictated by the neighborhood size.\n",
    "\n",
    "> The larger the size, the stronger the constraint, and the more sensitive the solution is to the particular choice of constraint.\n",
    "\n",
    "For example,\n",
    "* local constant fits in infinitesimally small neighborhoods is no constraints at all;\n",
    "* local linear fits in very large neighborhoods is almost a globally llinear model, and is very restrictive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric\n",
    "\n",
    "The nature of the constraint depends on the metric used.\n",
    "\n",
    "Some methods, such as kernel and local regression and tree-based methods, directly specify the metric and size of the neighborhood. The kNN methods discussed so far are based on the assumption that locally the function is constant; close to a target input $x_0$, the function does not change much, and so close outputs can be averagedd to produce $\\hat{f}(x_0)$.\n",
    "\n",
    "Other methods such as splines, neural networks and basis-function methods implicitly define neighborhoods of local behavior. In $\\S$ 5.4.1 we discuss the concept of an _equivalent kernel_, which describes this local dependence for any method linear in the outputs. These equivalent kernels in many cases look just like the explicitly defined weighting kernels discussed above -- peaked at the target point and falling smoothly away from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curse of dimensionality\n",
    "\n",
    "One fact should be clear by now. Any method that attempts to produce locally varying functions in small isotopic neighborhoods will run into problems in high dimensions -- again the curse of dimensionality.\n",
    "\n",
    "And conversely, all methods that overcome the dimensionality problems have an associated -- and often implicit or adaptive -- metric for measuring neighborhoods, which basically does not allow the neighborhood to be simultaneously small in all directions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
