{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 5.6. Nonparametric Logistic Regression\n",
    "\n",
    "The smoothing spline problem in $\\S$ 5.4,\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{RSS}(f, \\lambda) = \\sum_{i=1}^N \\left( y_i - f(x_i) \\right)^2 + \\lambda\\int \\left( f''(t) \\right)^2 dt,\n",
    "\\end{equation}\n",
    "\n",
    "is posed in a regression setting. It is typically straightforward to transfer this technology to other domains. Here we consider logistic regression with a single quantitative input $X$. Then the model is\n",
    "\n",
    "\\begin{equation}\n",
    "\\log \\frac{\\text{Pr}(Y=1|X=x)}{\\text{Pr}(Y=0|X=x)} = f(x),\n",
    "\\end{equation}\n",
    "\n",
    "which implies\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Pr}(Y=1|X=x) = \\frac{e^{f(x)}}{1+e^{f(x)}}.\n",
    "\\end{equation}\n",
    "\n",
    "Fitting $f(x)$ in a smooth fashion leads to a smooth estimate of the conditional probability $\\text{Pr}(Y=1|x)$, which can be used for classification or risk scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLE\n",
    "\n",
    "We construct the penalized log-likelihood criterion\n",
    "\n",
    "\\begin{align}\n",
    "l(f;\\lambda) &= \\sum_{i=1}^N \\left[ y_i\\log{p(x_i)} + (1-y_i)\\log{(1-p(x_i))} \\right] - \\frac{\\lambda}{2} \\int \\left( f''(t) \\right)^2 dt \\\\\n",
    "&= \\sum_{i=1}^N \\left[ y_i f(x_i) - \\log{(1+e^{f(x_i)})} \\right] - \\frac{\\lambda}{2} \\int \\left( f''(t) \\right)^2 dt,\n",
    "\\end{align}\n",
    "\n",
    "where $p(x) = \\text{Pr}(Y=1|x)$. The first term is the log-likelihood on the binomial distribution (c.f. Chapter 4, page 120)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative procedure using Newton-Raphson, again\n",
    "\n",
    "Arguments similar to those used in $\\S$ 5.4 show that the optimal $f$ is a finite-dimensional natural spline with knots at the unique values of $x$. This means that we can represent\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\sum_{j=1}^N N_j(x) \\theta_j.\n",
    "\\end{equation}\n",
    "\n",
    "We compute the first and second derivatives\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial l(\\theta)}{\\partial\\theta} &= \\mathbf{N}^T(\\mathbf{y}-\\mathbf{p}) - \\lambda\\mathbf{\\Omega}\\theta, \\\\\n",
    "\\frac{\\partial^2 l(\\theta)}{\\partial\\theta\\partial\\theta^T} &= -\\mathbf{N}^T\\mathbf{WN} - \\lambda\\mathbf{\\Omega},\n",
    "\\end{align}\n",
    "\n",
    "where\n",
    "* $\\mathbf{p}$ is the $N$-vector with elements $p(x_i)$,\n",
    "* $\\mathbf{W}$ is a diagonal matrix of weights $p(x_i)(1-p(x_i))$.\n",
    "\n",
    "The first derivative is nonlinear in $\\theta$, so we need to use an iterative algorithm as in $\\S$ 4.4.1. Using Newton-Raphson as for linear logistic regression, the update equation can be written\n",
    "\n",
    "\\begin{align}\n",
    "\\theta^{\\text{new}} &= \\left( \\mathbf{N}^T\\mathbf{WN} + \\lambda\\mathbf{\\Omega} \\right)^{-1} \\mathbf{N}^T\\mathbf{W} \\left( \\mathbf{N}\\theta^{\\text{old}} + \\mathbf{W}^{-1}(\\mathbf{y}-\\mathbf{p}) \\right) \\\\\n",
    "&= \\left( \\mathbf{N}^T\\mathbf{WN} + \\lambda\\mathbf{\\Omega} \\right)^{-1} \\mathbf{N}^T\\mathbf{Wz}.\n",
    "\\end{align}\n",
    "\n",
    "We can also express this update in terms of the fitted values\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{f}^{\\text{new}} &= \\mathbf{N} \\left( \\mathbf{N}^T\\mathbf{WN} + \\lambda\\mathbf{\\Omega} \\right)^{-1} \\mathbf{N}^T\\mathbf{W} \\left( \\mathbf{f}^{\\text{old}} + \\mathbf{W}^{-1}(\\mathbf{y}-\\mathbf{p}) \\right) \\\\\n",
    "&= \\mathbf{S}_{\\lambda,\\omega}\\mathbf{z}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison with regressions\n",
    "\n",
    "Referring back to the regression solution of the smoothing spline problem in $\\S$ 5.4,\n",
    "\n",
    "\\begin{align}\n",
    "\\hat\\theta &= \\left( \\mathbf{N}^T\\mathbf{N} + \\lambda\\mathbf{\\Omega}_N \\right)^{-1} \\mathbf{N}^T \\mathbf{y} \\\\\n",
    "\\hat{\\mathbf{f}} &= \\mathbf{N} \\left( \\mathbf{N}^T\\mathbf{N} + \\lambda\\mathbf{\\Omega}_N \\right)^{-1} \\mathbf{N}^T \\mathbf{y} \\\\\n",
    "&= \\mathbf{S}_\\lambda \\mathbf{y},\n",
    "\\end{align}\n",
    "\n",
    "we see that the update fits a weighted smoothing spline to the working response $\\mathbf{z}$ (Exercise 5.12).\n",
    "\n",
    "The form of $\\mathbf{f}^{\\text{new}}$ is suggestive. It is tempting to replace $\\mathbf{S}_{\\lambda,\\omega}$ by any nonparametric (weighted) regression operator, and obtain general families of nonparametric logistic regression models.\n",
    "\n",
    "Although here $x$ is one-dimensional, this procedure generalizes naturally to higher-dimensional $x$. These extensions are at the heart of _generalized additive models_, which we pursue in Chapter 9."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
