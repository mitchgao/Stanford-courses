{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 5.5.2. The Bias-Variance Tradeoff\n",
    "\n",
    "FIGURE 5.9 shows the effect of the choice of $\\text{df}_\\lambda$ when using a smoothing spline on a simple example:\n",
    "\n",
    "\\begin{align}\n",
    "Y &= f(X) + \\epsilon \\\\\n",
    "f(X) &= \\frac{\\sin(12(X+0.2))}{X+0.2},\n",
    "\\end{align}\n",
    "\n",
    "with\n",
    "* $X\\sim U[0,1]$,\n",
    "* $\\epsilon\\sim N(0,1)$;\n",
    "* Our training sample consists of $N=100$ pairs of $x_i, y_i$, drawn independently from this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under construction ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"FIGURE 5.9. CV results and fitted splines for three different df's.\n",
    "\"\"\"\n",
    "print('Under construction ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing bias and variance\n",
    "\n",
    "The yellow shaded region in the figure represents the pointwise standard error of $\\hat{f}_\\lambda$, e.g., we have shaded the region between \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}_\\lambda(x) \\pm 2 \\cdot \\text{se}(\\hat{f}_\\lambda(x)).\n",
    "\\end{equation}\n",
    "\n",
    "Since $\\hat{\\mathbf{f}} = \\mathbf{S}_\\lambda \\mathbf{y}$,\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Cov}(\\hat{\\mathbf{f}}) &= \\mathbf{S}_\\lambda \\text{Cov}(\\mathbf{y}) \\mathbf{S}_\\lambda^T \\\\\n",
    "&= \\mathbf{S}_\\lambda \\mathbf{S}_\\lambda^T.\n",
    "\\end{align}\n",
    "\n",
    "The diagonal contains the pointwise variances at the training $x_i$. The bias is given by\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Bias}(\\hat{\\mathbf{f}}) &= \\mathbf{f} - \\text{E}(\\hat{\\mathbf{f}}) \\\\\n",
    "&= \\mathbf{f} - \\mathbf{S}_\\lambda \\mathbf{f},\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{f}$ is the (unknown) vector of evaluations of the true $f$ at the training $X$'s.\n",
    "\n",
    "The expectations and variances are w.r.t. repeated draws of samples of size $N=100$ from the model $f$. In a similar fashion $\\text{Var}(\\hat{f}_\\lambda(x_0))$ and $\\text{Bias}(\\hat{f}_\\lambda(x_0))$ can be computed at any point $x_0$ (Exercise 5.10)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual interpretation of bias-variance tradeoff\n",
    "\n",
    "The three fits displayed in FIGURE 5.9 give a visual demonstration of the bias-variance tradeoff associated with selecting the smoothing parameter.\n",
    "\n",
    "* $\\text{df}_\\lambda = 5$: The spline under fits, and clearly _trims down hills and fills in the valleys_. This leads to a bias that is most dramatic in regions of high curvature. The standard error hand is very narrow, so we estimate a badly biased version of the true function with great reliability!\n",
    "* $\\text{df}_\\lambda = 9$: Here the fitted function is close to the true function, although a slight amount of bias seems evident. The variance has not increased appreicably.\n",
    "* $\\text{df}_\\lambda = 15$: The fitted function is somewhat wiggly, but close to the true function. The wiggliness also accounts for the increased width of the standard error bands -- the curve is starting to follow some individual points too closely.\n",
    "\n",
    "Note that in these figures we are seeing a single realization of data and hence fitted spline $\\hat{f}$ in each case, while the bias involves an expectation $\\text{E}(\\hat{f})$.\n",
    "\n",
    "The middle curve seems \"just right\", in that it has achieved a good compromise between bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integrated squared prediction error ($\\text{EPE}$) combines both bias and variance in a single summary:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{EPE}(\\hat{f}_\\lambda) &= \\text{E}\\left( Y - \\hat{f}_\\lambda(X) \\right)^2 \\\\\n",
    "&= \\text{Var}(Y) + \\text{E}\\left( \\text{Bias}^2(\\hat{f}_\\lambda(X)) + \\text{Var}(\\hat{f}_\\lambda(X)) \\right) \\\\\n",
    "&= \\sigma^2 + \\text{MSE}(\\hat{f}_\\lambda).\n",
    "\\end{align}\n",
    "\n",
    "Note that this is averaged both over the training sample (giving rise to $\\hat{f}_\\lambda$), and the values of the (independently chosen) prediction points $(X,Y)$.\n",
    "\n",
    "$\\text{EPE}$ is a natural quantity of interest, and does create a tradeoff between bias and variance. The test error rate (blue points) in the top left panel of FIGURE 5.9 suggest that $\\text{df}=9$ is spot on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation of EPE\n",
    "\n",
    "Since we don't know the true function, we do not have access to $\\text{EPE}$, and need an estimate. This topic is discussed in some detail in Chapter 7, and techniques such as $K$-fold cross-validation, $\\text{GCV}$ and $C_p$ are all in common use. In FIGURE 5.9 we include the $N$-fold (leave-one-out) cross-validation curve:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{CV}(\\hat{f}_\\lambda) &= \\frac{1}{N} \\sum_{i=1}^N \\left( y_i - \\hat{f}_\\lambda^{(-i)}(x_i)\\right)^2 \\\\\n",
    "&= \\frac{1}{N} \\sum_{i=1}^N \\left( \\frac{y_i - \\hat{f}_\\lambda(x_i)}{1 - S_\\lambda(i,i)} \\right)^2,\n",
    "\\end{align}\n",
    "\n",
    "which can (remarkably) be computed for each value of $\\lambda$ from the original fitted values and the diagonal elements $S_\\lambda(i,i)$ of $\\mathbf{S}_\\lambda$ (Exercise 5.13).\n",
    "\n",
    "The $\\text{EPE}$ and $\\text{CV}$ curves have a similar shape, but the entire $\\text{CV}$ curve is above the $\\text{EPE}$ curve. For some realizations this is reversed, and everall the $\\text{CV}$ curve is approximately unbiased as an estimate of the $\\text{EPE}$ curve."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
