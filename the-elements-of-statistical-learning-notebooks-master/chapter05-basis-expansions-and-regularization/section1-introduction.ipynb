{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5.Basis Expansions and Regularization\n",
    "# $\\S$ 5.1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearity is unrealistic, but necessary\n",
    "\n",
    "Linear regression, LDA, logistic regression and separating hyper planes all rely on a linear model. It is extremely unlikely that the true function $f(X)$ is actually linear in $X$. In regression problems, $f(X) = \\text{E}(Y|X)$ will typically be nonlinear and nonadditive in $X$.\n",
    "\n",
    "Representing $f(X)$ by a linear model is usually a convenient, and sometimes necessary, approximation.\n",
    "* Convenient because a linear model is easy to interpret, and is the first-order Taylor approximation to $f(X)$.\n",
    "* Sometimes necessary because with $N$ small and/or $p$ large, a linear model might be all we are able to fit to the data without overfitting.\n",
    "\n",
    "Likewise in classification, a linear, Bayes-optimal decision boundary implies that some monotone transformation of $\\text{Pr}(Y=1|X)$ is linear in $X$. This is inevitably an approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond linearity\n",
    "\n",
    "The core idea in this chapter is to augment/replace the vector of inputs $X$ with additional variables, which are transformatons of $X$, and then use linear models in this new space of derived input features.\n",
    "\n",
    "Denote by\n",
    "\n",
    "\\begin{equation}\n",
    "h_m(X): \\mathbb{R}^p \\mapsto \\mathbb{R}\n",
    "\\end{equation}\n",
    "\n",
    "the $m$th transformation of $X$ for $m=1,\\cdots,M$. We then model\n",
    "\n",
    "\\begin{equation}\n",
    "f(X) = \\sum_{m=1}^M \\beta_m h_m(X),\n",
    "\\end{equation}\n",
    "\n",
    "_a linear basis expansion_ in $X$.\n",
    "\n",
    "The beauty of this approach is that once the basis functions $h_m$ have been determined, the models are linear in these new variables, and the fitting proceeds as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "Some simple and widely used examples of the $h_m$ are the following.\n",
    "\n",
    "* $h_m(X)=X_m$, $m=1,\\cdots,p$ recovers the original linear model.\n",
    "* $h_m(X)=X_j^2$ or $h_m(X)=X_j X_k$ allows us to augment the inputs with polynomial terms to achieve higher-order Taylor expansions.  \n",
    "Note, however, that the number of variables grows exponentially in the degrees of the polynomial. A full quadratic model in $p$ variables requires $O(p^2)$ square and corss-product terms, or more generally $O(p^d)$ for a degree-$d$ polynomial.\n",
    "* $h_m(X)=\\log(X_j)$, $\\sqrt{X_j}$, $\\cdots$ permits other nonlinear transformations of single inputs. More generally one can use similar functions involving several inputs, such as $h_m(X)=\\|X\\|$.\n",
    "* $h_m(X)=I(L_m \\le X_k \\lt U_m)$, an indicator for a region of $X_k$. By breaking the range of $X_k$ up into $M_k$ such nonoverlapping regions results in a model with a piecewise constant contribution for $X_k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview\n",
    "\n",
    "* _Piecewise-polynomials_ and _splines_ allow for local polynomial representations.\n",
    "* _wavelet_ bases produce a _dictionary_ $\\mathcal{D}$ consisting of typically a very large number $\\left|\\mathcal{D}\\right|$ of basis functions, far more than we can afford to fit to our data.  \n",
    "Along with the dictionary we require a method for controlling the complexity of out model, using basis functions from the dictionary. These are three common approaches.\n",
    "  * Restriction methods, where we decide before-hand to limit the class of functions. Additivity is an example, where we assume that our model has the form\n",
    "\\begin{equation}\n",
    "f(X) = \\sum_{j=1}^p f_j(X_j) = \\sum_{j=1}^p \\sum_{m=1}^{M-j} \\beta_{jm} h_{jm}(X_j).\n",
    "\\end{equation}\n",
    "    The size of the model is limited by the number of basis functions $M_j$ used for each component function $f_j$.\n",
    "  * Selection methods, which adaptively scan the dictionary and include only those basis functions $h_m$ that contribute significantly to the fit of the model. Here the variable selection techniques discussed in Chapter 3 are useful. The stagewise greedy approaches such as CART, MARS and boosting fall into this category as well.\n",
    "  * Regularization methods where we use the entire dictionary but restrict the coefficients. Ridge regression is a simple example of a regularization approach, while the lasso is both a regularization and selection method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
