{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 5.8. Regularization and Reproducing Kernel Hilbert Spaces\n",
    "## $\\S$ 5.8.1. Spaces of Functions Generated by Kernels\n",
    "\n",
    "An important subclass of problems of the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{f\\in\\mathcal{H}} \\left[ \\sum_{i=1}^N L(y_i, f(x_i)) + \\lambda J(f) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "are generated by a positive definite kernel $K(x,y)$, and the corresponding space of functions $\\mathcal{H}_K$ is called a _reproducing kernel Hilbert space_ (RKHS). The penalty functional $J$ is defined in terms of the kernel as well. \n",
    "\n",
    "We give a brief and simplified introduction to this class of models, and below are the references:\n",
    "* Wahba (1990)\n",
    "* Girosi et al. (1995)\n",
    "* Evgeniou et al. (2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions and assumptions\n",
    "\n",
    "Let $x, y \\in\\mathbb{R}^p$ and a kernel $K$ be given.\n",
    "\n",
    "We consider the space $\\mathcal{H}_K$ of functions generated by the linear span of\n",
    "\n",
    "\\begin{equation}\n",
    "\\lbrace K(\\cdot, y), y\\in\\mathbb{R}^p \\rbrace;\n",
    "\\end{equation}\n",
    "\n",
    "i.e., arbitrary linear combinations of the form\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\sum_m \\alpha_m K(x, y_m).\n",
    "\\end{equation}\n",
    "\n",
    "Suppose that $K$ has an eigen-expansion\n",
    "\n",
    "\\begin{equation}\n",
    "K(x,y) = \\sum_{i=1}^\\infty \\gamma_i \\phi_i(x)\\phi_i(x),\n",
    "\\end{equation}\n",
    "\n",
    "with $\\gamma_i \\ge 0$, $\\sum_{i=1}^\\infty \\gamma_i^2 \\lt \\infty$.\n",
    "\n",
    "Elements of $\\mathcal{H}_K$ have an expansion in terms of these eigen-functions,\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\sum_{i=1}^\\infty c_i \\phi_i(x),\n",
    "\\end{equation}\n",
    "\n",
    "with the constraints that\n",
    "\n",
    "\\begin{equation}\n",
    "\\|f\\|_{\\mathcal{H}_K}^2 := \\sum_{i=1}^\\infty c_i^2/\\gamma_i \\lt \\infty,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\|f\\|_{\\mathcal{H}_K}$ is the norm induced by $K$.\n",
    "\n",
    "The penalty functional $J$ for the space $\\mathcal{H}_K$ is defined to be the squared norm\n",
    "\n",
    "\\begin{equation}\n",
    "J(f) = \\|f\\|_{\\mathcal{H}_K}^2.\n",
    "\\end{equation}\n",
    "\n",
    "The quantity $J(f)$ can be interpreted as a generalized ridge penalty, where functions with large eigenvalues in the expansion get penalized less, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "Rewritting the minimization problem, we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{f\\in\\mathcal{H}_K} \\left[ \\sum_{i=1}^N L(y_i, f(x_i)) + \\lambda \\|f\\|_{\\mathcal{H}_K}^2\\right],\n",
    "\\end{equation}\n",
    "\n",
    "or equivalently\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\lbrace c_j\\rbrace_1^\\infty} \\left[ \\sum_{i=1}^N L\\left(y_i, \\sum_{j=1}^\\infty c_j\\phi_j(x)\\right) + \\lambda \\sum_{j=1}^\\infty \\frac{c_j^2}{\\gamma_j} \\right].\n",
    "\\end{equation}\n",
    "\n",
    "It can be shown (Wahba (1990), see also Exercise 5.15) that the solution is finite-dimensional, and has the form\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\sum_{i=1}^N \\alpha_i K(x,x_i)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducing property of $\\mathcal{H}_K$\n",
    "\n",
    "The basis function\n",
    "\n",
    "\\begin{equation}\n",
    "h_i(x) = K(x,x_i)\n",
    "\\end{equation}\n",
    "\n",
    "is known as the _representer of evaluation_ at $x_i$ in $\\mathcal{H}_K$, since for $f\\in\\mathcal{H}_K$, it is easily seen that\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle K(\\cdot,x_i), f\\rangle_{\\mathcal{H}_K} = f(x_i).\n",
    "\\end{equation}\n",
    "\n",
    "Similarly,\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle K(\\cdot,x_i), K(\\cdot,x_j)\\rangle_{\\mathcal{H}_K} = K(x_i,x_j)\n",
    "\\end{equation}\n",
    "\n",
    "(a.k.a. the _reproducing_ property of $\\mathcal{H}_K$), and hence\n",
    "\n",
    "\\begin{equation}\n",
    "J(f) = \\sum_{i=1}^N\\sum_{j=1}^N K(x_i,x_j)\\alpha_j \\alpha_j\n",
    "\\end{equation}\n",
    "\n",
    "for $f(x) = \\sum_{i=1}^N \\alpha_i K(x, x_i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Down to the finite-dimension\n",
    "\n",
    "Then the minimization problem reduces to a finite-dimensional criterion\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_\\alpha L(\\mathbf{y}, \\mathbf{K\\alpha}) + \\lambda\\mathbf{\\alpha}^T\\mathbf{K\\alpha},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{K}$ is the $N \\times N$ matrix with $ij$th entry $K(x_i,x_j)$ and so on. Simple numerical algorithm can be used to optimize this problem.\n",
    "\n",
    "This phenomenon, whereby the infinite-dimensional problem reduces to a finite-dimensional optimization problem, has been dubbed the _kernel property_ in the literature on support-vector machine (Chapter 12)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian interpretation\n",
    "\n",
    "$f$ is interpreted as a realization of a zero-mean stationary Gausssian process, with prior covariance function $K$. Then the eigen-decomposition produces a series of orthogonal eigen-functions $\\phi_j(x)$ with associated variances $\\gamma_j$.\n",
    "\n",
    "The typical scenario is that\n",
    "* \"smooth\" functions $\\phi_j$ have large prior variances,\n",
    "* while \"rough\" $\\phi_j$ have small prior variances.\n",
    "\n",
    "The penalty $J$ is the contribution of the prior to the joint likelihood, and penalizes more those components with smaller prior variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More general approach\n",
    "\n",
    "For simplicity we have dealt with the case here where all members of $\\mathcal{H}$ are penalized, as in the above minimization problem.\n",
    "\n",
    "More generally, there may be some components in $\\mathcal{H}$ that we wish to leave alone, such as the linear functions for cubic smoothing splines in $\\S$ 5.4. The multidimensional thin-plate splines of $\\S$ 5.7 and tensor product splines fall into this category as well.\n",
    "\n",
    "In these cases there is a more convenient representation\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{H} = \\mathcal{H}_0 \\oplus \\mathcal{H}_1,\n",
    "\\end{equation}\n",
    "\n",
    "with the _null space_ $\\mathcal{H}_0$ consisting of, e.g., low degree polynomials in $x$ that do not get penalized. Then the penalty becomes\n",
    "\n",
    "\\begin{equation}\n",
    "J(f) = \\|P_1 f\\|,\n",
    "\\end{equation}\n",
    "\n",
    "where $P_1$ is the orthogonal projection of $f$ onto $\\mathcal{H}_1$.\n",
    "\n",
    "The solution has the form\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\sum_{j=1}^M \\beta_j h_j(x) + \\sum_{i=1}^N \\alpha_i K(x,x_i),\n",
    "\\end{equation}\n",
    "\n",
    "where the first term represents an expansion in $\\mathcal{H}_0$.\n",
    "\n",
    "From a Bayesian perspective, the coefficients of components in $\\mathcal{H}_0$ have improper priors, with infinite variance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
