{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 5.8.2. Examples of RKHS\n",
    "\n",
    "The machinary of the RKHS is driven by the choice of the kernal $K$ and the loss function $L$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Squared-error loss\n",
    "We consider first regression using squared-error loss. In this case the regularization problem\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{f\\in\\mathcal{H}} \\left[ \\sum_{i=1}^N L(y_i,f(x_i)) + \\lambda J(f) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "specializes to penalized least squares, and the solution can be characterized in two equivalent ways corresponding to\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\lbrace c_j\\rbrace_1^\\infty} \\sum_{i=1}^N \\left( y_i - \\sum_{j=1}^\\infty c_j\\phi_j(x_i) \\right)^2 + \\lambda \\sum_{j=1}^\\infty \\frac{c_j^2}{\\gamma_j^2}\n",
    "\\end{equation}\n",
    "\n",
    "an infinite-dimensional, generalized ridge regression problem, or a finite-dimensional matrix expression\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_\\mathbf{\\alpha} \\left(\\mathbf{y}-\\mathbf{K}\\boldsymbol\\alpha\\right)^T\\left(\\mathbf{y}-\\mathbf{K}\\boldsymbol\\alpha\\right) + \\lambda \\boldsymbol\\alpha^T\\mathbf{K}\\boldsymbol\\alpha.\n",
    "\\end{equation}\n",
    "\n",
    "The solution for $\\boldsymbol\\alpha$ is obtained simply as\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\boldsymbol\\alpha} = (\\mathbf{K} + \\lambda\\mathbf{I})^{-1}\\mathbf{y},\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x) = \\sum_{j=1}^N \\hat\\alpha_j K(x,x_j).\n",
    "\\end{equation}\n",
    "\n",
    "The vector of $N$ fitted values is given by\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{f}} &= \\mathbf{K}\\hat{\\boldsymbol\\alpha} \\\\\n",
    "&= \\mathbf{K}(\\mathbf{K}+\\lambda\\mathbf{I})^{-1}\\mathbf{y} \\\\\n",
    "&= (\\mathbf{I} + \\lambda\\mathbf{K}^{-1})^{-1}\\mathbf{y}.\n",
    "\\end{align}\n",
    "\n",
    "The estimate $\\hat{f}$ also arises as the _kriging_ estimate of a Gaussian random field in spatial statistics (Cressie, 1993). Compare also this $\\hat{\\mathbf{f}}$ with the smoothing spline fit on page 154."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalized Polynomial Regression\n",
    "\n",
    "(Vapnik, 1996) For $x,y\\in\\mathbb{R}^p$, the kernel\n",
    "\n",
    "\\begin{equation}\n",
    "K(x,y) = \\left( \\langle x,y\\rangle + 1 \\right)^d\n",
    "\\end{equation}\n",
    "\n",
    "has $M = \\left( \\begin{smallmatrix}p+d \\\\ d \\end{smallmatrix} \\right)$ eigen-functions that sapn the space of polynomials in $\\mathbb{R}^p$ of total degree $d$. For example, with $p=2$ and $d=2$, $M=6$ and\n",
    "\n",
    "\\begin{align}\n",
    "K(x,y) &= 1 + 2x_1 y_1 + 2x_2 y_2 + x_1^2y_1^2 + x_2^2 y_2^2 + 2x_1x_2y_1y_2 \\\\\n",
    "&= \\sum_{m=1}^M h_m(x)h_m(y)\n",
    "\\end{align}\n",
    "\n",
    "with\n",
    "\n",
    "\\begin{equation}\n",
    "h(x)^T = (1, \\sqrt{2}x_1, \\sqrt{2}x_2, x_1^2, x_2^2, \\sqrt{2}x_1x_2).\n",
    "\\end{equation}\n",
    "\n",
    "One can represent $h$ in terms of the $M$ orthogonal eigen-functions and eigenvalues of $K$,\n",
    "\n",
    "\\begin{equation}\n",
    "h(x) = \\mathbf{VD}_\\gamma^\\frac{1}{2} \\phi(x),\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "* $\\mathbf{D}_\\gamma = \\text{diag}(\\gamma_1,\\cdots,\\gamma_M)$, and\n",
    "* $\\mathbf{V}$ is $M \\times M$ orthogonal.\n",
    "\n",
    "Suppose we wish to solve the penalized polynomial regression problem\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\lbrace \\beta_m \\rbrace_1^M} \\sum_{i=1}^N \\left( y_i - \\sum_{m=1}^M \\beta_m h_m(x_i) \\right)^2 + \\lambda \\sum_{m=1}^M \\beta_m^2.\n",
    "\\end{equation}\n",
    "\n",
    "The number of basis functions $M = \\left( \\begin{smallmatrix}p+d \\\\ d \\end{smallmatrix} \\right)$ can be very large, often much larger than $N$. The solution\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\boldsymbol\\alpha} = (\\mathbf{K} + \\lambda\\mathbf{I})^{-1}\\mathbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "tells us that if we use the kernel representation for the solution function, we have only to evaluate the kernel $N^2$ times, and can compute the solution in $O(N^3)$ operations.\n",
    "\n",
    "This simplicity is not without implications. Each of the polynomials $h_m$ inherits a scaling factor from the particular form of $K$, which has a bearing on the impact of the penalty $\\|\\boldsymbol\\beta\\|^2$. We elaborate on this in the next section.\n",
    "\n",
    "The choice of the kernel $K(x,y) = \\left( \\langle x,y\\rangle + 1 \\right)^d$ is due to the fact that it represents an expansion of polynomials and can conveniently compute high-dimensional inner products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Radial Basis Functions\n",
    "\n",
    "In this example the kernel is chosen because of its functional form in the representation\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\sum_{j=1}^N \\alpha_j K(x,x_j).\n",
    "\\end{equation}\n",
    "\n",
    "The Gaussian kernel\n",
    "\n",
    "\\begin{equation}\n",
    "K(x,y) = \\exp\\left({-\\nu\\|x-y\\|^2}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "along with the squared-error loss, for example, leads to a regression model that is an expansion in Gaussian radial basis functions,\n",
    "\n",
    "\\begin{equation}\n",
    "k_m(x) = \\exp\\left({-\\nu\\|x-x_m\\|^2}\\right), \\text{ for } m=1,\\cdots, N,\n",
    "\\end{equation}\n",
    "\n",
    "each one centered at one of the training feature vectors $x_m$. The coefficients are estimated using the finite-dimensional problem\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_\\boldsymbol\\alpha \\left(\\mathbf{y}-\\mathbf{K}\\boldsymbol\\alpha\\right)^T\\left(\\mathbf{y}-\\mathbf{K}\\boldsymbol\\alpha\\right) + \\lambda \\boldsymbol\\alpha^T\\mathbf{K}\\boldsymbol\\alpha.\n",
    "\\end{equation}\n",
    "\n",
    "FIGURE 5.14 illustrates the implicit feature space for the radial kernel with $x\\in\\mathbb{R}$. We computed the $200\\times200$ kernel matrix $\\mathbf{K}$, and its eigen-decomposition\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{K} = \\mathbf{\\Phi D}_\\gamma\\mathbf{\\Phi}^T.\n",
    "\\end{equation}\n",
    "\n",
    "We can think of the columns of $\\mathbf{\\Phi}$ and the corresponding eigenvalues of $\\mathbf{D}_\\gamma$ as empirical estimates of the eigen expansion\n",
    "\n",
    "\\begin{equation}\n",
    "K(x,y) = \\sum_{i=1}^\\infty \\gamma_i \\phi_i(x)\\phi_i(y)\n",
    "\\end{equation}\n",
    "\n",
    "Although the eigenvectors are discrete, we can represent them as functions on $\\mathbb{R}$ (Exercise 5.17).\n",
    "\n",
    "FIGURE 5.15 shows the largest 50 eigenvalues of $\\mathbf{K}$. The leading eigen-functions are smooth and get wigglier so forth. This brings life the penalty, where we see the coefficients of higher-order functions get penalized more than lower-order ones. The right panel in FIGURE 5.14 shows the corresponding _feature space_ representation of the eigen-functions\n",
    "\n",
    "\\begin{equation}\n",
    "h_l(x) = \\sqrt{\\hat\\gamma_l} \\hat\\phi_l(x), \\text{ for } l = 1,\\cdots, N.\n",
    "\\end{equation}\n",
    "\n",
    "Note that\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle h(x_i), h(x_j) \\rangle = K(x_i, x_j).\n",
    "\\end{equation}\n",
    "\n",
    "The scaling by the eigenvalues quickly shrinks most of the functions down to zero, leaving an effective dimension of about 12 in this case. The corresponding optimization problem is a standard ridge regression. So although in principle the implicit feature space is infinit dimensional, the effective dimension is dramatically lower because of the relative amounts of shrinkage applied to each basis function.\n",
    "\n",
    "The kernel scale parameter $\\nu$ plays a role here as well; larger $\\nu$ implies more local $k_m$ functions, and increases the effective dimension of the feature space. See Hastie and Zhu (2006) for more details.\n",
    "\n",
    "It is also known (Girosi et al., 1995) that a thin-plate spline ($\\S$ 5.7) is an expansion in radial basis functions, generated by the kernel\n",
    "\n",
    "\\begin{equation}\n",
    "K(x,y) = \\|x-y\\|^2\\log\\left(\\|x-y\\|\\right).\n",
    "\\end{equation}\n",
    "\n",
    "Radial basis functions are discussed in $\\S$ 6.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Classifier\n",
    "\n",
    "The support vector machines of Chapter 12 for a two-class classification problem have the form\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\alpha_0 + \\sum_{i=1}^N \\alpha_i K(x,x_i),\n",
    "\\end{equation}\n",
    "\n",
    "where the parameters are chosen to minimize\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\alpha_0, \\boldsymbol\\alpha} \\left( \\sum_{i=1}^N\\left[ 1-y_if(x_i)\\right]_+ + \\frac{\\lambda}{2} \\boldsymbol\\alpha^T\\mathbf{K}\\boldsymbol\\alpha \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where\n",
    "* $y_i\\in[-1,1]$, and\n",
    "* $[z]_+$ denotes the positive part of $z$.\n",
    "\n",
    "This can be viewed as a quadratic optimization problem with linear constraints, and requires a quadratic programming algorithm for its solution.\n",
    "\n",
    "The name _support vector_ arises from the fact that typically many of the $\\hat\\alpha_i = 0$, due to the piecewise-zero natrue of the loss function, and so $\\hat{f}$ is an expansion in a subset of the $K(\\cdot, x_i)$.\n",
    "\n",
    "See $\\S$ 12.3.3 for more details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
