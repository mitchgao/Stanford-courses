{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 5.7. Multidimensional Splines\n",
    "\n",
    "So far we have focused on one-dimensional spline models. Each of the approaches have multidimensional analoigs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor product basis\n",
    "\n",
    "Suppose $X\\in\\mathbb{R}^2$, and we have\n",
    "* a basis of functions $h_{1k}(X_1)$, for $k=1,\\cdots,M_1$\n",
    "  for representing functions of coordinate $X_1$,\n",
    "* a set of $M_2$ functions $h_{2k}(X_2)$ for coordinate $X_2$, likewise.\n",
    "\n",
    "Then the $M_1 \\times M_2$ dimensional _tensor product basis_ defined by\n",
    "\n",
    "\\begin{equation}\n",
    "g_{jk}(X) = h_{1j}(X_1) h_{2k}(X_2), \\text{ for } j=1,\\cdots,M_1 \\text{ and } k=1,\\cdots,M_2\n",
    "\\end{equation}\n",
    "\n",
    "can be used for representing a two-dimensional function:\n",
    "\n",
    "\\begin{equation}\n",
    "g(X) = \\sum_{j=1}^{M_1}\\sum_{k=1}^{M_2} \\theta_{jk} g_{jk}(X).\n",
    "\\end{equation}\n",
    "\n",
    "FIGURE 5.10 illustrates a tensor product basis using B-splines.\n",
    "\n",
    "The coefficients can be fit by least squares, as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond 2D\n",
    "\n",
    "This can be generalized to $d$ dimensions, but note that the dimension of the basis grow exponentially fast -- yet another manifestation of the curse of dimensionality.\n",
    "\n",
    "The MARS procedure discussed in Chapter 9 is a greedy forward algorithm for including only thos tensor products that are deemed necessary by least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive vs. tensor product splines\n",
    "\n",
    "FIGURE 5.11 illustrates the difference between additive and tensor product (natural) splines on the simulated classification example from Chapter 2.\n",
    "\n",
    "A logistic regression model\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{logit}\\left[ \\text{Pr}(T|x) \\right] = h(x)^T\\theta\n",
    "\\end{equation}\n",
    "\n",
    "is fit to the binary response, and the estimated decision boundary is the contour\n",
    "\n",
    "\\begin{equation}\n",
    "h(x)^T \\hat\\theta = 0.\n",
    "\\end{equation}\n",
    "\n",
    "The tensor product basis can achieve more flexibility at the decision boundary, but introduces some spurious structure along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under construction ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"FIGURE 5.11. The simulation example of FIGURE 2.1.\"\"\"\n",
    "print('Under construction ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing splines for higher dimension\n",
    "\n",
    "One-dimensional smoothing splines (via regularization) generalize to higher-dimensions as well.\n",
    "\n",
    "Suppose we have pairs $(y_i, x_i)$ with $x_i\\in\\mathbb{R}^d$, and we seek a $d$-dimensional regression function $f(x)$. The idea is to set up the problem\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_f \\sum_{i=1}^N \\left( y_i - f(x_i) \\right)^2 + \\lambda J[f],\n",
    "\\end{equation}\n",
    "\n",
    "where $J$ is an appropriate penalty functional for stabilizing a function $f$ in $\\mathbb{R}^d$. For example, a natural generalization of the one-dimensional roughness penalty for functions of $\\mathbb{R}^2$ is\n",
    "\n",
    "\\begin{equation}\n",
    "J[f] = \\int\\int_{\\mathbb{R}^2} \\left[ \\left( \\frac{\\partial^2f(x)}{\\partial x_1^2} \\right)^2 + 2\\left( \\frac{\\partial^2f(x)}{\\partial x_1 \\partial x_2} \\right)^2 + \\left( \\frac{\\partial^2f(x)}{\\partial x_2^2} \\right)^2\\right] dx_1dx_2.\n",
    "\\end{equation}\n",
    "\n",
    "Optimizing the above minimization with this penalty leads to a smooth two-dimensional surface, a.k.a. a thin-plate spline. It shares many properties with the one-dimensional cubic smoothing spline:\n",
    "* as $\\lambda\\rightarrow 0$, the solution approaches an interpolating function (the one with smallest penalty $J$);\n",
    "* as $\\lambda\\rightarrow\\infty$, the solution approaches the least squares plane;\n",
    "* for intermediate values of $\\lambda$, the solution can be represented as a linear expansion of basis functions, whose coefficients are obtained by a form of generalized ridge regression.\n",
    "\n",
    "The solution has the form\n",
    "\n",
    "\\begin{align}\n",
    "f(x) &= \\beta_0 + \\beta^Tx + \\sum_{j=1}^N \\alpha_j h_j(x), \\\\\n",
    "\\text{where } h_j(x) &= \\|x-x_j\\|^2\\log\\|x-x_j\\|.\n",
    "\\end{align}\n",
    "\n",
    "These $h_j$ are examples of _radial basis functions_, which are discussed in more detail in the next section.\n",
    "\n",
    "The coefficients are found by plugging the form to the above minimization problem, which reducees to a finite-dimensional penalized least squares problem. For the penalty to be finite, the coefficients $\\alpha_j$ have to satisfy a set of linear constraints (see Exercise 5.14).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybrid approaches for computational and conceptual simplicity\n",
    "\n",
    "Unlike one-dimensional smoothing splines, the computational complexity for thin-plate splines is $O(N^3)$, since there is not in general any sparse structure that can be exploited. However, as with univariate smoothing splines, we can get away with substantially less than the $N$ knots prescibed by the previous solution.\n",
    "\n",
    "In practice, it is usually sufficient to work with a lattice of knots covering the domain. The penalty is computed for the reduced expansion just as before. Using $K$ knots reduces the computations to $O(NK^2+K^3)$. FIGURE 5.12 shows the result of fitting a thin-plate spline to some heart disease risk factors, representing the surface as a contour plot. Note that $\\lambda$ was specified via $\\text{df}_\\lambda = \\text{trace}(S_\\lambda) =15$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under construction ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"FIGURE 5.12. A thin-plate spline fit to the heart disease data, displayed as a contour plot.\n",
    "Indicated are the location of te input features, as well as the knots used in the fit.\n",
    "\n",
    "The response is `systolic blood pressure, modeled as a function of `age` and `obesity`.\n",
    "Care should be taken to use knots from the lattice inside the convex hull of the data (red),\n",
    "and ignore those outside (green).\n",
    "\"\"\"\n",
    "print('Under construction ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalized expansion and additive spline models\n",
    "\n",
    "More generally one can represent $f\\in\\mathbb{R}^d$ as an expansion in any arbitrarily large collection of basis functions, and control the complexity by applying a regularizer. For example, we can construct a basis by forming the tensor products of all pairs of univariate smoothing-spline basis function. This leads to an exponential growth in basis functions as the dimension increases, and typically we have to reduce the number of functions per coordinate accordingly.\n",
    "\n",
    "The additive spline models (discussed in Chapter 9) are a restricted class of multidimensional splines. They can be represented in this general formulation as well; e.g., there exists a penalty $J[f]$ that guarantees that the solution has the form\n",
    "\n",
    "\\begin{equation}\n",
    "f(X) = \\alpha + f_1(X_1) + \\cdots + f_d(X_d),\n",
    "\\end{equation}\n",
    "  \n",
    "and that each of functions $f_j$ are univariate splines. In this case the penalty is somewhat degenerate, and it is more natural to _assume_ that $f$ is additive, and then simply impose an additional penalty on each of the component functions:\n",
    "\n",
    "\\begin{align}\n",
    "J[f] &= J(f_1 + f_2 + \\cdots + f_d) \\\\\n",
    "&= \\sum_{j=1}^d \\int f_j''(t_j)^2 dt.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA spline decomposition for additive spline models\n",
    "\n",
    "These are naturally extended to ANOVA spline decompositions,\n",
    "\n",
    "\\begin{equation}\n",
    "f(X) = \\alpha + \\sum_j f_j(X_j) + \\sum_{j<k} f_{jk}(X_j,X_k) + \\cdots,\n",
    "\\end{equation}\n",
    "\n",
    "where each of the components are splines of the required dimension.\n",
    "\n",
    "There are many choices to be made:\n",
    "* The maximum order of interaction -- we have shown up to order 2 above.\n",
    "* Which terms to include -- not all main effects and interactions are necessarily needed.\n",
    "* What representation to use -- some choices are:\n",
    "  * Regression splines with a relatively small number of basis functions per coordinate, and their tensor products for interactions;\n",
    "  * a complete basis as in smoothing splines, and include appropriate regularizers for each term in the expansion.\n",
    "  \n",
    "In many cases when the number of potential dimensions (features) is large, automatic methods are more desirable. The MARS and MART procedures (Chapter 9 and 10, respectively) both fall into this category."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
