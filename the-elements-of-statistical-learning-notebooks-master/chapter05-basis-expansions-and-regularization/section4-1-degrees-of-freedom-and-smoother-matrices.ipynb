{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 5.4.1. Degrees of Freedom and Smoother Matrices\n",
    "\n",
    "How is $\\lambda$ chosen for the smoothing spline? Later in this chapter automatic methods such as cross-validation is described. In this section we discuss intuitive ways of prespecifying the amount of smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear smoother and smoother matrix\n",
    "\n",
    "A smoothing spline with prechosen $\\lambda$ is an example of a _linear smoonther_ (as in linear operator), because the estimated parameters in\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat\\theta = \\left( \\mathbf{N}^T\\mathbf{N} + \\lambda\\mathbf{\\Omega}_N \\right)^{-1}\\mathbf{N}^T\\mathbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "are a linear combination of the $y_i$.\n",
    "\n",
    "Denote by $\\hat{\\mathbf{f}}$ the $N$-vector of fitted values $\\hat{f}(x_i)$ at the training predictors $x_i$. Then\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{f}} &= \\mathbf{N} \\left( \\mathbf{N}^T\\mathbf{N} + \\lambda\\mathbf{\\Omega}_N \\right)^{-1}\\mathbf{N}^T\\mathbf{y} \\\\\n",
    "&= \\mathbf{S}_\\lambda \\mathbf{y}.\n",
    "\\end{align}\n",
    "\n",
    "Again the fit is linear in $\\mathbf{y}$, and the finite linear operator $\\mathbf{S}_\\lambda$ is known as the _smoother matrix_. One consequence of this linearity is that the recipe for producing $\\hat{\\mathbf{f}}$ from $\\mathbf{y}$ does not depend on $\\mathbf{y}$ itself; $\\mathbf{S}_\\lambda$ depends on only on $x_i$ and $\\lambda$.\n",
    "\n",
    "Linear operators are familiar in more traditional least squares fitting as well. Suppose $\\mathbf{B}_\\xi$ is a $N \\times M$ matrix of $M$ cubic-spline basis functions evaluated at the $N$ training points $x_i$, with know sequence $\\xi$, and $M \\ll N$. Then the vector of fitted spline values is given by\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\mathbf{f}} &= \\mathbf{B}_\\xi \\left( \\mathbf{B}_\\xi^T\\mathbf{B}_\\xi \\right)^{-1} \\mathbf{B}_\\xi^T\\mathbf{y} \\\\\n",
    "&= \\mathbf{H}_\\xi\\mathbf{y}.\n",
    "\\end{align}\n",
    "\n",
    "Here the linear operator $\\mathbf{H}_\\xi$ is a projection operator a.k.a. the _hat_ matrix in statistics. There are some important similiarities and differences between $\\mathbf{H}_\\xi$ and $\\mathbf{S}_\\lambda$:\n",
    "\n",
    "* Both are symmetric, positive semidefinite matrices.\n",
    "* $\\mathbf{H}_\\xi\\mathbf{H}_\\xi = \\mathbf{H}_\\xi$ (idempotent), while $\\mathbf{S}_\\lambda\\mathbf{S}_\\lambda \\preceq \\mathbf{S}_\\lambda$ (meaning that the RHS exceeeds the LHS by a positive semidefinite matrix). This is a consequence of the _shrinking_ nature of $\\mathbf{S}_\\lambda$, which we discuss further below.\n",
    "* $\\mathbf{H}_\\xi$ has rank $M$, while $\\mathbf{S}_\\lambda$ has $N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effective degrees of freedom of smoothers\n",
    "\n",
    "The expression $M = \\text{trace}(\\mathbf{H}_\\xi)$ gives the dimension of the projection space, which is also the number of basis functions, and hence the number of parameters involved in the fit. By analogy we define the _effective degrees of freedom_ of a smoothing spline to be\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{df}_\\lambda = \\text{trace}(\\mathbf{S}_\\lambda).\n",
    "\\end{equation}\n",
    "\n",
    "This very useful definition allows us a more intuitive way to parametrize the smoothing spline, and indeed many other smoothers as well, in a consistent fashion.\n",
    "\n",
    "For example in FIGURE 5.6, we specified $\\text{df}_\\lambda = 12$ for each of the curves, and the corresponding $\\lambda \\approx 0.00022$ was derived numerically by solving $\\text{trace}(\\mathbf{S}_\\lambda) =12$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigen-representation of the smoother matrix\n",
    "\n",
    "Since $\\mathbf{S}_\\lambda$ is symmetric (and positive semidefinite), it has a real eigen-decomposition. Before we proceed, it is convenient to write $\\mathbf{S}_\\lambda$ in the _Reinsch_ form (Exercise 5.9)\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{S}_\\lambda = \\left( \\mathbf{I} + \\lambda\\mathbf{K} \\right)^{-1},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{K}$ does not depend on $\\lambda$.\n",
    "\n",
    "Since $\\hat{\\mathbf{f}} = \\mathbf{S}_\\lambda \\mathbf{y}$ solves\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_\\mathbf{f} (\\mathbf{y}-\\mathbf{f})^T(\\mathbf{y}-\\mathbf{f}) + \\lambda \\mathbf{f}^T\\mathbf{K}\\mathbf{f},\n",
    "\\end{equation}\n",
    "\n",
    "$\\mathbf{K}$ is known as the _penalty matrix_, and indeed a quadratic form in $\\mathbf{K}$ has a representation in terms of a weighted sum of squared (divided) second differences. The eigen-decomposition of $\\mathbf{S}_\\lambda$ is\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{S}_\\lambda = \\sum_{k=1}^N \\rho_k(\\lambda) \\mathbf{u}_k\\mathbf{u}_k^T,\n",
    "\\end{equation}\n",
    "\n",
    "with\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho_k(\\lambda) = \\frac{1}{1+\\lambda d_k},\n",
    "\\end{equation}\n",
    "\n",
    "and $d_k$ the corresponding eigenvalue of $\\mathbf{K}$.\n",
    "\n",
    "Some highlights:\n",
    "\n",
    "* The eigenvectors are not affectec by changes in $\\lambda$, and hence the whole family of smoothing splines (for a particular sequence $mathbf{x}$) indexed by $\\lambda$ have the same eigenvectors.\n",
    "* $\\mathbf{S}_\\lambda\\mathbf{y} = \\sum_{k=1}^N u_k \\rho_k(\\lambda) \\langle \\mathbf{u}_k, \\mathbf{y} \\rangle$, and hence the smoothing spline operates by\n",
    "  * decomposing $\\mathbf{y}$ w.r.t. the (complete) basis $\\lbrace \\mathbf{u_k} \\rbrace$, and\n",
    "  * differentially shrinking the contributions using $\\rho_k(\\lambda)$.\n",
    "\n",
    "  This is to be contrasted with a basis-regression method, where the components are either left alone, or shrunk to zero -- that is, a projection matrix such as $\\mathbf{H}_\\xi$ has $M$ eigenvalues equal to 1, and the rest are 0. For this reason smoothing splines are referred to as _shrinking_ smoothers, while regression splines are _projection_ smoothers (see FIGURE 3.17).\n",
    "* The sequqnce of eigenvectors $\\mathbf{u}_k$, ordered by decreasing $\\rho_k(\\lambda)$, appear to increase in complexity.  \n",
    "  Since $\\mathbf{S}_\\lambda \\mathbf{u}_k = \\rho_k(\\lambda) \\mathbf{u}_k$, we see how each of the eigenvectors themselves are shrunk by the smoothing spline: The higher the complexity, the more they are shrunk.\n",
    "* The first two eigenvalues are _always_ one, and they correspond to the two-dimensional eigenspace of functions linear in $x$ (Exercise 5.11), which are never shrunk.\n",
    "* The eigenvalues $\\rho_k(\\lambda)$ are an inverse function of the eigenvalues of the penalty matrix $\\mathbf{K}$, moderated by $\\lambda$.\n",
    "  $d_1 = d_2 = 0$ and again linear functions are not penalized.\n",
    "* One can reparametrize the smoothing spline using the basis vectors $\\mathbf{u}_k$ (the _Demmler-Reinsch_ basis). In this case the smoothing spline solves\n",
    "\\begin{equation}\n",
    "\\min_\\mathbf{\\theta} \\|\\mathbf{y} - \\mathbf{U\\theta}\\|^2 + \\lambda\\mathbf{\\theta}^T\\mathbf{D\\theta},\n",
    "\\end{equation}\n",
    "  where $\\mathbf{U}$ has columns $\\mathbf{u}_k$ and $\\mathbf{D}$ is diagonal with elements $d_k$.\n",
    "* $\\text{df}_\\lambda = \\text{trace}(\\mathbf{S}_\\lambda) = \\sum_{k=1}^N \\rho_k(\\lambda)$. For projection smoothers, all the eigenvalues are 1, each one corresponding to a dimension of the projection subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under construction ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"FIGURE 5.8. The smoother matrix\n",
    "The banded nature of this representation suggests that a smoothing spline is a local fitting\n",
    "method, much like the locally weighted regression procedures in Chapter 6.\n",
    "\"\"\"\n",
    "print('Under construction ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The right panel of FIGURE 5.8 shows in detail selected rows of $\\mathbf{S}, which we call the _equivalent kernels_.\n",
    "\n",
    "Additional notes:\n",
    "* As $\\lambda \\rightarrow 0$, $\\text{df}_\\lambda \\rightarrow N$ and $\\mathbf{S}_\\lambda \\rightarrow \\mathbf{I}$.\n",
    "* As $\\lambda \\rightarrow \\infty$, $\\text{df}_\\lambda \\rightarrow 2$ and $\\mathbf{S}_\\lambda \\rightarrow \\mathbf{H}$, the hat matrix for linear regression on $x$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
