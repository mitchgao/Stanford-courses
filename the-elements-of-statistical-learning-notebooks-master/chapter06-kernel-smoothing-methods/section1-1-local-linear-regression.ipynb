{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 6.1.1. Local Linear Regression\n",
    "\n",
    "The smooth kernel fit still has a problems. Locally-weighted averages can be badly biased on the boundaries of the domain, because of the asymmetry of the kernel in that region, as exhibited in FIGURE 6.3 (left panel).\n",
    "\n",
    "By fitting straight lines rather than constants locally, we can remove this bias exactly to first order; see the right panel of FIGURE 6.3. Actually, this bias can be present in the interior of the domain as well, if the $X$ values are not equally spaced (for the same reasons, but usually less severe). Again locally weighted linear regression will make a first-order correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation\n",
    "\n",
    "Locally weighted regression solves a separate weighted least squares problem at each target point $x_0$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\alpha(x_0),\\beta(x_0)} \\sum_{i=1}^N K_\\lambda(x_0,x_i) \\left( y_i - \\alpha(x_0) - \\beta(x_0)x_i \\right)^2.\n",
    "\\end{equation}\n",
    "\n",
    "The estimate is then\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\hat\\alpha(x_0) + \\hat\\beta(x_0).\n",
    "\\end{equation}\n",
    "\n",
    "Notice that although we fit an entire linear model to the data in the region, we only use it to evaluate the fit at the single point $x_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix formulation and equivalent kernel\n",
    "\n",
    "Define\n",
    "* the vector-valued function $b(x)^T = (1, x)$,\n",
    "* the $N\\times2$ regression matrix $\\mathbf{B}$ with $i$th row $b(x_i)^T$, and\n",
    "* the $N\\times N$ diagonal matrix $\\mathbf{W}(x_0)$ with $i$th diagonal element $K_\\lambda(x_0,x_i)$.\n",
    "\n",
    "Then\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{f}(x_0) &= b(x_0)^T \\left( \\mathbf{B}^T\\mathbf{W}(x_0)\\mathbf{B} \\right)^{-1} \\mathbf{B}^T \\mathbf{W}(x_0) \\mathbf{y} \\\\\n",
    "&= \\sum_{i=1}^N l_i(x_0)y_i.\n",
    "\\end{align}\n",
    "\n",
    "Note that $l_i(x_0)$ do not involve $\\mathbf{y}$ and thus the estimate is _linear_ in $y_i$. These weights $l_i(x_0)$ combine the weighting kernel $K_\\lambda(x_0,\\cdot)$ and the least squares operations, and are sometimes referred to as the _equivalent kernel_.\n",
    "\n",
    "FIGURE 6.4 illustrates the effect of local linear regression on the equivalent kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under construction ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"FIGURE 6.4. Equivalent kernel li(x0) for local regression\"\"\"\n",
    "print('Under construction ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic kernel carpentry\n",
    "\n",
    "Historically, the bias in the Nadaraya-Watson and other local average kernel methods were corrected by modifying the kernel. These modifications were based on theoretical asymptotic MSE considerations, and besides being tedious to implement, are only approximate for finite sample sizes.\n",
    "\n",
    "Local linear regression _automatically_ modfies the kernel to correct the bias _exactly_ to first order, a phenomenon dubbed as _automatic kernel carpentry_.\n",
    "\n",
    "Consider the following expansion for $\\text{E}\\hat{f}(x_0)$, using the linearity of local regression and a series expansion of the true function $f$ around $x_0$,\n",
    "\n",
    "\\begin{align}\n",
    "\\text{E}\\hat{f}(x_0) &= \\sum_{i=1}^N l_i(x_0)f(x_i) \\\\\n",
    "&= f(x_0)\\sum_{i=1}^N l_i(x_0) + f'(x_0)\\sum_{i=1}^N (x_i-x_0)l_i(x_0) + \\frac{f''(x_0)}2 \\sum_{i=1}^N \\sum_{i=1}^N (x_i-x_0)^2 l_i(x_0) + R,\n",
    "\\end{align}\n",
    "\n",
    "where the remainder term $R$ involves third- and higher-order derivatives of $f$, and is typically small under suitable smoothness assumptions. It can be shown (Exercise 6.2) that for local linear regression,\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^N l_i(x_0) = 1 \\text{ and } \\sum_{i=1}^N (x_i-x_0)l_i(x_0) = 0.\n",
    "\\end{equation}\n",
    "\n",
    "Hence\n",
    "\n",
    "\\begin{align}\n",
    "\\text{E}\\hat{f}(x_0) &= f(x_0)\\sum_{i=1}^N l_i(x_0) + f'(x_0)\\sum_{i=1}^N (x_i-x_0)l_i(x_0) + \\frac{f''(x_0)}2 \\sum_{i=1}^N \\sum_{i=1}^N (x_i-x_0)^2 l_i(x_0) + R \\\\\n",
    "&= f(x_0) + \\frac{f''(x_0)}2 \\sum_{i=1}^N \\sum_{i=1}^N (x_i-x_0)^2 l_i(x_0) + R,\n",
    "\\end{align}\n",
    "\n",
    "and the bias\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{E}\\hat{f}(x_0) - f(x_0) = \\frac{f''(x_0)}2 \\sum_{i=1}^N \\sum_{i=1}^N (x_i-x_0)^2 l_i(x_0) + R.\n",
    "\\end{equation}\n",
    "\n",
    "We see that it depends only on quadratic and higher-order terms in the expansion of $f$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
