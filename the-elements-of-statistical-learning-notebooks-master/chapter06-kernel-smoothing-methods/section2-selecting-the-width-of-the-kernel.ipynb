{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.2. Selecting the Width of the Kernel\n",
    "\n",
    "In each of the kernels $K_\\lambda$, $\\lambda$ is a parameter that controls its width:\n",
    "\n",
    "* For the Epanechnikov or tri-cube kernel with metric width, $\\lambda$ is the radius of the support region.\n",
    "* For the Gaussian kernel, $\\lambda$ is the standard deviation.\n",
    "* $\\lambda$ is the number $k$ of nearest neighbors in $k$-nearest neighborhoods, often expressed as a fraction or _span_ $k/N$ of the total training sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-variance tradeoff, again and again\n",
    "\n",
    "There is a natural bias-variance tradeoff as we change the width of the averaging window, which is most explicit for local averages:\n",
    "\n",
    "* If the window is narrow, $\\hat{f}(x_0)$ is an average of a small number of $y_i$ close to $x_0$, and its variance will be relatively large -- close to that of an individual $y_i$. The bias will tend to be small, again because each of the $\\text{E}(y_i) = f(x_i)$ should be close to $f(x_0)$.\n",
    "* If the window is wide, the variance of $\\hat{f}(x_0)$ will be small relative to the variance of any $y_i$, because of the effects of averaging. The bias will be higher, because we are now using observations $x_i$ further from $x_0$, and there is no quarantee that $f(x_i)$ will be close to $f(x_0)$.\n",
    "\n",
    "Similar arguments apply to local regression estimates, say local linear:\n",
    "* As the width goes to zero, the estimates approach a piecewise-linear function that interpolates the training data;\n",
    "* as the width gets infinitely large, the fit approaches the global linear least-squares fit to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discussion in Chapter 5 on selecting the regularization parameter for smoothing splines applies here, and will not be repeated.\n",
    "\n",
    "Local regression smoothers are linear estimators; the smoother matrix in\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{f}} = \\mathbf{S}_\\lambda\\mathbf{y}\n",
    "\\end{equation}\n",
    "\n",
    "is built up from the equivalent kernels ($\\S$ 6.1.1), and has $ij$th entry $\\{\\mathbf{S}_\\lambda\\}_{ij} = l_i(x_j)$.\n",
    "\n",
    "Leave-one-out cross-validation is particularly simple (Exercise 6.7), as is generalized cross-validation, $C_p$ (Exercise 6.10), and $k$-fold cross-validation. The effective degrees of freedom is again defined as $\\text{trace}(\\mathbf{S}_\\lambda)$, and can be used to calibrate the amount of smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIGURE 6.7 compares the equivalent kernels for a smoothing spline and local linear regression. The local regression smoother has a span of $40%$, which results in $\\text{df} = \\text{trace}(\\mathbf{S}_\\lambda) = 5.86$. The smoothing spline was calibrated to have the same $\\text{df}$, and their equivalent kernels are qualitatively quite similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under construction ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"FIGURE 6.7. Equivalent kernels for a local linear regreession smoother and\n",
    "a smoothing spline\"\"\"\n",
    "print('Under construction ...')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
