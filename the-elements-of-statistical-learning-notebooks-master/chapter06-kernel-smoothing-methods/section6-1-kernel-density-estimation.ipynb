{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 6.6.1. Kernel Density Estimation\n",
    "\n",
    "### Density estimation\n",
    "\n",
    "> Suppose we have a random sample $x_1,\\cdots,x_N$ drawn from a probability density $f_X(x)$, and we wish to estimate $f_X$ at a point $x_0$.\n",
    "\n",
    "For simplicity assume for now that $X\\in\\mathbb{R}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local estimate\n",
    "\n",
    "A natural local estimate has the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}_X(x_0) = \\frac{\\#x_i \\in \\mathcal{N}(x_0)}{N\\lambda},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathcal{N}$ is a small metric neighborhood around $x_0$ of width $\\lambda$.\n",
    "\n",
    "This estimate is bumpy, so..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate with kernels\n",
    "\n",
    "the smooth _Parzen_ estimate is preferred.\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\frac1{N\\lambda} \\sum_{i=1}^N K_\\lambda(x_0,x_i),\n",
    "\\end{equation}\n",
    "\n",
    "because it counts observations close to $x_0$ with weights that decrease with distance from $x_0$. In this case a popular choice for $K_\\lambda$ is the Gaussian kernel\n",
    "\n",
    "\\begin{equation}\n",
    "K_\\lambda(x_0,x) = \\phi\\left(\\frac{|x-x_0|}\\lambda\\right).\n",
    "\\end{equation}\n",
    "\n",
    "FIGURE 6.13 shows a Gaussian kernel density fit to the sample values for $\\textsf{systolic blood pressure}$ for the $\\textsf{CHD}$ group.\n",
    "\n",
    "Letting $\\phi_\\lambda$ denote the Gaussian density with mean zero and standard-deviation $\\lambda$, the Parzen estimate has the form\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{f}_X(x) &= \\frac1N \\sum_{i=1}^N \\phi_\\lambda(x-x_i) \\\\\n",
    "&= \\left( \\hat{F}\\star\\phi_\\lambda \\right)(x),\n",
    "\\end{align}\n",
    "\n",
    "the convolution of the sample empirical distribution $\\hat{F}$ with $\\phi_\\lambda$. The distribution $\\hat{F}(x)$ puts mass $1/N$ at each of the observed $x_i$, and is jumpy; in $\\hat{f}_X(x)$ we have smoothed $\\hat{F}$ by adding independent Gaussian noise to each observation $x_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution = local averaging\n",
    "\n",
    "The Parzen density estimate is the equivalent of the local average, and improvements have been proposed along the lines of local regression [on the log scale for densities; see Loader (1999)]. We will not pursue these here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In $\\mathbb{R}^p$\n",
    "\n",
    "The natural generalization of the Gaussian density estimate amounts to using the Gaussian product kernel,\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\frac1{N(2\\lambda^2\\pi)^{\\frac{p}2}} \\sum_{i=1}^N \\exp \\left\\{ -\\frac12 \\left( \\|x_i-x_0\\|/\\lambda \\right)^2 \\right\\}.\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
