{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.3. Local Regression in $\\mathbb{R}^p$\n",
    "\n",
    "Kernel smoothing and local regression generalize very naturally to two or more dimensions.\n",
    "\n",
    "* The Nadaraya-Watson kernel smoother fits a constant locally with weights supplied by a $p$-dimensional kernel.\n",
    "* Local linear regression will fit a hyperplane locally in $X$, by weighted least squares, with weights supplied by a $p$-dimensional kernel.  \n",
    "  It is simple to implement and is generally preferred to the local constant fit for its superior performacne on the boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation\n",
    "\n",
    "Let $b(X)$ be a vector of polynomial terms in $X$ of maximum degree $d$; e.g., with $d=1$ and $p=2$ we get\n",
    "\n",
    "\\begin{equation}\n",
    "b(X) = (1, X_1, X_2);\n",
    "\\end{equation}\n",
    "\n",
    "with $d=2$ we get\n",
    "\n",
    "\\begin{equation}\n",
    "b(X) = (1, X_1, X_2, X_1^2, X_2^2, X_1X_2);\n",
    "\\end{equation}\n",
    "\n",
    "and trivially with $d=0$ we get\n",
    "\n",
    "\\begin{equation}\n",
    "b(X) = 1.\n",
    "\\end{equation}\n",
    "\n",
    "At each $x_0 \\in \\mathbb{R}^p$ solve\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\beta(x_0)} \\sum_{i=1}^N K_\\lambda(x_0,x_i) \\left( y_i - b(x_i)^T\\beta(x_0) \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "to produce the fit\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = b(x_0)^T \\hat\\beta(x_0).\n",
    "\\end{equation}\n",
    "\n",
    "Typically the kernel will be a radial function, such as the radial Epanechnikov or tri-cube kernel\n",
    "\n",
    "\\begin{equation}\n",
    "K_\\lambda(x_0,x) = D\\left( \\frac{\\|x-x_0\\|}\\lambda \\right),\n",
    "\\end{equation}\n",
    "\n",
    "where $\\|\\cdot\\|$ is the Euclidean norm.\n",
    "\n",
    "Since the Euclidean norm depends on the units in each coordinate, it makes most sense to standardize each predictor, e.g., to unit standard deviation, prior to smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boundary problem gets serious with the curse of dimensionality\n",
    "\n",
    "While boundary effects are a problem in 1D smoothing, they are a much bigger problem in two or higher dimensions, since the fraction of points on the boundary is larger. In fact, one of the manifestations of the curse of dimensionality is that the fraction of points close to the boundary increases to one as the dimension grows.\n",
    "\n",
    "Directly modifying the kernel to accommodate two-dimensional boundaries becomes very messy, especially for irregular boundaries.\n",
    "\n",
    "Local polynomial regression seamlessly performs boundary correction to the desired order in any dimensions. FIGURE 6.8 illustrates local linear regression on some measurements from an astronomical study with an unusual predictor design (star-shaped). Here the boundary is extremely irregular, and the fitted surface must also interpolate over regions of increasing data sparsity as we approach the boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under construction ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"FIGURE 6.8 3D Galaxy data\"\"\"\n",
    "print('Under construction ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local regression becomes less useful in dimensions much higher than two or three. We have discussed in some detail the problems of the dimensionality, e.g., in Chapter 2. It is impossible to simultaneously maintain localness ($\\Rightarrow$ low bias) and a sizeable sample in the neighborhood ($\\Rightarrow$ low variance) as the dimension increases, without the total sample size increasing exponentially in $p$.\n",
    "\n",
    "Visualization of $\\hat{f}(X)$ also becomes difficult in higher dimensions, and this is often one of the primary goals of smoothing. Although the scatter-cloud and wire-frame pictures in FIGURE 6.8 look at attractive, it is quite difficult to interpret the results except at a gross level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a data analysis perspective, conditional plots are far more useful.\n",
    "\n",
    "FIGURE 6.9 shows an analysis of some environmental data with three predictors. The _trellis_ display here show ozone as a function of radiation, conditioned on the other two variables, temperature and wind speed. However, conditioning on the value of a variable really implies loca to that value (as in local regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under construction ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"FIGURE 6.9. Conditional plots for Los Angeles Ozone data\"\"\"\n",
    "print('Under construction ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above each of the panels in FIGURE 6.9 is an indication of the range of values present in that panel for each of the conditioning values. In the panel itself the data subsets are displayed (response versus remaining variable), and a 1D local linear regression is fit to the data.\n",
    "\n",
    "Although this is not quite the same as looking at slices of a fitted 3D surface, it is probably more useful in terms of understanding the joint behavior of the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
