{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 6.6.2. Kernel Density Classification\n",
    "\n",
    "One can use nonparametric density estimates for classification in a straight-forward fashion using Bayes' theorem.\n",
    "\n",
    "Suppose for a $J$ class problem\n",
    "* we fit nonparametric density estimates $\\hat{f}_j(X)$, for $j=1,\\cdots,J$ separately in each of the classes,\n",
    "* and we also have estimates of the class priors $\\hat\\pi_j$ (usually the sample proportions).\n",
    "\n",
    "Then\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\text{Pr}}(G=j|X=x_0) = \\frac{\\hat\\pi_j \\hat{f}_j(x_0)}{\\sum_{k=1}^J \\hat\\pi_k \\hat{f}_k(x_0)}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Difficulty with sparse data\n",
    "\n",
    "FIGURE 6.14 uses this method to estimate to prevalence of $\\textsf{CHD}$ for the heart risk factor study, and should be compared with the left panel of FIGURE 6.12. The main difference occurs in the region of high SBP in the right panel of FIGURE 6.14. In this region the data are sparse for both classes, and since the Gaussian kernel density estimates use metric kernels, the density estimates are low and of poor quality (high variance) in these region.\n",
    "\n",
    "The local logistic regression method uses the tri-cube kernel with $k$-NN of the local linear assumption to smooth out the estimate (on the logit scale)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If classification is the ultimate goal, then learning the separate class densities well may be unnecessary, and can in fact be misleading.\n",
    "\n",
    "FIGURE 6.15 shows an example where the densities are both multimodal, but the posterior ratio is quite smooth. In learning the separate densities from data, one might decide to settle for a rougher, high-variance fit to capture these features, which are irrelevant for the purposes of estimating the posterior probabilities. In fact, if classification is the ultimate goal, then we need only to estimate the posterior well near the decision boundary. e.g., for two classes, this is the set\n",
    "\n",
    "\\begin{equation}\n",
    "\\{ x| \\text{Pr}(G=1|X=x) = \\frac12 \\}.\n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
