{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 6.1.2. Local Polynomial Regression\n",
    "\n",
    "Why stop at local linear fit? We can fit local polynomial fits of any degree $d$,\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\alpha(x_0),\\beta_j(x_0), j=1,\\cdots,d} \\sum_{i=1}^N K_\\lambda(x_0,x_i) \\left( y_i - \\alpha(x_0) - \\sum_{j=1}^d \\beta_j(x_0)x_i^j \\right)^2\n",
    "\\end{equation}\n",
    "\n",
    "with solution\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}(x_0) = \\hat\\alpha(x_0) + \\sum_{j=1}^N \\hat\\beta(x_0)x_o^j.\n",
    "\\end{equation}\n",
    "\n",
    "In fact, the expansion shown in the previous section will tell us that the bias will only have components of degree $d+1$ and higher (Exercise 6.2).\n",
    "\n",
    "FIGURE 6.5 illustrates local quadratic regression. Local linear fits tend to be biased in regions of curvature of the true function, a phenomenon referred to as _trimming the hills_ and _filling the valleys_. Local quadratic regression is generally able to correct this bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-variance tradeoff, again\n",
    "\n",
    "There is of course a price to be paid for this bias reduction, and this is increased variance. The fit in the right panel of FIGURE 6.5 is slightly more wiggly, especially in the tails.\n",
    "\n",
    "Assume the model\n",
    "\n",
    "\\begin{equation}\n",
    "y_i = f(x_i) + \\epsilon_i,\n",
    "\\end{equation}\n",
    "\n",
    "with $\\epsilon_i \\sim^{\\text{iid}} (0, \\sigma^2)$. Then\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Var}(\\hat{f}(x_0)) = \\sigma^2 \\|l(x_0)\\|^2,\n",
    "\\end{equation}\n",
    "\n",
    "where $l(x_0)$ is the vector of equivalent kernel weights at $x_0$.\n",
    "\n",
    "It can be shown (Exercise 6.3) that $\\|l(x_0)\\|$ increases with $d$, and so there is a bias-variance tradeoff in selecting the polynomial degree.\n",
    "\n",
    "FIGURE 6.6 illustrates these variance curves for degree zero, one and two local polynomials. To summarize some collected wisdom on this issue:\n",
    "\n",
    "* Local linear fits can help bias dramatically at the boundaries at a modest cost in variance. Local quadratic fits do little at the boundaries for bias, but increase the variance a lot.\n",
    "* Local quadratic fits tend to be most helpful in reducing bias due to curvature in the interior of the domain.\n",
    "* Asymptotic analysis suggest that local polynomials of odd degree dominate those of even degree. This is largely due to the fact that asymptotically the MSE is dominated by boundary effects.\n",
    "\n",
    "While it may be helpful to tinker, and move from local linear fits at the boundary to local quadratic fits in the interior, we do not recommend such strategies. Usually the application will dictate the degree of the fit. For example, if we are interested in extrapolation, then the boundary is of more interest and local linear fits are probably more reliable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
