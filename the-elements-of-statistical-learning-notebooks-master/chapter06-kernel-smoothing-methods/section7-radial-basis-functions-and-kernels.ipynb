{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.7. Radial Basis Functions and Kernels\n",
    "### Review on basis expansion\n",
    "\n",
    "In Chapter 5, functions are represented as expansions in basis functions:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\sum_{j=1}^M \\beta_j h_j(x).\n",
    "\\end{equation}\n",
    "\n",
    "The art of flexible modeling using basis expansions consists of picking an appropriate family of basis functions, and then controlling the complexity of the representation by selection, regularization, or both.\n",
    "\n",
    "Some of the families of basis functions have elements that are defined locally; e.g., B-splines. If more flexibility is desired in a particular region, then that region needs to be represented by more basis functions (which in the case of B-splines translates to more knots).\n",
    "\n",
    "Tensor products of $\\mathbb{R}$-local basis functions deliver basis functions local in $\\mathbb{R}^p$. Not all basis functions are local -- e.g., the truncated power bases for splines, or  sigmoidal basis functions $\\sigma(\\alpha_0 + \\alpha x)$ used in neural-networks (Chapter 11).\n",
    "\n",
    "The composed function $f(x)$ can nevertheless show local behavior, because of the particular signs and values of the coefficients causing cancellations of global effects. For example, the truncated power basis has an equivalent B-spline basis for the same space of functions; the cancellation is exact in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review on kernel smoothing\n",
    "\n",
    "Kernel methods achieve flexibility by fitting simple models in a region local to the target point $x_0$. Localization is achieved via a weighting kernel $K_\\lambda$, and individual observations receive weights $K_\\lambda(x_0,x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial basis functions\n",
    "\n",
    "Radial basis functions combine these ideas, by treating the kernel functions $K_\\lambda(\\xi,x)$ as basis functions. This leads to the model\n",
    "\n",
    "\\begin{align}\n",
    "f(x) &= \\sum_{j=1}^M K_{\\lambda_i}(\\xi_j,x)\\beta_j \\\\\n",
    "&= \\sum_{j=1}^N D\\left( \\frac{\\|x-\\xi_j\\|}{\\lambda_j} \\right)\\beta_j,\n",
    "\\end{align}\n",
    "\n",
    "where each basis element is indexed by a location or _prototype_ parameter $xi_j$ and a scale parameter $\\lambda_j$. A popular choice for $D$ is the standard Gaussian density function.\n",
    "\n",
    "There are several approaches to learning the parameters $\\{\\lambda_j, \\xi_j, \\beta_j\\}$, for $j=1,\\cdots,M$. For simplicity we will focus on least squares methods for regression, and use the Gaussian kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1)  \n",
    "Optimize the sum-of-squares w.r.t. all the parameters,\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\{\\lambda_j,\\xi_j,\\beta_j\\}_1^M} \\sum_{i=1}^N \\left( y_i - \\beta_0 - \\sum_{j=1}^M \\beta_j \\exp\\left( -\\frac{(x_i-\\xi_j)^T(x_i-\\xi_j)}{\\lambda_j^2} \\right)\\right)^2.\n",
    "\\end{equation}\n",
    "\n",
    "This model is commonly referred to as an RBF network, an alternative to the sigmoidal neural network; the $\\xi_j$ and $\\lambda_j$ playing the role of the weights. This criterion is nonconvex with multiple local minima, and the algorithms for optimization are similar to those used for neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2)  \n",
    "Estimate the $\\{\\lambda_j,\\xi_j\\}$ separately from the $\\beta_j$. Given $\\{\\lambda_j,\\xi_j\\}$, the estimation of $\\beta_j$ is a simple least squares problem.\n",
    "\n",
    "Often the kernel parameters $\\lambda_j$ and $\\xi_j$ are chosen in an unsupervised way using the $X$ distribution alone. One of the methods is to fit a Gaussian mixture density model to the training $x_i$, which provides both the centers $\\xi_j$ and the scales $\\lambda_j$.\n",
    "\n",
    "Other even more adhoc approaches use clustering methods to local the prototypes $\\xi_j$, and treat $\\lambda_j = \\lambda$ as a hyper-parameter. The obvious drawback of these approaches is that the conditional distribution $\\text{Pr}(Y|X)$ and in particular $\\text{E}(Y|X)$ is having no say in where the action is concentrated.\n",
    "\n",
    "On the positive side, they are much simpler to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renormalize\n",
    "\n",
    "While it would seem attractive to reduce the parameter set and assume a constant value for $\\lambda_j=\\lambda$, this can have an undesirable side effect of creating _holes_ -- regions of $\\mathbb{R}^p$ where none of the kernels has appreciable support, as illustrated in FIGURE 6.16 upper panel. _Renormalized_ radial basis functions,\n",
    "\n",
    "\\begin{equation}\n",
    "h_j(x) = \\frac{D(\\|x-\\xi_j\\|/\\lambda)}{\\sum_{k=1}^M D(\\|x-\\xi_k\\|/\\lambda)}\n",
    "\\end{equation}\n",
    "\n",
    "avoid this problem (FIGURE 6.16 lower panel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Nadaraya-Watson kernel regression estimator in $\\mathbb{R}^p$ can be viewed as an expansion in renormalized radial basis functions,\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{f}(x_0) &= \\sum_{i=1}^N y_i \\frac{K_\\lambda(x_0,x_i)}{\\sum_{j=1}^N K_\\lambda(x_0,x_j)} \\\\\n",
    "&= \\sum_{i=1}^N y_i h_i(x_0),\n",
    "\\end{align}\n",
    "\n",
    "with a basis function $h_i$ located at every observation and coefficients $y_i$; i.e.,\n",
    "\n",
    "\\begin{align}\n",
    "\\xi_i &= x_i, \\\\\n",
    "\\hat\\beta_i &= y_i, \\text{ for } i=1,\\cdots,N.\n",
    "\\end{align}\n",
    "\n",
    "Note the similarity between the expansion and the solution (5.50 on page 169) to the regularization problem induced by the kernel $K$. Radial basis functions form the bridge between the modern \"kernel methods\" and local fitting technology."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
