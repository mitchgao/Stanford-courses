{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 6.5. Local Likelihood and Other Models\n",
    "\n",
    "The concept of local regression and varying coefficient models is extremely broad: Any parametric model can be made local if the fitting method accommodates observation weights. Here are some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "\n",
    "Associated with each observation $y_i$ is a parameter\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_i = \\theta(x_i) = x_i^T\\beta\n",
    "\\end{equation}\n",
    "\n",
    "linear in the covariate(s) $x_i$, and inference for $\\beta$ is based on the log-likelihood\n",
    "\n",
    "\\begin{equation}\n",
    "l(\\beta) = \\sum_{i=1}^N l(y_i,x_i^T\\beta).\n",
    "\\end{equation}\n",
    "\n",
    "We can model $\\theta(X)$ more flexibly by using the likelihood local to $x_0$ for inference of $\\theta(x_0) = x_0^T\\beta(x_0)$:\n",
    "\n",
    "\\begin{equation}\n",
    "l(\\beta(x_0)) = \\sum_{i=1}^N K_\\lambda(x_0,x_i) l(y_i,x_i^T\\beta(x_0)).\n",
    "\\end{equation}\n",
    "\n",
    "Many likelihood models, in particular the family of  generalized linear models including logistic and log-linear models, involve the covariates in a linear fashion. Local likelihood allows a relaxation from a globally linear model to one that is locally linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.\n",
    "\n",
    "As above, except different variables are associated with $\\theta$ from those used for defining the local likelihood:\n",
    "\n",
    "\\begin{equation}\n",
    "l(\\theta(z_0)) = \\sum_{i=1}^N K_\\lambda(z_0,z_i) l\\left(y_i,\\eta(x_i,\\theta(z_0))\\right).\n",
    "\\end{equation}\n",
    "\n",
    "For example, $\\eta(x,\\theta) = x^T\\theta$ could be a linear model in $x$. This will fit a varying coefficient model $\\theta(z)$ by maximizing the local likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.\n",
    "\n",
    "Autoregressive time series models of order $k$ have the form\n",
    "\n",
    "\\begin{equation}\n",
    "y_t = \\beta_0 + \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\cdots + \\beta_k y_{t-k} + \\epsilon_t.\n",
    "\\end{equation}\n",
    "\n",
    "Denoting the _lag set_ by\n",
    "\n",
    "\\begin{equation}\n",
    "z_t = (y_{t-1}, y_{t-2}, \\cdots, y_{t-k}),\n",
    "\\end{equation}\n",
    "\n",
    "the model looks like a standard linear model\n",
    "\n",
    "\\begin{equation}\n",
    "y_t = z_t^T\\beta + \\epsilon,\n",
    "\\end{equation}\n",
    "\n",
    "and is typically fit by least squares. Fitting by local least squares with a kernel $K(z_0,z_t)$ allows the model to vary according to the short-term history of the series.\n",
    "\n",
    "This is to be distinguished from the more traditional dynamic linear models that vary by windowing time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an illustration of local likelihood, we consider the local version of the multiclass linear logistic regression model of Chapter 4. The data consist of features $x_i$ and an associated categorical response $g_i \\in \\{1,2,\\cdots,J\\}$, and the linear model has the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Pr}(G=j|X=x) = \\frac{\\exp\\left(\\beta_{j0} + \\beta_j^Tx\\right)}{1 + \\sum_{k=1}^{J-1} \\exp\\left(\\beta_{k0} + \\beta_k^Tx\\right)}.\n",
    "\\end{equation}\n",
    "\n",
    "The local log-likelihood for this $J$ class model can be written\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^N K_\\lambda(x_0,x_i) \\left\\{ \\beta_{g_i 0}(x_0) + \\beta_{g_i}^T(x_i-x_0) - \\log\\left( 1 + \\sum_{k=1}^{J-1} \\exp\\left( \\beta_{k0}(x_0) + \\beta_k(x_0)^T(x_i-x_0) \\right) \\right) \\right\\}.\n",
    "\\end{equation}\n",
    "\n",
    "Notice that\n",
    "* we have used $g_i$ as a subscript in the first line to pick out the appropriate numerator;\n",
    "* $\\beta_{J0} = 0$ and $\\beta_J = 0$ by the definition of the model;\n",
    "* we have centered the local regressions at $x_0$, so that the fitted posterior probabilities at $x_0$ are simply  \n",
    "\n",
    "  \\begin{equation}\n",
    "  \\hat{\\text{Pr}}(G=j|X=x) = \\frac{\\exp\\left(\\hat\\beta_{j0} + \\hat\\beta_j^Tx\\right)}{1 + \\sum_{k=1}^{J-1} \\exp\\left(\\hat\\beta_{k0} + \\hat\\beta_k^Tx\\right)}.\n",
    "  \\end{equation}\n",
    "  \n",
    "This model can be used for flexible multiclass classification in moderately low dimensions, although successes have been reported with the high-dimensional ZIP-code classficiation problem. Generalized additive models (Chapter 9) using kernel smoothing methods are closely related, and avoid dimensionality problems by assuming an additive structure for the regression function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heart disease data, again\n",
    "\n",
    "As a simple illustration we fit a two-class local linear logistic model to the heart disease data of Chapter 4. FIGURE 6.12 shows the univariate local logistic models fit to two of the risk factors (separately). This is a useful screening device for detecting nonlinearities, when the data themselves have little visual information to offer. In this case an unexpected anomaly is uncovered in the data, which may have gone unnoticed with traditional method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under construction ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"FIGURE 6.12. Local logistic models for the heart disease data\"\"\"\n",
    "print('Under construction ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\textsf{CHD}$ is a binary indicator, we could estimate the conditional prevalence $\\text{Pr}(G=j|x_0)$ by simply smoothing this binary response directly without resorting to a likelihood formulation. This amounts to fitting a locally constant logistic regression model (Exercise 6.5). In order to enjoy the bias-correction of local-linear smoothing, it is more natural to operate on the unstricted logit scale.\n",
    "\n",
    "Typically with logistic regression, we compute parameter estimates as well as their standard errors. This can be done locally as well, and so we can produce, as shown in the plot, estimated pointwise standard-error bands about our fitted prevalence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
