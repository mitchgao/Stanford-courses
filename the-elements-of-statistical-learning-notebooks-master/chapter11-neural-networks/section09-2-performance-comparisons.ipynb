{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 11.9.2. Performance Comparisons\n",
    "Based on the similarities above, we decided to compare Bayesian neural networks to boosted trees, boosted neural networks, random forests and bagged neural networks on the five datasets in TABLE 11.2.\n",
    "\n",
    "Bagging and boosting of neural networks are not methods that we have previously used in our work. We decided to try them here, because of the success of Bayesian neural networks in this competition, and the good performance of bagging and boosting with trees. We also felt that by bagging and boosting neural nets, we could assess both the choice of model as well as the model search strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List-up\n",
    "Here are the details of the learning methods that were compared:\n",
    "\n",
    "* __Bayesian neural nets.__ The results here are taken from Neal and Zhang (2006), using their Bayesian approach to fitting neural networks. The models had two hidden layers of 20 and 8 units. We re-ran some networks for timing purposes only.\n",
    "* __Boosted trees.__ We used the $\\textsf{gbm}$ package (version 1.5-7) in the R language. Tree depth and shrinkage factors varied from dataset to dataset. We consistantly bagged 80% of the data at each boosting iteration (the default is 50%). Shrinkage was between 0.001 and 0.1. Tree depth was between 2 and 9.\n",
    "* __Boosted neural networks.__ Since boosting is typically most effective with \"weak\" learners, we boosted a single hidden layer neural network with two or four units, fit with the $\\textsf{nnet}$ package (version 7.2-36) in R.\n",
    "* __Random forests.__ We used the R package $\\textsf{randomForest}$ (version 4.5-16) with default settings for the parameters.\n",
    "* __Bagged neural networks.__ We used the same architecture as in the Bayesian neural network above (two hideen layers of 20 and 8 units), fit using both Neal's C language package \"Flexible Bayesian Modeling\" (2004-11-10 release), and Matlab neural-net toolbox (version 5.1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis was carried out by Nicholas Johnson, and full details may be found in Johnson (2008). The results are shown in FIGURE 11.12 and TABLE 11.3.\n",
    "\n",
    "![figure 11.12](fig11-12.jpg)\n",
    "\n",
    "#### TABLE 11.3. Performance of different methods.\n",
    "Values are average rank of test error across the five problems (low is good), and mean computation time and standard error of the mean, in minutes.\n",
    "\n",
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th rowspan=\"2\">Method</th>\n",
    "            <th colspan=\"2\">Screened Features</th>\n",
    "            <th colspan=\"2\">ARD Reduced Features</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <th>Average Rank</th>\n",
    "            <th>Average Time</th>\n",
    "            <th>Average Rank</th>\n",
    "            <th>Average Time</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Bayesian neural networks</td>\n",
    "            <td>1.5</td>\n",
    "            <td>384(138)</td>\n",
    "            <td>1.6</td>\n",
    "            <td>600(186)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Boosted trees</td>\n",
    "            <td>3.4</td>\n",
    "            <td>3.03(2.5)</td>\n",
    "            <td>4.0</td>\n",
    "            <td>34.1(32.4)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Boosted neural networks</td>\n",
    "            <td>3.8</td>\n",
    "            <td>9.4(8.6)</td>\n",
    "            <td>2.2</td>\n",
    "            <td>35.6(33.5)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Random forests</td>\n",
    "            <td>2.7</td>\n",
    "            <td>1.9(1.7)</td>\n",
    "            <td>3.2</td>\n",
    "            <td>11.2(9.3)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Bagged neural networks</td>\n",
    "            <td>3.6</td>\n",
    "            <td>3.5(1.1)</td>\n",
    "            <td>4.0</td>\n",
    "            <td>6.4(4.4)</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "The figure and table show Bayesian, boosted, and bagged neural networks, boosted trees and random forests, using both the screened and reduced feature sets.\n",
    "\n",
    "The error bars at the top of each plot indicate one standard error of the difference between two error rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison results\n",
    "Bayesian neural networks again emerge as the winner, although for some datasets the differences between the test error rates is not statistically significant.\n",
    "\n",
    "Random forests performs the best among the competitors using the selected feature set, while the boosted neural networks perform best with the reduced feature set, and nearly match the Bayesian neural net.\n",
    "\n",
    "The superiority of boosted neural networks over boosted trees suggest that the neural network model is better suited to these particular problems. Specifically, individual features might not be good predictors here and linear combinations of features work better. However the impressive performance of random forests is at odds with this explanation, and came as a surprise to us.\n",
    "\n",
    "Since the reduced feature sets come from the Bayesian neural network approach, only the methods that use the screened features are legitimate, self-contained procedures. However, this does suggest that better methods for internal feature selection might help the overall performance of boosted neural networks.\n",
    "\n",
    "The table also shows the approximate training time required for each method. Here the non-Bayesian methods show a clear advantage.\n",
    "\n",
    "Overall, the superior performance of Bayesian neural networks here may be due to the fact that\n",
    "1. the neural network model is well suited to these five problems, and\n",
    "1. the MCMC approach provides an efficient way of exploring the important part of the parameter space, and then averaging the resulting models according to their quality.\n",
    "\n",
    "The Bayesian appraoch works well for smoothly parametrized models like neural nets; it is not yet clear that it works as well for non-smooth models like trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
