{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 11.4. Fitting Neural Networks\n",
    "\n",
    "The neural network model has unknown parameters, often called _weights_, and we seek values for them that make the model fit the training data well.\n",
    "\n",
    "We denote the complete set of weights by $\\theta$, which consists of\n",
    "\n",
    "\\begin{align}\n",
    "\\{ \\alpha_{0m}, \\alpha_m : m=1,2,\\cdots,M \\} & & M(p+1) \\text{ weights}, \\\\\n",
    "\\{ \\beta_{0k}, \\beta_k : k=1,2,\\cdots,K \\} & & K(M+1) \\text{ weights}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regression, we use sum-of-squared errors as our measure of fit (error function)\n",
    "\n",
    "\\begin{equation}\n",
    "R(\\theta) = \\sum_{k=1}^K \\sum_{i=1}^N (y_{ik} - f_k(x_i))^2.\n",
    "\\end{equation}\n",
    "\n",
    "For classification we use either squared error or cross-entropy (deviance)\n",
    "\n",
    "\\begin{equation}\n",
    "R(\\theta) = -\\sum_{i=1}^N \\sum_{k=1}^K y_{ik}\\log f_k(x_i),\n",
    "\\end{equation}\n",
    "\n",
    "and the corresponding classifier is\n",
    "\n",
    "\\begin{equation}\n",
    "G(x) = \\arg\\max_k f_k(x).\n",
    "\\end{equation}\n",
    "\n",
    "With the softmax activation function and the cross-entropy error function, the neural network model is exactly a linear logistic regression in the hidden units, and all the parameters are estimated by maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "Typically we don't want the global minimizer of $R(\\theta)$, as this is likely to be an overfit solution. Instead some regularization is needed: this is achieved directly through a penalty term, or indirectly by early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back-propagation = gradient descent\n",
    "\n",
    "The generic approach to minimizing $R(\\theta)$ is by gradient descent, called _back-propagation_ in this setting. Because of the compositional form of the model, the gradient can be easily derived using the chain rule for differentiation.\n",
    "\n",
    "This can be computed by a forward and backward sweep over the network, keeping track only of quantities local to each unit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back-propagation for squared error loss\n",
    "\n",
    "Let\n",
    "* $z_{mi} = \\sigma(\\alpha_{0m} + \\alpha_m^T x_i)$, and\n",
    "* $z_i = (z_{1i}, z_{2i}, \\cdots, z_{Mi})$.\n",
    "\n",
    "Then we have\n",
    "\n",
    "\\begin{align}\n",
    "R(\\theta) &\\equiv \\sum_{i=1}^N R_i \\\\\n",
    "&= \\sum_{i=1}^N \\sum_{k=1}^K \\left( y_{ik} - f_k(x_i) \\right)^2,\n",
    "\\end{align}\n",
    "\n",
    "with derivatives\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial R_i}{\\partial\\beta_{km}} &= -2\\left( y_{ik} - f_k(x_i) \\right) g_k'(\\beta_k^T z_i) z_{mi}, \\\\\n",
    "\\frac{\\partial R_i}{\\partial\\alpha_{ml}} &= -2\\sum_{k=1}^K \\left( y_{ik} - f_k(x_i) \\right) g_k'(\\beta_k^T z_i) \\beta_{km} \\sigma'(\\alpha_m^T x_i) x_{il}.\n",
    "\\end{align}\n",
    "\n",
    "Given these derivatives, a gradient descent update at the $(r+1)$st iteration has the form\n",
    "\n",
    "\\begin{align}\n",
    "\\beta_{km}^{(r+1)} &= \\beta_{km}^{(r)} - \\gamma_r \\sum_{i=1}^N \\frac{\\partial R_i}{\\partial\\beta_{km}^{(r)}}, \\\\\n",
    "\\alpha_{ml}^{(r+1)} &= \\alpha_{ml}^{(r)} - \\gamma_r \\sum_{i=1}^N \\frac{\\partial R_i}{\\partial \\alpha_{ml}^{(r)}},\n",
    "\\end{align}\n",
    "\n",
    "where $\\gamma_r$ is the learning rate, discussed below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back-propagation equations\n",
    "\n",
    "Now write the gradients as\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial R_i}{\\partial\\beta_{km}} &= \\delta_{ki} z_{mi}, \\\\\n",
    "\\frac{\\partial R_i}{\\partial\\alpha_{ml}} &= s_{mi} x_{il}.\n",
    "\\end{align}\n",
    "\n",
    "The quantities $\\delta_{ki}$ and $s_{mi}$ are \"errors\" from the current model at the output and hidden layer units, respectively. From their definitions, these errors satisfy\n",
    "\n",
    "\\begin{equation}\n",
    "s_{mi} = \\sigma'(\\alpha_m^T x_i) \\sum_{k=1}^K \\beta_{km} \\delta_{ki},\n",
    "\\end{equation}\n",
    "\n",
    "known as the _back-propagation equations_. Using this, the gradient descent updates can be implemented with a two-pass algorithm.\n",
    "1. In the _forward pass_, the current weights are fixed and the predicted values $\\hat{f}_k(x_i)$ are computed from the formula  \n",
    "  \n",
    "  \\begin{align}\n",
    "  Z_m &= \\sigma(\\alpha_{0m} + \\alpha_m^T X), & m=1,\\cdots,M, \\\\\n",
    "  T_k &= \\beta_{0k} + \\beta_k^T Z, & k=1,\\cdots,K, \\\\\n",
    "  f_k(X) &= g_k(T), & k=1,\\cdots,K.\n",
    "  \\end{align}\n",
    "  \n",
    "2. In the _backward pass_, the errors $\\delta_{ki}$ are computed, and then back-propagated via  \n",
    "\n",
    "  \\begin{equation}\n",
    "  s_{mi} = \\sigma'(\\alpha_m^T x_i) \\sum_{k=1}^K \\beta_{km} \\delta_{ki},\n",
    "  \\end{equation}\n",
    "  \n",
    "  to give the errors $s_{mi}$.\n",
    "\n",
    "Both set of errors are then used to compute the gradients for the updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of back-propagation\n",
    "\n",
    "This two-pass procedure is what is known as back-propagation. It has also been called the _delta rule_ (Widrow and Hoff, 1960). The computational components for cross-entropy have the same form as those for the sum of squares error function (Exercise 11.3).\n",
    "\n",
    "The advantages of back-propagation are its simple, local nature. In the back-propagation algorithm, each hidden unit passes and receives information only to and from units that share a connection. Hence it ca be implemented efficiently on a parallel architecture computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch and online learning\n",
    "\n",
    "The updates\n",
    "\n",
    "\\begin{align}\n",
    "\\beta_{km}^{(r+1)} &= \\beta_{km}^{(r)} - \\gamma_r \\sum_{i=1}^N \\frac{\\partial R_i}{\\partial\\beta_{km}^{(r)}}, \\\\\n",
    "\\alpha_{ml}^{(r+1)} &= \\alpha_{ml}^{(r)} - \\gamma_r \\sum_{i=1}^N \\frac{\\partial R_i}{\\partial \\alpha_{ml}^{(r)}},\n",
    "\\end{align}\n",
    "\n",
    "are a kind of _batch learning_, with the parameter updates being a sum over all of training cases.\n",
    "\n",
    "Learning can also be carried out online -- processing each observation one at a time , updating the gradient after each training case, and cycling through the training cases many times. In this cases, the sums in the updates are replaced by a single summand.\n",
    "\n",
    "A _training epoch_ refers to one sweep through the entire training set. Online training allows the network to handle very large training sets, and also to update the weights as new observations come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rate\n",
    "\n",
    "The learning rate $\\gamma_r$ for batch learning is usually taken to be a constant, and can also be optimized by a line search that minimizes the error function at each update.\n",
    "\n",
    "With online learning $\\gamma_r$ should decrease to zero as the iteration $r \\rightarrow \\infty$. This learning is a form of _stochastic approximation_ (Robbins and Munro, 1951); results in this field ensure convergence if\n",
    "* $\\gamma_r \\rightarrow \\infty$,\n",
    "* $\\sum_r \\gamma_r = \\infty$, and\n",
    "* $\\sum_r \\gamma_r^2 \\lt \\infty$.\n",
    "\n",
    "Satisfied, for example, by $\\gamma_r = 1/r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's slow\n",
    "\n",
    "Back-propagation can be very slow, and for that reason is usually not the method of choice. Second-order techniques such as Newton's method are not attractive here, because the second derivative matrix of $R$ (the Hessian) can be very large.\n",
    "\n",
    "Better approaches to fitting include conjugate gradient and variable metric methods. These avoid explicit computation of the second derivative matrix while still providing faster convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
