{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 4.5. Separating Hyperplanes\n",
    "\n",
    "We describe separating hyperplane classifiers, constructing linear decision boundaries that explicitly try to separate the data into different classes as well as possible. They provide the basis for support vector classifiers, discussed in Chapter 12.\n",
    "\n",
    "FIGURE 4.14 shows 20 data points of two classes in $\\mathbb{R}^2$, which can be separated by a linear boundary but there are infinitely many possible *separating hyperplanes*.\n",
    "\n",
    "The orange line is the least squares solution to the problem, obtained by regressing the $-1/1$ response $Y$ on $X$ with intercept; the line is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\left\\lbrace x: \\hat\\beta_0 + \\hat\\beta_1x_1 + \\hat\\beta_2x_2=0 \\right\\rbrace.\n",
    "\\end{equation}\n",
    "\n",
    "This least squares solution does not do a perfect job in separating the points, and makes one error. This is the same boundary found by LDA, in light of its equivalence with linear regression in the two-class case ($\\S$ 4.3 and Exercise 4.2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptrons\n",
    "\n",
    "Classifiers such as above, that compute a linear combination of the input features and return the sign, were called *perceptrons* in the engineering literatur in the late 1950s (Rosenblatt, 1958). Perceptrons set he foundations for the neural network models of the 1980s and 1990s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review on vector algebra\n",
    "\n",
    "FIGURE 4.15 depicts a hyperplane or *affine set* $L$ defined by the equation\n",
    "\n",
    "\\begin{equation}\n",
    "f(x) = \\beta_0 + \\beta^T x = 0,\n",
    "\\end{equation}\n",
    "\n",
    "since we are in $\\mathbb{R}^2$ this is a line.\n",
    "\n",
    "Here we list some properties:\n",
    "1. For any two points $x_1$ and $x_2$ lying in $L$,  \n",
    "\\begin{equation}\n",
    "\\beta^T(x_1-x_2)=0,\n",
    "\\end{equation}\n",
    "and hence the unit vector $\\beta^* = \\beta/\\|\\beta\\|$ is normal to the surface of $L$.\n",
    "2. For any point $x_0$ in $L$,  \n",
    "\\begin{equation}\n",
    "\\beta^Tx_0 = -\\beta_0.\n",
    "\\end{equation}\n",
    "3. The signed distance of any point $x$ to $L$ is given by  \n",
    "\\begin{align}\n",
    "\\beta^{*T}(x-x_0) &= \\frac{1}{\\|\\beta\\|}(\\beta^Tx+\\beta_0) \\\\\n",
    "&= \\frac{1}{\\|f'(x)\\|}f(x).\n",
    "\\end{align}\n",
    "Hence $f(x)$ is proportional to the signed distance from $x$ to the hyperplane defined by $f(x)=0$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
