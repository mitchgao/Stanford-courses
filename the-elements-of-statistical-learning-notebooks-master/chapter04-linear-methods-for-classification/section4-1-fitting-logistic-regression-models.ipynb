{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 4.4.1. Fitting Logistic Regression Models\n",
    "\n",
    "### Maximum likelihood\n",
    "\n",
    "Logistic regression models are usually fit by maximum likelihood, using the conditional likelihood of $G$ given $X$. Since $\\text{Pr}(G|X)$ completely specifies the conditional distribution, the *multinomial* distribution is appropriate.\n",
    "\n",
    "The log-likelihood for $N$ observation is\n",
    "\n",
    "\\begin{equation}\n",
    "l(\\theta) = \\sum_{i=1}^N \\log p_{g_i}(x_i;\\theta),\n",
    "\\end{equation}\n",
    "\n",
    "where $p_k(x_i;\\theta) = \\text{Pr}(G=k|X=x_i;\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood for $K=2$ case\n",
    "\n",
    "We discuss in detail the two-class case, sine the algorithms simplify considerably. It is convenient to code the two-class $g_i$ via a $0/1$ response $y_i$, where $y_i=1$ when $g_i=1$, and $0$ otherwise. Then we can let\n",
    "\n",
    "\\begin{align}\n",
    "p_1(x;\\theta) &= p(x;\\theta), \\\\\n",
    "p_2(x;\\theta) &= 1- p(x;\\theta). \\\\\n",
    "\\end{align}\n",
    "\n",
    "The log-likelihood can be written\n",
    "\n",
    "\\begin{align}\n",
    "l(\\beta) &= \\sum_{i=1}^N \\left\\lbrace y_i\\log p(x_i;\\beta) + (1-y_i)\\log(1-p(x_i;\\beta)) \\right\\rbrace \\\\\n",
    "&= \\sum_{i=1}^N \\left\\lbrace y_i\\beta^Tx_i - \\log(1+\\exp(\\beta^Tx)) \\right\\rbrace,\n",
    "\\end{align}\n",
    "\n",
    "where $\\beta^T = \\lbrace \\beta_{10}, \\beta_1^T \\rbrace$, and we assume that the vector of inputs $x_i$ includes the constant term 1 to acommodate the intercept.\n",
    "\n",
    "To maximize the log-likelihood, we set its derivatives to zero. These *score* equations are\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial l(\\beta)}{\\partial\\beta} = \\sum_{i=1}^N x_i(y_i-p(x_i;\\beta)) = 0,\n",
    "\\end{equation}\n",
    "\n",
    "which are $p+1$ equations *nonlinear* in $\\beta$. Notice that since $x_{i1} =1$, the first score equation specifies that\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^N y_i = \\sum_{i=1}^N p(x_i;\\beta),\n",
    "\\end{equation}\n",
    "\n",
    "implying that the *expected* number of class ones matches the observed number (and hence also class twos)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton-Raphson algorithm\n",
    "\n",
    "To solve the score equation, we use the Newton-Raphson algorithm, which requires the second-derivative or Hessian matrix\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial^2 l(\\beta)}{\\partial\\beta\\partial\\beta^T} = -\\sum_{i=1}^N x_ix_i^T p(x_i;\\beta)(1-p(x_i;\\beta)).\n",
    "\\end{equation}\n",
    "\n",
    "Starting with $\\beta^{\\text{old}}$, a single Newton update is\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta^{\\text{new}} = \\beta^{\\text{old}} - \\left( \\frac{\\partial^2 l(\\beta)}{\\partial\\beta\\partial\\beta^T} \\right)^{-1} \\frac{\\partial l(\\beta)}{\\partial\\beta},\n",
    "\\end{equation}\n",
    "\n",
    "where the derivatives are evaluated at $\\beta^{\\text{old}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The same thing in matrix notation\n",
    "\n",
    "Let\n",
    "* $\\mathbf{y}$ be the vector of $y_i$ values,\n",
    "* $\\mathbf{X}$ the $N\\times (p+1)$ matrix of $x_i$ values,\n",
    "* $\\mathbf{p}$ the vector of fitted probabilities with $i$th element $p(x_i;\\beta^{\\text{old}})$, and\n",
    "* $\\mathbf{W}$ $N\\times N$ diagonal matrix of weights with $i$th diagonal elements $p(x_i;\\beta^{\\text{old}})(1-p(x_i;\\beta^{\\text{old}}))$.\n",
    "\n",
    "Then we have\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial l(\\beta)}{\\partial\\beta} &= \\mathbf{X}^T(\\mathbf{y}-\\mathbf{p}) \\\\\n",
    "\\frac{\\partial^2l(\\beta)}{\\partial\\beta\\partial\\beta^T} &= -\\mathbf{X}^T\\mathbf{WX},\n",
    "\\end{align}\n",
    "\n",
    "and thus the Newton step is\n",
    "\n",
    "\\begin{align}\n",
    "\\beta^{\\text{new}} &= \\beta^{\\text{old}} + (\\mathbf{X}^T\\mathbf{WX})^{-1}\\mathbf{X}^T(\\mathbf{y}-\\mathbf{p}) \\\\\n",
    "&= (\\mathbf{X}^T\\mathbf{WX})^{-1} \\mathbf{X}^T\\mathbf{W}\\left( \\mathbf{X}\\beta^{\\text{old}} + \\mathbf{W}^{-1}(\\mathbf{y}-\\mathbf{p}) \\right) \\\\\n",
    "&= (\\mathbf{X}^T\\mathbf{WX})^{-1}\\mathbf{X}^T\\mathbf{W}\\mathbf{z},\n",
    "\\end{align}\n",
    "\n",
    "where we have re-expressed the Newton step as weighted least squares step, with the response\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{z} = \\mathbf{X}\\beta^{\\text{old}} + \\mathbf{W}^{-1}(\\mathbf{y}-\\mathbf{p}),\n",
    "\\end{equation}\n",
    "\n",
    "sometimes known as the *adjusted response*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iteratively reweighted least squares\n",
    "\n",
    "These equations for the Newton step get solved repeatedly, since at each iteration $p$ changes, and hence so does $\\mathbf{W}$ and $\\mathbf{z}$. This algorithm is referred to as *iteratively reweighted least squares* or IRLS, since each iteration solves the weighted least squares problem:\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta^{\\text{new}} \\leftarrow \\arg\\min_\\beta (\\mathbf{z}-\\mathbf{X}\\beta)^T\\mathbf{W}(\\mathbf{z}-\\mathbf{X}\\beta)\n",
    "\\end{equation}\n",
    "\n",
    "It seems that $\\beta=0$ is a good starting value, although convergence is never guaranteed. Typically the algorithm does converge, since the log-likelihood is concave, but overshooting can occur. In the rare cases that the log-likelihood decreases, step size halving will guarantee convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass case with $K\\ge 3$\n",
    "\n",
    "The Newton step can also be expressed as an IRLS, but with a *vector* of $K-1$ responses and a nondiagonal weight matrix per observation. (Exercise 4.4)\n",
    "\n",
    "Alternatively coordinate-descent methods ($\\S$ 3.8.6) can be used to maximize the log-likelihood efficiently.\n",
    "\n",
    "The $\\textsf{R}$ package $\\textsf{glmnet}$ (Friedman et al., 2010) can fit very large logistic regression problems efficiently, both in $N$ and $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal of logistic regression\n",
    "\n",
    "Logistic regression models are used mostly as a data analysis and inference tool, where the goal is to understand the role of the input variables in *explaning* the outcome. Typically many models are fit in a search for a parsimonious model involving a subset of the variables, possibly with some interactions terms. The following example illustrates some of the issues involved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
