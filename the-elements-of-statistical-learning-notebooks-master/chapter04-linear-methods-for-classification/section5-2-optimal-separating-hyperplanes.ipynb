{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 4.5.2. Optimal Separating Hyperplanes\n",
    "\n",
    "The *optimal separating hyperplanes* separates the two classes and maximizes the distance to the closest point from either class (Vapnik, 1996). Not only does this provide a unique solution to the separating hyperplane problem, but by maximizing the margin between two classes on the training data, this leads to better classification performance on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulation\n",
    "\n",
    "We need to generalize the perceptron criterion\n",
    "\n",
    "\\begin{equation}\n",
    "D(\\beta,\\beta_0) = -\\sum_{i\\in\\mathcal{M}} y_i(x_i^T\\beta + \\beta_0).\n",
    "\\end{equation}\n",
    "\n",
    "Consider the optimization problem\n",
    "\n",
    "\\begin{equation}\n",
    "\\max_{\\beta,\\beta_0,\\|\\beta\\|=1} M \\\\\n",
    "\\text{subject to } y_i(x_i^T\\beta + \\beta_0) \\ge M, \\text{ for } i = 1,\\cdots,N.\n",
    "\\end{equation}\n",
    "\n",
    "The set of conditions ensure that all the points are at least a signed distance $M$ from the decision boundary defined by $\\beta$ and $\\beta_0$, and we seek the largest such $M$ and associated parameters.\n",
    "\n",
    "We can get rid of the $\\|\\beta\\| = 1$ constraint by replacing the conditions with\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{\\|\\beta\\|} y_i(x_i^T\\beta + \\beta_0) \\ge M, \\\\\n",
    "\\text{or equivalently} \\\\\n",
    "y_i(x_i^T\\beta + \\beta_0) \\ge M\\|\\beta\\|,\n",
    "\\end{equation}\n",
    "\n",
    "which redefines $\\beta_0$.\n",
    "\n",
    "Since for any $\\beta$ and $\\beta_0$ satisfying these inequalities, any positively scaled multiple satisfies them too, we can arbitrarily set\n",
    "\n",
    "\\begin{equation}\n",
    "\\|\\beta\\| = \\frac{1}{M},\n",
    "\\end{equation}\n",
    "\n",
    "which leads to the equivalent formulation as\n",
    "\n",
    "\\begin{equation}\n",
    "\\min_{\\beta,\\beta_0} \\frac{1}{2}\\|\\beta\\|^2 \\\\\n",
    "\\text{subject to } y_i(x_i^T\\beta + \\beta_0) \\ge 1, \\text{ for }i=1,\\cdots,N.\n",
    "\\end{equation}\n",
    "\n",
    "In light of $(4.40)$, the constraints define an empty slab or margin around the linear decision boundary of thickness $1/\\|\\beta\\|$. Hence we choose $\\beta$ and $\\beta_0$ to maximize its thickness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convex optimization\n",
    "\n",
    "This is a convex optimization problem (quadratic criterion with linear inequality constraints). The Lagrange (primal) function, to be minimized w.r.t. $\\beta$ and $\\beta_0$, is\n",
    "\n",
    "\\begin{equation}\n",
    "L_P = \\frac{1}{2}\\|\\beta\\|^2 - \\sum_{i=1}^N \\alpha_i \\left[ y_i(x_i^T\\beta + \\beta_0) -1 \\right].\n",
    "\\end{equation}\n",
    "\n",
    "Setting the derivatives to zero, we obtain:\n",
    "\n",
    "\\begin{align}\n",
    "\\beta &= \\sum_{i=1}^N \\alpha_i y_i x_i, \\\\\n",
    "0 &= \\sum_{i=1}^N \\alpha_i y_i,\n",
    "\\end{align}\n",
    "\n",
    "and substitutig these in $L_P$ we obtain the so-called Wolfe dual\n",
    "\n",
    "\\begin{equation}\n",
    "L_D = \\sum_{i=1}^N \\alpha_i - \\frac{1}{2} \\sum_{i=1}^N \\sum_{k=1}^N \\alpha_i \\alpha_k y_i y_k x_i^T x_k \\\\\n",
    "\\text{subject to } \\alpha_i \\ge 0 \\text{ and } \\sum_{i=1}^N \\alpha_i y_i = 0.\n",
    "\\end{equation}\n",
    "\n",
    "The solution is obtained by maximizing $L_D$ in the positive orthant, a simpler convex optimization problem, for which standard software can be used. In addition the solution must satisfy the Karush-Kuhn-Tucker (KKT) conditions, which includes the above three conditions and\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_i \\left[ y_i (x_i^T\\beta + \\beta_0) \\right] = 0, \\forall i.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implications of the algorithm\n",
    "\n",
    "From these we can see that\n",
    "* if $\\alpha_i \\gt 0$, then $y_i(x_i^T\\beta + \\beta_0) = 1$, or in other words, $x_i$ is on the boundary of the slab;\n",
    "* if $y_i(x_i^T\\beta + \\beta_0) \\gt 1$, $x_i$ is not on the boundary of the slab, and $\\alpha_i = 0$.\n",
    "\n",
    "From the above condition of the primal Lagrange function\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta = \\sum_{i=1}^N \\alpha_i y_i x_i,\n",
    "\\end{equation}\n",
    "\n",
    "we see that the solution vector $\\beta$ is defined in terms of a linear combination of the *support points* $x_i$ -- those points defined to be on the boundary of slab via $\\alpha_i \\gt 0$.\n",
    "\n",
    "FIGURE 4.16 shows the optimal separating hyperplane for our toy example; these are three support points. Likewise, $\\beta_0$ is obtained by solving the last KKT condition\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_i \\left[ y_i (x_i^T\\beta + \\beta_o) \\right] = 0,\n",
    "\\end{equation}\n",
    "\n",
    "for any of the support points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under construction (CVXOPT may be needed, priority low)...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"FIGURE 4.16. Optimal separating hyperplane\n",
    "\n",
    "The same data aas in FIGURE 4.14. The shaded region delineates the maximum\n",
    "margin separating the two classes. There are three support points\n",
    "indicated, which lie on the boundary of the margin, and the optimal\n",
    "separating hyperplane (blue line) bisects the slab. Included in the figure\n",
    "is the boundary found using logistic regreesion (red line), which is very\n",
    "close to the optimal separating hyperplane (see Section 12.3.3).\n",
    "\n",
    "https://docs.scipy.org/doc/scipy/reference/optimize.html\"\"\"\n",
    "print('Under construction (CVXOPT may be needed, priority low)...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "The optimal separating hyperplane produces a function $\\hat{f}(x) = x^T\\hat\\beta + \\hat\\beta_0$ for classifying new observations:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{G}(x) = \\text{sign } \\hat{f}(x).\n",
    "\\end{equation}\n",
    "\n",
    "Although none of the training observations fall in the margin (by construction), this will not necessarily be the case for test observations. The intuition is that a large margin on the training data will lead to good separation on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency on model assumption\n",
    "\n",
    "The description of the solution in terms of support points seems to suggest that the optimal hyperplane focuses more on the points that count, and is more robust to model misspecification.\n",
    "\n",
    "The LDA solution, on the other hand, depends on all of the data, even points far away from the decision boundary. Note, however, that the identification of these support points required the use of all the data. Of course, if the classes are really Gaussian, then LDA is optimal, and separating hyperplane will pay a price for focusing on the (noisier) data at the boundaries if the classes.\n",
    "\n",
    "Included in FIGURE 4.16 is the logistic regression solution to this problem, fit by maximum likelihood. Both solutions are similar in this case. When a separating hyperplane exists, logistic regression will always find it, since the log-likelihood can be driven to $0$ in this case (Exercise 4.5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*skipped*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When the data are not separable\n",
    "\n",
    "There will be no feasible solution to this problem, and an alternative formulation is needed.\n",
    "\n",
    "Again one can enlarge the space using basis transformations, but this can lead to artificial separation through over-fitting. In Chapter 12 we discuss a more attractive alternative known as the *support vector machine*, which allows for overlap, but minimizes a measure of the extent of this overlap."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
