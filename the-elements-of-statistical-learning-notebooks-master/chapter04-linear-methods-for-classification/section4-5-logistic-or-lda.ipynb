{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 4.4.5. Logistic Regression or LDA?\n",
    "### Common linearity\n",
    "\n",
    "LDA has the log-posterior odds which are linear functions of x:\n",
    "\n",
    "\\begin{align}\n",
    "\\log\\frac{\\text{Pr}(G=k|X=x)}{\\text{Pr}(G=K|X=x)} &= \\log\\frac{\\pi_k}{\\pi_K} - \\frac{1}{2}(\\mu_k-\\mu_K)^T\\Sigma^{-1}(\\mu_k-\\mu_K) + x^T\\Sigma^{-1}(\\mu_k-\\mu_K) \\\\\n",
    "&= \\alpha_{k0} + \\alpha_k^Tx,\n",
    "\\end{align}\n",
    "\n",
    "and this linearity is a consequence of the Gaussian assumption for the class densities with a common covariance matrix.\n",
    "\n",
    "The linear logistic model by construction has linear logits:\n",
    "\n",
    "\\begin{equation}\n",
    "\\log\\frac{\\text{Pr}(G=k|X=x)}{\\text{Pr}(G=K|X=x)} = \\beta_{k0} + \\beta_k^Tx\n",
    "\\end{equation}\n",
    "\n",
    "It seems that the models are the same, and they have the common logit-linear form for the posterior probabilities:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Pr}(G=k|X=x) = \\frac{\\exp(\\beta_{k0}+\\beta_k^Tx)}{1+\\sum_{l=1}^{K-1}\\exp(\\beta_{l0}+\\beta_l^Tx)}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different assumptions\n",
    "\n",
    "Although they have exactly the same form, the difference lies in the way the linear coefficients are estimated: The logistic regression model is more general, in that it makes less assumptions.\n",
    "\n",
    "Note the *joint density* of $X$ and $G$ as\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Pr}(X,G=k) = \\text{Pr}(X)\\text{Pr}(G=k|X),\n",
    "\\end{equation}\n",
    "\n",
    "where $\\text{Pr}(X)$ denotes the marginal density of the inputs $X$.\n",
    "\n",
    "The logistic regression model leaves $\\text{Pr}(X)$ as an arbitrary density function, and fits the parameters of $\\text{Pr}(G|X)$ by maximizing the *conditional likelihood* -- the multinomial likelihood with probabilities the $\\text{Pr}(G=k|X)$. Although $\\text{Pr}(X)$ is totally ignored, we can think of this marginal density as being estimated in a fully nonparametric and unrestricted fashion, using empirical distribution function which places mass $1/N$ at each observation.\n",
    "\n",
    "LDA fits the parameters by maximizing the full log-likelihood, based on the joint density\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Pr}(X,G=k) = \\phi(X;\\mu_k,\\Sigma)\\pi_k,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\phi$ is the Gaussian density function. Standard normal theory leads easily to the estimates $\\hat\\mu_k$, $\\hat\\Sigma$, and $\\hat\\pi_k$ given in Section 4.3. Since the linear parameters of the logistic form\n",
    "\n",
    "\\begin{equation}\n",
    "\\log\\frac{\\text{Pr}(G=k|X=x)}{\\text{Pr}(G=K|X=x)} = \\log\\frac{\\pi_k}{\\pi_K} - \\frac{1}{2}(\\mu_k-\\mu_K)^T\\Sigma^{-1}(\\mu_k-\\mu_K) + x^T\\Sigma^{-1}(\\mu_k-\\mu_K)\n",
    "\\end{equation}\n",
    "\n",
    "are functions of the Gaussian parameters, we get their maximum-likelihood estimates by plugging in the corresponding estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role of the marginal density $\\text{Pr}(X)$ in LDA\n",
    "\n",
    "However, unlike in the conditional case, the marginal density $\\text{Pr}(X)$ does play a role here. It is a mixture density\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Pr}(X) = \\sum_{k=1}^K \\pi_k\\phi(X;\\mu_k,\\Sigma),\n",
    "\\end{equation}\n",
    "\n",
    "which also involves the parameters. What role can this additional component or restriction play?\n",
    "\n",
    "By relying on the additional model assumptions, we have more information about the parameters, and hence can estimate them more efficiently (lower variance). If in fact the true $f_k(x)$ are Gaussian, then in the worst case ignoring this marginal part of the likelihood constitutes a loss of efficiency of about $30\\%$ asymptotically in the error rate (Efron, 1975). Paraphrasing: With $30\\%$ more data, the conditional likelihood will do as well.\n",
    "\n",
    "For example, observations far from the decision boundary (which are down-weighted by logistic regression) play a role in estimating the common covariance matrix. This is not a good news, because it also means that LDA is not robust to gross outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marginal likelihood as a regularizer\n",
    "\n",
    "The marginal likelihood can be thought of as a regularizer, requiring in some sense that class densities be *visible* from this marginal view.\n",
    "\n",
    "For example, if the data in a two-class logistic regression model can be perfectly separated by a hyperplane, the maximum likelihood estimates of the parameters are undefined (i.e., infinite; see Exercise 4.5).\n",
    "\n",
    "The LDA coefficients for the same data will be well defined, since the marginal likelihood will not permit these degeneracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In real world\n",
    "\n",
    "> It is generally felt that logistic regression is a safer and more robust bet than the LDA model, relying on fewer assumptions.\n",
    "\n",
    "In practice these assumptions are never correct, and often some of the components of $X$ are qualitative variables. It is our experience that the models give very similar results, even when LDA is used inappropriately, such as with qualitative predictors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
