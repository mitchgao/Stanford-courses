{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4. Linear Methods for Classification\n",
    "# $\\S$ 4.1. Introduction\n",
    "\n",
    "Since our predictor $G(x)$ takes values in a discrete set $\\mathcal{G}$, we can always divide the input space into a collection of regions labeled according to the classification. We saw in Chapter 2 that the boundaries of these regions can be rough or smooth, depending on the prediction function. For an important class of procedures, these *decision boundaries* are linear; this is what we will mean by linear methodds for classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression\n",
    "\n",
    "In Chapter 2 we fit linear regression models to the class indicator variable, and classify to the largest fit. Suppose there are $K$ classes labeled $1,\\cdots,K$, and the fitted linear model for the $k$th indicator response variable is\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}_k(x) = \\hat\\beta_{k0} + \\hat\\beta_k^Tx.\n",
    "\\end{equation}\n",
    "\n",
    "The decision boundary between class $k$ and $l$ is that set of points\n",
    "\n",
    "\\begin{equation}\n",
    "\\left\\lbrace x: \\hat{f}_k(x) = \\hat{f}_l(x) \\right\\rbrace = \\left\\lbrace x: (\\hat\\beta_{k0}-\\hat\\beta_{l0}) + (\\hat\\beta_k-\\hat\\beta_l)^Tx = 0 \\right\\rbrace,\n",
    "\\end{equation}\n",
    "\n",
    "which is an affine set or hyperplane. Since the same is true for any pair of classes, the input space is divided into regions of constant classification, with piecewise hyperplanar decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminant function\n",
    "\n",
    "The regression approach is a member of a class of methods that model *discriminant functions* $\\delta_k(x)$ for each class, and then classify $x$ to the class with the largest value for its discriminant function. Methods that model the posterior probabilities $\\text{Pr}(G=k|X=x)$ are also in this class. Clearly, if either the $\\delta_k(x)$ or $\\text{Pr}(G=k|X=x)$ are linear in $x$, then the decision boundaries will be linear.\n",
    "\n",
    "### Logit transformation\n",
    "\n",
    "Actually, all we require is that some monotone transformation of $\\delta_k$  or $\\text{Pr}(G=k|X=x)$ be linear for the decision boundaries to be linear. For example, if there are two classes, a popular model for the posterior probabilities is\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Pr}(G=1|X=x) &= \\frac{\\exp(\\beta_0+\\beta^Tx)}{1+\\exp(\\beta_0+\\beta^Tx)},\\\\\n",
    "\\text{Pr}(G=2|X=x) &= \\frac{1}{1+\\exp(\\beta_0+\\beta^Tx)},\\\\\n",
    "\\end{align}\n",
    "\n",
    "where the monotone transformation is the *logit* transformation\n",
    "\n",
    "\\begin{equation}\n",
    "\\log\\frac{p}{1-p},\n",
    "\\end{equation}\n",
    "\n",
    "and in fact we see that\n",
    "\n",
    "\\begin{equation}\n",
    "\\log\\frac{\\text{Pr}(G=1|X=x)}{\\text{Pr}(G=2|X=x)} = \\beta_0 + \\beta^Tx.\n",
    "\\end{equation}\n",
    "\n",
    "The decision boundary is the set of points for which the *log-odds* are zero, and this is a hyperplane defined by\n",
    "\n",
    "\\begin{equation}\n",
    "\\left\\lbrace x: \\beta_0+\\beta^Tx = 0 \\right\\rbrace.\n",
    "\\end{equation}\n",
    "\n",
    "We discuss two very popular but different methods that result in linear log-odds or logits: Linear discriminant analysis and linear logistic regression. Although they differ in their derivation, the essential difference between them is in the way the lineaer function is fir to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating hyperplanes\n",
    "\n",
    "A more direct approach is to explicitly model the boundaries between the classes as linear. For a two-class problem, this amounts to modeling the decision boundary as a hyperplane; a normal vector and a cut-point.\n",
    "\n",
    "We will look at two methods that explicitly look for \"separating hyperplanes\".\n",
    "1. The well-known *perceptron* model of Rosenblatt (1958), with an algorithm that finds a separating hyperplane in the training data, if one exists.\n",
    "2. Vapnik (1996) finds an *optimally separating hyperplane* if one exists, else finds a hyperplane that minimizes some measure of overlap in the training data.\n",
    "\n",
    "We treat separable cases here, and defer the nonseparable case to Chapter 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scope for generalization\n",
    "\n",
    "We can expand the input by including their squares $X_1^2,X_2^2,\\cdots$, and cross-products $X_1X_2,\\cdots$, thereby adding $p(p+1)/2$ additional variables. Linear functions in the augmented space map down to quadratic decision boundaires. FIGURE 4.1 illustrates the idea.\n",
    "\n",
    "This approach can be used with any basis transformation $h(X): \\mathbb{R}^p\\mapsto\\mathbb{R}^q$ with $q > p$, and will be explored in later chapters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
