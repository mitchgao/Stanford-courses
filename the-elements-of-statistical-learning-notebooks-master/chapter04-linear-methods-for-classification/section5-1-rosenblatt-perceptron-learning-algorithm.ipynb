{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 4.5.1. Rosenblatt's Perceptron Learning Algorithm\n",
    "\n",
    "> The *perceptron learning algorithm* tries to find a separating hyperplane by minimizing the distance of misclassified points to the decision boundary.\n",
    "\n",
    "If a response $y_i=1$ is misclassified, then $x_i^T\\beta + \\beta_0 \\lt 0$, and the opposite for a misclassified response with $y_i=-1$. The goal is to minimize\n",
    "\n",
    "\\begin{equation}\n",
    "D(\\beta,\\beta_0) = -\\sum_{i\\in\\mathcal{M}} y_i(x_i^T\\beta + \\beta_0),\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathcal{M}$ indexes the set of misclassified points. The quantity is non-negative and proportional to the distance of the misclassified points to the decision boundary defined by $\\beta^Tx+\\beta_0=0$.\n",
    "\n",
    "Assuming $\\mathcal{M}$ is fixed, the gradient is given by\n",
    "\n",
    "\\begin{align}\n",
    "\\partial\\frac{D(\\beta,\\beta_0)}{\\partial\\beta} &= -\\sum_{i\\in\\mathcal{M}} y_ix_i, \\\\\n",
    "\\partial\\frac{D(\\beta,\\beta_0)}{\\partial\\beta_0} &= -\\sum_{i\\in\\mathcal{M}} y_i.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descent\n",
    "\n",
    "The algorithm in face uses *stochastic gradient descent* to minimize this piecewise linear criterion. This means that rather than computing the sum of the gradient contributions of each observation followed by a step in the negative gradient direction, a step in taken after each observation is visited.\n",
    "\n",
    "Hence the misclassified observations are visited in some sequence, and the parameters $\\beta$ are updated via\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{pmatrix}\\beta \\\\ \\beta_0\\end{pmatrix}\n",
    "\\leftarrow\n",
    "\\begin{pmatrix}\\beta \\\\ \\beta_0\\end{pmatrix}\n",
    "+\n",
    "\\rho \\begin{pmatrix}y_ix_i \\\\ y_i\\end{pmatrix},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\rho$ is the learning rate, which in this case can be taken to be $1$ WLOG.\n",
    "\n",
    "If the classes are linearly separable, it can be shown that the algorithm converges to a separating hyperplane in a finite number of steps (Exercise 4.6). FIGURE 4.14 shows two solutions to a toy problem, each started at a different random guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under construction ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"FIGURE 4.14. A toy example with two classes separable by a hyperplane.\n",
    "\n",
    "The orange line is the least squares solution, which misclassifies one of\n",
    "the training points. Also shown are two blue separating hyperplanes found\n",
    "by the perceptron learning algorithm with different random starts.\n",
    "\"\"\"\n",
    "print('Under construction ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues\n",
    "\n",
    "There are a number of problems with this algorithm, summarized in Ripley (1996):\n",
    "* When the data are separable, there are many solutions, and which one is found depends on the starting values.\n",
    "* The \"finite\" number of steps can be very large. The smaller the gap, the longer the time to find it.\n",
    "* When the data are not separable, the algorithm will not converge, and cycles develop. The cycles can be long and therefore hard to detect.\n",
    "\n",
    "The second problem can often be eliminated by seeking a hyperplane not in the orignal space, but in a much enlarged space obtained by creating many basis-function transformations of the original variables. This is analogous to driving the residuals in a ploynomial regression problem down to zero by making the degree sufficiently large.\n",
    "\n",
    "Perfect separation cannot always be achieved: For example, if observations from two different classes share the same input. It may not be desirable either, since the resulting model is likely to be overfit and will not generalizes well.\n",
    "\n",
    "A rather elegant solution to the first problem is to add additional constraints to the separating hyperplane."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
