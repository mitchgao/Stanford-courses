{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 7.2. Bias, Variance, and Model Complexity\n",
    "\n",
    "FIGURE 7.1 illustrates the important issue in assessing the ability of a learning method to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under construction ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"FIGURE 7.1. Behavior of test sample and training sample error\n",
    "as the model complexity varied.\n",
    "\n",
    "The light blue curves show the training error, while\n",
    "the light red curves show the conditional test error\n",
    "for 100 training sets of size 50 each, as the model complexity increased.\n",
    "\n",
    "The solid curves show the expected test error and the expected training error.\n",
    "\n",
    "The lasso was used to produce the sequence of fits.\"\"\"\n",
    "print('Under construction ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics\n",
    "Consider first the case of a quantitative or interval scale response. We have\n",
    "* a target variable $Y$,\n",
    "* a vector of inputs $X$, and\n",
    "* a prediction model $\\hat{f}(X)$ that has been estimated from a training set $\\mathcal{T}$.\n",
    "\n",
    "The loss function for measuring errors between $Y$ and $\\hat{f}(X)$ is denoted by $L(Y, \\hat{f}(X))$. Typical choices are\n",
    "\n",
    "\\begin{equation}\n",
    "L(Y,\\hat{f}(X)) = \\begin{cases}\n",
    "(Y-\\hat{f}(X))^2 & \\text{squared error,} \\\\\n",
    "|Y-\\hat{f}(X)| & \\text{absolute error.}\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "_Test error_, also referred to as _generalization error_, is the prediction error over an independent test sample\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Err}_\\mathcal{T} = \\text{E}\\left[ L(Y,\\hat{f}(X)) | \\mathcal{T} \\right]\n",
    "\\end{equation}\n",
    "\n",
    "where both $X$ and $Y$ are drawn randomly from their joint distribution (population). Here the training set $\\mathcal{T}$ is fixed, and test error refers to the error for this specific training set. A related quantity is the expected prediction error (or expected test error)\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Err} = \\text{E}\\left[ L(Y,\\hat{f}(X)) \\right] = \\text{E}\\left[ \\text{Err}_\\mathcal{T} \\right]\n",
    "\\end{equation}\n",
    "\n",
    "Note that this expectation averages over everything that is random, including the randomness in the training set that produces $\\hat{f}$.\n",
    "\n",
    "FIGURE 7.1 shows the prediction error (light red curves) $\\text{Err}_\\mathcal{T}$ for 100 simulated training sets each of size 50. The lasso ($\\S$ 3.4.2) was used to produce the sequence of fits. The solid red curve is the average, and hence an estimate of $\\text{Err}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible to estimate $\\text{Err}$?\n",
    "\n",
    "Estimation of $\\text{Err}_\\mathcal{T}$ will be our goal, although we will see that $\\text{Err}$ is more amenable to statistical analysis, and most methods effectively estimate the expected error. It does not seem possible to estimate conditional error effectively, given only the information in the same training set. Some discussion of this point is given in $\\S$ 7.12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model complexity\n",
    "\n",
    "_Training error_ is the average loss over the training sample\n",
    "\n",
    "\\begin{equation}\n",
    "\\overline{\\text{err}} = \\frac1N \\sum_{i=1}^N L(y_i,\\hat{f}(x_i)).\n",
    "\\end{equation}\n",
    "\n",
    "We would like to know the expected test error of our estimated model $\\hat{f}$. As the model becomes more and more complex, it uses the training data more and is able to adapt to more compicated underlying structures. Hence there is a decrease in bias but an increase in variance.\n",
    "\n",
    "There is some intermediate model complexity that give minimum expected test error.\n",
    "\n",
    "Unfortunately training error is not a good estimate of the test error, as seen in FIGURE 7.1. Training error consistently decreases with model complexity. However, a model with zero training error is overfit to the training data and will typically generalize poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The similar story for categorical response\n",
    "\n",
    "Let $G$ be a categorical response taking one of $K$ values in a set $\\mathcal{G} = \\{1,2,\\cdots,K\\}$.\n",
    "\n",
    "Typically we model the probabilities\n",
    "\n",
    "\\begin{equation}\n",
    "p_k(X) = \\text{Pr}(G=k|X),\n",
    "\\end{equation}\n",
    "\n",
    "or some monotone transformation $f_k(X)$, and then\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{G}(X) = \\arg\\max_k \\hat{p}_k(X).\n",
    "\\end{equation}\n",
    "\n",
    "In some cases, such as 1NN classification (Chapter 2 and 13) we produce $\\hat{G}(X)$ directly.\n",
    "\n",
    "#### Loss\n",
    "Typical loss functions are\n",
    "\n",
    "\\begin{align}\n",
    "L(G, \\hat{G}(X)) &= I(G \\neq \\hat{G}(X)) &\\text{ (0-1 loss)}, \\\\\n",
    "L(G, \\hat{p}(X)) &= -2\\sum_{k=1}^K I(G=k) \\log \\hat{p}_k(X) \\\\\n",
    "&= -2\\log \\hat{p}_G(X) &\\text{ (}-2 \\times \\text{log-likelihood)}.\n",
    "\\end{align}\n",
    "\n",
    "The quantity $-2\\times\\text{log-likelihood}$ is sometimes referred to as the _deviance_.\n",
    "\n",
    "#### Test error\n",
    "Again, test error here is\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Err}_{\\mathcal{T}} = \\text{E}\\left[ L(G, \\hat{G}(X)) \\mid \\mathcal{T} \\right],\n",
    "\\end{equation}\n",
    "\n",
    "the population misclassification error of the classifier trained on $\\mathcal{T}$, and $\\text{Err}$ is the expected misclassification error.\n",
    "\n",
    "#### Training error\n",
    "Training error is the sample analogue, e.g.,\n",
    "\n",
    "\\begin{equation}\n",
    "\\overline{\\text{err}} = -\\frac2N \\sum_{i=1}^N \\log \\hat{p}_{g_i}(x_i),\n",
    "\\end{equation}\n",
    "\n",
    "the sample log-likelihood for the model.\n",
    "\n",
    "#### Log-likelihood as a loss\n",
    "The log-likelihood can be used as a loss-function for general response densities, such as the Poisson, gamma, exponential, log-normal and others. If $\\text{Pr}_{\\theta(X)}(Y)$ is the density of $Y$, indexed by a parameter $\\theta(X)$ that depends on the predictor $X$, then\n",
    "\n",
    "\\begin{equation}\n",
    "L(Y,\\theta(X)) = -2 \\log \\text{Pr}_{\\theta(X)}(Y).\n",
    "\\end{equation}\n",
    "\n",
    "The \"-2\" in the definition makes the log-likelihood loss for the Gaussian distribution match squared-error loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplified assumptions & notations\n",
    "\n",
    "> For ease of exposition, for the remainder of this chapter we will use $Y$ and $f(X)$ to represent all of the above situation, since we focus mainly on the quantitative response (squared-error loss) setting. For the other situations, the appropriate translations are obvious.\n",
    "\n",
    "In this chapter we describe a number of methods for estimating the expected test error for a model. Typically our model will have a tuning parameter(s) $\\alpha$ and so we can write our predictions as $\\hat{f}_\\alpha(x)$. The tuning parameter varies the complexity of our model, and we wish to find the value of $\\alpha$ that minimizes error, i.e., produces the minimum of the average test error curve in FIGURE 7.1. Having said this, for brevity we will often suppress the dependence of $\\hat{f}(x)$ on $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection and model assessment\n",
    "\n",
    "It is important to note that there are in fact two separate goals that we might have in mind:\n",
    "\n",
    "* __Model selection__: estimating the performance of different models in order to choose the best one.\n",
    "* __Model assessment__: having chosen a final model, estimating its prediction error (generalization error) on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data set for training, validation, and testing\n",
    "\n",
    "If we are in a data-rich situation, the best approach for both problems is to randomly divide the dataset into three parts: A training set, a validation set, and a test set.\n",
    "* The training set is used to fit the models;\n",
    "* the validation set is used to estimate prediction error for model selection;\n",
    "* the test set is used for assessment of the generalization error of the final chosen model.\n",
    "\n",
    "Ideally, the test set should be kept in a \"vault\", and be brought out only at the end of the data analysis. Suppose instead that we use the test set repeatedly, choosing the model with smallest test-set error. Then the test-set error of the final chosen model will underestimate the true test error, sometimes substantially.\n",
    "\n",
    "It is difficult to give a general rule on how to choose the number of observations in each of the three parts, as this depends on the signal-to-noise ratio (SNR) in the data and the training sample size. A typical split might be 50% for training, and 25% each for validation and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The major topic from now on: What if data is insufficient\n",
    "\n",
    "The methods in this chapter are designed for situations where there is insufficient data to split into three parts. Again it is too difficult to give a general rule on how much training data is enough; among other things, this depends on the SNR of the underlying function, and the complexity of the models being fit to the data.\n",
    "\n",
    "The methods of this chapter approximate the validation step either\n",
    "* analytically (AIC, BIC, MDL, SRM) or\n",
    "* by efficient sample re-use (cross validation and the bootstrap).\n",
    "\n",
    "Besides their use in model selection, we also examine to what extent each method provides a reliable estimate of test error of the final chosen model.\n",
    "\n",
    "Before jumping into these topics, we first explore in more detail the nature of test error and the bias-variance tradeoff."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
