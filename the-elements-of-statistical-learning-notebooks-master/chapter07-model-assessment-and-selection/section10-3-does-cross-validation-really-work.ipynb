{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 7.10.3. Does Cross-Validation Really Work?\n",
    "\n",
    "We once again examine the behavior of cross-validation in a high-dimensional classification problem.\n",
    "\n",
    "Consider a scenario with $N=20$ samples in two equal-sized classes, and $p=500$ quantitative predictors that are independent of the class labels. Once again, the true error rate of any classifier is 50%.\n",
    "\n",
    "Consider a simple univariate classifier: A single split that minimizes the misclassification error (a \"stump\"). Stumps are trees with a single split, and are used in boosting methods (Chapter 10). A simple argument suggests that cross-validation will not work properly in this setting (This argument was made to us by a scientist at a proteomics lab meeting, and led to material in this section):\n",
    "\n",
    "> Fitting to the entire training set, we will find a predictor that splits the data very well. If we do 5-fold cross-validation, this same predictor should split any 4/5ths and 1/5th of the data well too, and hence its cross-validation error will be small (much less than 50%). Thus CV does not give an accurate estimate of error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation\n",
    "\n",
    "To investigate whether this argument is correct, FIGURE 7.11 shows the result of a simulation from this setting.\n",
    "\n",
    "There are 500 predictors and 20 samples, in each of two equal-sized classes, with all predictors having a standard Gaussian distribution.\n",
    "\n",
    "The panel in the top left shows the number of training errors for each of the 500 stumps fit to the training data. We have marked in color the six predictor yielding the fewest errors.\n",
    "\n",
    "In the top right panel, the training errors are shown for stumps fit to a random 4/5ths partition of the data (16 samples), and tested on the remaining 1/5th (four samples). The colored points indicate the same predictors marked in the top left panel. We see that the stump for the blue predictor (whose stump was the best in the top left panel), makes two out of four test errors (50%), and is no better than random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Necessary to reproduce this figure?\n"
     ]
    }
   ],
   "source": [
    "\"\"\"FIGURE 7.11. Simulation study to investigate the performance of cross-validation in a\n",
    "high-dimensional problem where the predictors are independent of the class labels.\n",
    "\n",
    "The top-left panel shows the number of errors made by individual stump classifiers on\n",
    "the full training set (20 observations).\n",
    "\n",
    "The top right panel shows the errors made by individual stumps trained on a random split of\n",
    "the dataset into 4/5ths (16 observations) and tested on the remaining 1/5th (4 observations).\n",
    "\n",
    "The best performers are depicted by colored dots in each panel.\n",
    "\n",
    "The bottom left panel shows the effect of re-estimating the split point in each fold:\n",
    "The colored points correspond to the four samples in the 1/5th validation set. The split\n",
    "point derived from the full dataset classifies all four samples correctly, but when the split\n",
    "point is re-estimated on the 4/5ths data (as it should be), it commits two errors on the four\n",
    "validation samples.\n",
    "\n",
    "In the bottom right we see the overall result of five-fold cross-validation applied to\n",
    "50 simulated datasets. The average error rate is about 50%, as it should be.\"\"\"\n",
    "N, p, K = 20, 500, 5\n",
    "print('Necessary to reproduce this figure?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What has happened? The preceding argument has ignored the fact that in cross-validation, the model must be completely retrained for each fold of the process.\n",
    "\n",
    "In the present example, this means that the best predictor and corresponding split point are found from 4/5ths of the data. The effect of predictor choice is seen in the top right panel. Since the class labels are independent of the predictors, the performance of a stump on the 4/5ths training data contains no information about its performance in the remaining 1/5th.\n",
    "\n",
    "The effect of the choice of split point is shown in the bottom left panel. Here we see the data for predictor 436, corresponding to the blue dot in the top left plot. The colored points indicate the 1/5th data, while the remaining points belong to the 4/5ths. The optimal split points for this predictor based on both the full training set and 4/5ths data are indicated.\n",
    "\n",
    "The split based on the full data makes no errors on the 1/5ths data. But cross-validation must base its split on the 4/5ths data, and this incurs two errors out of four samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of applying five-fold cross-validation to each of 50 simulated datasets is shown in the bottom right panel. As we would hope, the average cross-validation error is around 50%, which is the true expected prediction error for this classifier. Hence cross-validation has behaved as it should.\n",
    "\n",
    "On the other hand, there is considerable variability in the error, underscoring the importance of reporting the estimated standard error of the CV estimate. See Exercise 7.10 for another variation of this problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
