{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 7.5. Estimates of In-Sample Prediction Error\n",
    "\n",
    "The general form of the in-sample estimates is\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\text{Err}_\\text{in}} = \\overline{\\text{err}} + \\hat\\omega,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\hat\\omega$ is an estimate of the average optimism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $C_p$\n",
    "\n",
    "Using the expression (simplified with assumption; linear fit of additive model)\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{E}_\\mathbf{y}(\\text{Err}_\\text{in}) = \\text{E}_\\mathbf{y}(\\overline{\\text{err}}) + 2\\cdot\\frac{d}{N} \\sigma_\\epsilon^2,\n",
    "\\end{equation}\n",
    "\n",
    "applicable when $d$ parameters are fit under squared error loss, leads to a version of the so-called $C_p$ statistic,\n",
    "\n",
    "\\begin{equation}\n",
    "C_p = \\overline{\\text{err}} + 2\\cdot \\frac{d}{N} \\hat\\sigma_\\epsilon^2.\n",
    "\\end{equation}\n",
    "\n",
    "Here $\\hat\\sigma_\\epsilon^2$ is an estimate of the noise variance, obtained from the mean-squared error of a low-bias model.\n",
    "\n",
    "Using this criterion we adjust the training error by a factor proportional to the number of basis functions used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\text{AIC}$\n",
    "\n",
    "The _Akaike information criterion_ is a similar but more generally applicable estimate of $\\text{Err}_\\text{in}$ when a log-likelihood loss function is used. It relies on a relationship that holds asymptotically as $N \\rightarrow \\infty$:\n",
    "\n",
    "\\begin{equation}\n",
    "-2\\cdot \\text{E}\\left[ \\log \\text{Pr}_{\\hat\\theta}(Y) \\right] \\approx -\\frac{2}{N} \\cdot \\text{E}\\left[\\text{loglik}\\right] + 2\\cdot \\frac{2}{N}.\n",
    "\\end{equation}\n",
    "\n",
    "Here $\\text{Pr}_{\\hat\\theta}(Y)$ is a family of densities for $Y$ (containing the \"true\" density), $\\hat\\theta$ is the maximum-likelihood estimate of $\\theta$, and \"loglik\" is the maximized log-likelihood:\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{loglik} = \\sum_{i=1}^N \\log\\text{Pr}_{\\hat\\theta} (y_i).\n",
    "\\end{equation}\n",
    "\n",
    "For example, for the logistic regression model, using the binomial log-likelihood, we have\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{AIC} = -\\frac{2}{N}\\cdot\\text{loglik} + 2\\cdot\\frac{d}{N}.\n",
    "\\end{equation}\n",
    "\n",
    "For the Gaussian model (with variance $\\sigma_\\epsilon^2=\\hat\\sigma_\\epsilon^2$ assumed known). the $\\text{AIC}$ statistic is equivalent to $C_p$, and so we refer to them collectively as $\\text{AIC}$.\n",
    "\n",
    "To use $\\text{AIC}$ for model selection, we simply choose the model giving smallest $\\text{AIC}$ over the set of models considered. For nonlinear and other complex models, we need to replace $d$ by some measure of model complexity. We discuss this in $\\S$ 7.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\text{AIC}$ for more general models\n",
    "\n",
    "Given a set of models $f_\\alpha(x)$ indexed by a tuning parameter $\\alpha$, denote\n",
    "* the training error by $\\overline{\\text{err}}(\\alpha)$ and\n",
    "* the number of parameters by $d(\\alpha)$\n",
    "\n",
    "for each model. Then for this set of models we define\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{AIC}(\\alpha) = \\overline{\\text{err}}(\\alpha) + 2\\cdot\\frac{d(\\alpha)}{N}\\hat\\sigma_\\epsilon^2.\n",
    "\\end{equation}\n",
    "\n",
    "The function $\\text{AIC}(\\alpha)$ provides an estimate of the test error curve, and we find the tuning parameter $\\hat\\alpha$ that minimizes it. Our final chosen model is $f_{\\hat\\alpha}(x)$.\n",
    "\n",
    "Note that if the basis functions are chosen adaptively, it does not hold any longer;\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^N \\text{Cov}(\\hat{y}_i, y_i) = d\\sigma_\\epsilon^2\n",
    "\\end{equation}\n",
    "\n",
    "For example, if we have a total of $p$ inputs, and we choose the best-fitting linear model with $d<p$ inputs, the optimism will exceed $(2d/N)\\sigma_\\epsilon^2$. Put another way, by choosing the best-fitting model with $d$ inputs, the _effective number of parameters_ fit is more than $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIGURE 7.4 shows $\\text{AIC}$ in action for the phoneme recognition example of $\\S$ 5.2.3 on page 148. Briefly speaking,\n",
    "\n",
    "* The input vector is the log-periodogram of the spoken vowel, quantized to 256 uniformly spacedd frequencies.\n",
    "* A linear logistic regression model is used to predict the phoneme class, with coefficient function\n",
    "\n",
    "  \\begin{equation}\n",
    "  \\beta(f) = \\sum_{m=1}^M h_(f)\\theta_m,\n",
    "  \\end{equation}\n",
    "\n",
    "  an expansion in $M$ spline basis functions.\n",
    "* For any given $M$, a basis of natural cubic splines is used for the $h_m$, with knots chosen uniformly over the range of frequencies (so $d(\\alpha) = d(M) = M$).\n",
    "\n",
    "Using $\\text{AIC}$ to select the number of basis functions will approximately minimize $\\text{Err}(M)$ for both entropy and 0-1 loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under construction ...\n"
     ]
    }
   ],
   "source": [
    "\"\"\"FIGURE 7.4.\"\"\"\n",
    "print('Under construction ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple formula\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{2}{N}\\sum_{i=1}^N \\text{Cov}(\\hat{y}_i, y_i) = \\frac{2d}{N}\\sigma_\\epsilon^2\n",
    "\\end{equation}\n",
    "\n",
    "holds exactly for linear models with additive errors and squared error loss, and approximately for linear models and log-likelihoods.\n",
    "\n",
    "In particular, the formula does not hold in general for 0-1 loss (Efron, 1986), although many authors nevertheless use it in that context (right panel of FIGURE 7.4)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
