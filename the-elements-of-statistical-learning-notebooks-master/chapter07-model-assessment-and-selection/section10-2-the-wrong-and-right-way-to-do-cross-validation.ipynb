{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 7.10.2. The Wrong and Right Way to Do Cross-Validation\n",
    "\n",
    "Consider a classification problem with a large number of predictors, as may arise, for example, in genomic or proteomic applications. A typical strategy for analysis might be as follows:\n",
    "\n",
    "1. Screen the predictors: find a subset of \"good\" predictors that show fairly strong (univariate) correlation with the class labels.\n",
    "1. Using just this subset of predictors, build a multivariate classifier.\n",
    "1. Use cross-validation to estimate the unknown tuning parameters and to estimate the prediction error of the final model.\n",
    "\n",
    "Is this a correct application of cross-validation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a scenario with $N=50$ samples in two equal-sized classes, and $p=5000$ quantitative predictors (standard Gaussian) that are independent of the class labels. The true (test) error rate of any classifier is 50%.\n",
    "\n",
    "We carried out the above recipe,\n",
    "1. choosing in step (1) the 100 predictors having highest correlation with the class labels, and then\n",
    "1. using a 1NN classifier, based on just these 100 predictors, in step (2).\n",
    "\n",
    "Over 50 simulations from this setting, the average CV error rate was 3%. This is far lower than the true error rate of 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What happened?\n",
    "\n",
    "The problem is that the predictors have an unfair advantage, as they were chosen in step (1) on the basis of _all of the samples_. Leaving samples out _after_ the variables have been selected does not correctly mimic the application of the classifier to a completely independent test set, since these predictors \"have already seen\" the left out samples.\n",
    "\n",
    "FIGURE 7.10 (top panel) illustrates the problem. We selected the 100 predictors having largest correlation with the class labels over all 50 samples. Then we chose a random set of 10 samples, as we would do in five-fold cross-validation, and computed the correlations of the pre-selected 100 predictors with the class labels over just these 10 samples (top panel). We see that the correlations average about 0.28, rather than 0, as one might expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the correct way to carry out cross-validation in this example:\n",
    "1. Divide the samples into $K$ cross-validation folds (groups) at random.\n",
    "1. For each fold $k=1,2,\\cdots,K$\n",
    "  1. Find a subset of \"good\" predictors that show fairly strong (univariate) correlation with the class labels, using all of the samples except those in fold $k$.\n",
    "  1. Using just this subset of predictors, build a multivariate classifier, using all of the samples except those in fold $k$.\n",
    "  1. Use the classifier to predict the class labels for the samples in fold $k$.\n",
    "\n",
    "The error estimates from step 2(c) are then accumulated over all $K$ folds, to produce the cross-validation estimate of prediction error. The lower panel of FIGURE 7.10 shows the correlations of class labels with the 100 predictors chosen in step 2(a) of the correct procedure, over the samples in a typical fold $k$. We see that they average about zero, as they should."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, with a multistep modeling procedure, cross-validation must be applied to the entire sequence of modeling steps. In particular, samples must be \"left out\" before any selection or filtering steps are applied.\n",
    "\n",
    "> There is one qualification: Initial _unsupervised_ screening steps can be done before samples are left out.\n",
    "\n",
    "For example, we could select the 1000 predictors with highest variance across all 50 samples, before starting cross-validation. Since this filtering does not involve the class labels, it does not give the predictors an unfair advantage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even in published papers!\n",
    "\n",
    "While this point may seem obvious to the reader, we have seen this blunder committed many times in published papers in top rank journals. With the large numbers of predictors that are so common in genomic and other areas, the potential consequences of this error have also increased dramatically; see Ambroise and McLachlan (2002) for a detailed discussion of this issue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
