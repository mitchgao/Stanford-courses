{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\S$ 7.3.1. Example: Bias-Variance Tradeoff\n",
    "\n",
    "### Example specification\n",
    "\n",
    "FIGURE 7.3 shows the bias-variance tradeoff for two simulated examples. There are 80 observations and 20 predictors, uniformly distributed in the hypercube $[0,1]^{20}$. The situations are as follows:\n",
    "\n",
    "* _Left panels_: $Y=\\begin{cases}0 &\\text{ if } X_1 \\le 0.5 \\\\ 1 &\\text{ otherwise,}\\end{cases}$ and we apply kNN.\n",
    "* _Right panels_: $Y=\\begin{cases}1 &\\text{ if } \\sum_{j=1}^{10} X_j \\gt 5 \\\\ 5 &\\text{ otherwise,}\\end{cases}$ and we use best subset linear regression of size $p$.\n",
    "\n",
    "* The top row is regression with squares error loss;\n",
    "* the bottom row is classification with 0-1 loss.\n",
    "\n",
    "The figures show\n",
    "* the prediction error (red),\n",
    "* squared bias (green), and\n",
    "* variance (blue),\n",
    "\n",
    "all computed for a large test sample.\n",
    "\n",
    "In the regression problem, bias and variance add to produce the prediction error curve, with minima at about $k=5$ for $k$NN, and $p\\ge 10$ for the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FIGURE 7.3. simulation for bias-variance tradeoff\"\"\"\n",
    "%matplotlib inline\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set\n",
    "size_training = 80\n",
    "size_predictor = 20\n",
    "train_x = scipy.rand(size_training, size_predictor)\n",
    "train_x.shape\n",
    "\n",
    "train_y1 = scipy.where(train_x[:,0] <= .5, 0, 1)\n",
    "train_y2 = scipy.where(train_x[:,:10].sum(axis=1) > 5, 1, 5)\n",
    "# print(train_x[:,0], train_y_cls)\n",
    "# print(train_x[:,:10].sum(axis=1), train_y_rgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kNN simulation\n",
    "# kNN regression\n",
    "# kNN classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under construction ...\n"
     ]
    }
   ],
   "source": [
    "# Linear model simulation\n",
    "# Linear regression\n",
    "# Linear classification\n",
    "print('Under construction ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "For classification loss (bottom figures), some interesting phenomena can be seen. The bias and variance curves are the same as in the top figures, and prediction error now refers to misclassification rate. We see that prediction error is no longer the sum of squared bias and variance.\n",
    "\n",
    "1. For the $k$NN classifier, prediction error decreases or stays the same as the number of neighbors is increased to 20, despite the fact that the squared bias is rising.\n",
    "2. For the linear model classifier the minimum occurs for $p\\ge 10$ as in regression, but the improvement over the $p=1$ model is more dramatic.\n",
    "\n",
    "We see that bias and variance seem to interact in determining prediction error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer for Question 1\n",
    "\n",
    "Why does this happen? There is a simple explanation for the first phenomenon.\n",
    "\n",
    "Suppose at a given input point, the true probability of class 1 is $0.9$ while the expected value of our estimate is $0.6$. Then the squared bias -- $(0.6 - 0.9)^2$ -- is considerable, but the prediction error is zero since we make the correct decision.\n",
    "\n",
    "In other words, estimation errors that leave us on the right side of the decision boundary don't hurt. Exercise 7.2 demonstrates this phenomenon analytically, and also shows the interaction effect between bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "The overall point is that the bias-variance tradeoff behaves differently for 0-1 loss than it does for squared error loss. This in turn means that the best choices of tuning parameters may differ substantially in the two settings. One should base the choice of tuning parameter on an estimate of prediction error, as described in the following sections."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
