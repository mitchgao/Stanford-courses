{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 7.6. The Effective Number of Parameters\n",
    "\n",
    "The concept of \"number of parameters\" can be generalized, especially to models where regularization is used in the fitting.\n",
    "\n",
    "Suppose we stack the outcomes $y_1,y_2,\\cdots,y_N$ into a vector $\\mathbf{y}$, and similarly for the predictions $\\hat{\\mathbf{y}}$. Then a linear fitting method is one for which we can write\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{y}} = \\mathbf{Sy},\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{S}$ is an $N\\times N$ matrix depending on the input vectors $x_i$, but not on the $y_i$.\n",
    "\n",
    "Linear fitting methods include\n",
    "* linear regression on the original features or on a derived basis set, and\n",
    "* smoothing methods that use quadratic shrinkage, such as ridge regression and cubic smoothing splines.\n",
    "\n",
    "Then the _effective number of parameters_ (a.k.a. the _effective degrees of freedom_) is defined as\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{df}(\\mathbf{S}) = \\text{trace}(\\mathbf{S}).\n",
    "\\end{equation}\n",
    "\n",
    "Note that if $\\mathbf{S}$ is an orthogonal-projection matrix onto a basis set spanned by $M$ features, then\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{trace}(\\mathbf{S}) = M.\n",
    "\\end{equation}\n",
    "\n",
    "It turns out that $\\text{trace}(\\mathbf{S})$ is exactly the correct quantity to replace $d$ as the number of parameters in the $C_p$ statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For additive-error models\n",
    "\n",
    "If $\\mathbf{y}$ arises from an additive-error model\n",
    "\n",
    "\\begin{equation}\n",
    "Y = f(X) + \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "with $\\text{Var}(\\epsilon) = \\sigma_\\epsilon^2$, then one can show that\n",
    "\n",
    "\\begin{equation}\n",
    "\\sum_{i=1}^N \\text{Cov}(\\hat{y}_i,y_i) = \\text{trace}(\\mathbf{S})\\sigma_\\epsilon^2,\n",
    "\\end{equation}\n",
    "\n",
    "which motivates the more general definition (Exercise 7.4 and 7.5)\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{df}(\\hat{\\mathbf{y}}) = \\frac{\\sum_{i=1}^N \\text{Cov}(\\hat{y}_i,y_i)}{\\sigma_\\epsilon^2}.\n",
    "\\end{equation}\n",
    "\n",
    "$\\S$ 5.4.1 on page 153 gives some more intuition for the definition $\\text{df} = \\text{trace}(\\mathbf{S})$ in the context of smoothing splines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For models like neural networks\n",
    "\n",
    "in which  we minimize an error function $R(\\omega)$ with weight decay penalty (regularization) $\\alpha \\sum_m \\omega_m^2$, the effective number of parameters has the form\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{df}(\\alpha) = \\sum_{m=1}^M \\frac{\\theta_m}{\\theta_m+\\alpha},\n",
    "\\end{equation}\n",
    "\n",
    "where the $\\theta_m$ are the eigenvalues of the Hessian matrix $\\frac{\\partial^2 R(\\omega)}{\\partial\\omega\\partial\\omega^T}$.\n",
    "\n",
    "Expression $(7.34)$ follows from $(7.32)$ if we make a quadratic approximation to the error function at the solution (Bishop, 1995)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
