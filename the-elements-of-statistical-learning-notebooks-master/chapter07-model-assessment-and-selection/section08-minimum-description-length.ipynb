{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 7.8. Minimum Description Length\n",
    "\n",
    "The minimum description length (MDL) approach gives a selection criterion formally indentical to the $\\text{BIC}$ approach, but is motivated from an optimal coding viewpoint.\n",
    "\n",
    "We first review the theory of coding for data compression, and then apply it to model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data compression\n",
    "\n",
    "We think of\n",
    "* our datum $z$ as a message that we want to encode and send to someone else (the \"receiver\"),\n",
    "* our model as a way of encoding the datum,\n",
    "\n",
    "and we will choose the most parsimonious model, that is the shortest code, for the transmission.\n",
    "\n",
    "Suppose\n",
    "* the possible messages we might want to transmit are $z_1,z_2,\\cdots,z_m$.\n",
    "* Our code uses a finite alphabet of length $A$: e.g., we might use a binary code $\\{0,1\\}$ of length $A=2$.\n",
    "\n",
    "Here is an example with four possible messages and a binary coding:\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>Message</td>\n",
    "        <td>$z_1$</td>\n",
    "        <td>$z_2$</td>\n",
    "        <td>$z_3$</td>\n",
    "        <td>$z_4$</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Code</td>\n",
    "        <td>$0$</td>\n",
    "        <td>$10$</td>\n",
    "        <td>$110$</td>\n",
    "        <td>$111$</td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "This code is known as an instantaneous prefix code: No code is the prefix of any other, and the receiver (who knows all of the possible codes), knows exactly when the message has been completely sent. We restrict our discussion to such instantaneous prefix codes.\n",
    "\n",
    "One could use the coding in the above table or we could permute the codes, e.g., use codes $110$, $10$, $111$, $0$ for $z_1,z_2,z_3,z_4$. How do you decide which to use? It depends on how often we will be sending each of the messages. The one most sent to be $0$. Using this kind of strategy -- shorter codes for more frequent messages -- the average message length will be shorter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy\n",
    "\n",
    "In general, if messages are sent with probabilities $\\text{Pr}(z_i)$, $i=1,2,3,4$, a famous theorem due to Shannon says we should use code lengths\n",
    "\n",
    "\\begin{equation}\n",
    "l_i = -\\log_2 \\text{Pr}(z_i)\n",
    "\\end{equation}\n",
    "\n",
    "and the average message length satisfies\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{E}(\\text{length}) \\ge -\\sum \\text{Pr}(z_i) \\log_2 \\text{Pr}(z_i).\n",
    "\\end{equation}\n",
    "\n",
    "The RHS above is also called the entropy of the distribution $\\text{Pr}(z_i)$.\n",
    "\n",
    "The inequality is an equality when the probabilities satisfy $p_i = A^{-l_i}$.\n",
    "\n",
    "In our example, if $\\text{Pr}(z_i) = 1/2, 1/4, 1/8, 1/8$, respectively, then the coding shown in the above table is optimal and achieves the entropy lower bound.\n",
    "\n",
    "In general the lower bound cannot be achieved, but procedures like the Huffman coding scheme can get close to the bound. Note that with an infinite set of messages, the entropy is replaced by\n",
    "\n",
    "\\begin{equation}\n",
    "-\\int \\text{Pr}(z) \\log_2 \\text{Pr}(z) dz.\n",
    "\\end{equation}\n",
    "\n",
    "From this result we glean the following:\n",
    "\n",
    "> To transmit a random variable $z$ having probability density function $\\text{Pr}(z)$, we require about $-\\log_2 \\text{Pr}(z)$ bits of information.\n",
    "\n",
    "We henceforth change notation from $\\log_2 \\text{Pr}(z)$ to $\\log \\text{Pr}(z) = \\log_e \\text{Pr}(z)$; this is for convenience, and just introduces an unimportant multiplicative constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection\n",
    "\n",
    "Now we apply this result to the problem of model selection. We have\n",
    "* a model $M$ with parameters $\\theta$, and\n",
    "* data $\\mathbf{Z} = (\\mathbf{X}, \\mathbf{y})$ consisting of both inputs and outputs,\n",
    "* $\\text{Pr}(\\mathbf{y}|\\theta,M,\\mathbf{X})$, the (conditional) probability of the outputs under the model,\n",
    "* assumed the receiver knows all of the inputs, and\n",
    "* a wish to transmit the outputs.\n",
    "\n",
    "Then the message length required to transmit the outputs is\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{length} = -\\log \\text{Pr}(\\mathbf{y}|\\theta,M,\\mathbf{X}) - \\log\\text{Pr}(\\theta|M),\n",
    "\\end{equation}\n",
    "\n",
    "the log-probability of the target values given the inputs.\n",
    "* The second term is the average code length for transmitting the model parameters $\\theta$,\n",
    "* while the first term is the average code length for transmitting the discrepancy between the model and actual target values.\n",
    "\n",
    "For example suppose we have a single target $y \\sim N(\\theta, \\sigma^2)$, parameter $\\theta \\sim N(0,1)$ and no input (for simplicity). Then the message length is\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{length} = \\text{constant} + \\log\\sigma + \\frac{(y-\\theta)^2}{2\\sigma^2} + \\frac{\\theta^2}{2}\n",
    "\\end{equation}\n",
    "\n",
    "Note that the smaller $\\sigma$ is, the shorter on average is the message length, since $y$ is more concentrated around $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "The MDL principle says that we should choose the model that minimizes\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{length} = -\\log \\text{Pr}(\\mathbf{y}|\\theta,M,\\mathbf{X}) - \\log\\text{Pr}(\\theta|M).\n",
    "\\end{equation}\n",
    "\n",
    "We recognize this as the (negative) log-posterior distribution, and hence minimizing description length is equivalent to maximizing posterior probability. Hence the $\\text{BIC}$ criterion, derived as approximation to log-posterior probability, can also be viewed as a device for (approximate) model choice by MDL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For continuous variables\n",
    "\n",
    "Note that we have ignored the precision with which a random variable $z$ is coded. With a finite code length we cannot code a continuous variable exactly.\n",
    "\n",
    "However, if we code $z$ within a tolerance $\\delta z$, the message length needed is the log of the probability in the interval $[z, z+\\delta z]$ which is well approximated by $\\delta z \\text{Pr}(z)$ if $\\delta z$ is small. Since\n",
    "\n",
    "\\begin{equation}\n",
    "\\log \\delta z \\text{Pr}(z) = \\log \\delta z + \\log \\text{Pr}(z),\n",
    "\\end{equation}\n",
    "\n",
    "this means we can just ignore the constant $\\log \\delta z$ and use $\\log\\text{Pr}(z)$ as our measure of message length, as we did above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding view of MDL for model selection says that we should choose the model with highest posterior probability. However, many Bayesian would instead do inference by sampling from the posterior distribution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
