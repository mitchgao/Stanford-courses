{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 7.7. The Bayesian Approach and $\\text{BIC}$\n",
    "\n",
    "The Bayesian information criterion ($\\text{BIC}$), like $\\text{AIC}$, is applicable in settings where the fitting is carried out by maximization of a log-likelihood.\n",
    "\n",
    "The generic form of $\\text{BIC}$ is\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{BIC} = -2\\cdot\\text{loglik} + (\\log N)\\cdot d.\n",
    "\\end{equation}\n",
    "\n",
    "The $\\text{BIC}$ statistic (times $1/2$) is a.k.a. the Schwarz criterion (Schwarz, 1978)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\text{BIC}$ and $\\text{AIC}$\n",
    "\n",
    "Under the Gaussian model, assuming the variance $\\sigma_\\epsilon^2$ is known, $-2\\cdot\\text{loglik}$ equals (up to a constant)\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\sum_i (y_i - \\hat{f}(x_i))^2}{\\sigma_\\epsilon^2},\n",
    "\\end{equation}\n",
    "\n",
    "which is\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{N\\cdot\\overline{\\text{err}}}{\\sigma_\\epsilon^2}\n",
    "\\end{equation}\n",
    "\n",
    "for squared error loss.\n",
    "\n",
    "Hence we can write\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{BIC} = \\frac{N}{\\sigma_\\epsilon^2} \\left[ \\overline{\\text{err}} + \\frac{d\\sigma_\\epsilon^2}{N}\\log N \\right].\n",
    "\\end{equation}\n",
    "\n",
    "Therefore $\\text{BIC}$ is proportional to $\\text{AIC}$ ($C_p$), with the factor $2$ replaced by $\\log N$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prefer simpler\n",
    "\n",
    "Assuming $N > e^2 \\approx 7.4$, $\\text{BIC}$ tends to penalize complex models more heavily, giving preference to simpler models in selection. As with $\\text{AIC}$, $\\sigma_\\epsilon^2$ is typically estimated by the MSE of a low-bias model.\n",
    "\n",
    "For classification problems, use of the multinomial log-likelihood leads to a similar relationship with the $\\text{AIC}$, using cross-entropy as the error measure.\n",
    "\n",
    "Note however that the misclassification error measure does not arise in the $\\text{BIC}$ context, since it does not correspond to the log-likelihood of the data under any probability model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian motivation\n",
    "\n",
    "> Therefore, choosing the model with minimum $\\text{BIC}$ is equivalent to choosing the model with largest (approximate) posterior probability.\n",
    "\n",
    "Despite its similarity with $\\text{AIC}$, $\\text{BIC}$ is motivated in quite a different way. It arises in the Bayesian approach to model selection.\n",
    "\n",
    "Suppose\n",
    "* we have a set of candidate models $\\mathcal{M}_m$, $m=1,\\cdots,M$ and\n",
    "* corresponding model parameters $\\theta_m$, and\n",
    "* we wish to choose a best model from among them.\n",
    "* Also we have a prior distribution $\\text{Pr}(\\theta_m|\\mathcal{M}_m)$ for the parameters of each model $\\mathcal{M}_m$,\n",
    "\n",
    "Then the posterior probability of a given model is\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Pr}(\\mathcal{M}_m | \\mathbf{Z}) &\\propto \\text{Pr}(\\mathcal{M}_m) \\cdot \\text{Pr}(\\mathbf{Z} | \\mathcal{M}_m) \\\\\n",
    "&\\propto \\text{Pr}(\\mathcal{M}_m) \\cdot \\int \\text{Pr} (\\mathbf{Z} | \\theta_m,\\mathcal{M}_m) \\text{Pr}(\\theta_m | \\mathcal{M}_m) d\\theta_m,\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{Z} = \\{x_i,y_i\\}_1^N$ represents the training data.\n",
    "\n",
    "To compare two models $\\mathcal{M}_m$ and $\\mathcal{M}_l$, we form the posterior odds\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\text{Pr}(\\mathcal{M}_m | \\mathbf{Z})}{\\text{Pr}(\\mathcal{M}_l | \\mathbf{Z})} = \\frac{\\text{Pr}(\\mathcal{M}_m)}{\\text{Pr}(\\mathcal{M}_l)} \\cdot \\frac{\\text{Pr}( \\mathbf{Z}|\\mathcal{M}_m)}{\\text{Pr}(\\mathbf{Z}|\\mathcal{M}_l)} = \\frac{\\text{Pr}(\\mathcal{M}_m)}{\\text{Pr}(\\mathcal{M}_l)} \\cdot \\text{BF}(\\mathbf{Z}).\n",
    "\\end{equation}\n",
    "\n",
    "If the odds are greater than one we choose model $m$, otherwise we choose model $l$.\n",
    "\n",
    "The rightmost quantity $\\text{BF}(\\mathbf{Z})$ is called the _Bayes factor_, the contribution of the data toward the posterior odds.\n",
    "\n",
    "Typically we assume that the prior over models is uniform, so that $\\text{Pr}(\\mathcal{M}_m)$ is constant.\n",
    "\n",
    "We need some way of approximating $\\text{Pr}(\\mathbf{Z}|\\mathcal{M}_m)$. A so-called Laplace approximation to the integral followed by some other simplification (Riple, 1996, page 64) to the above integral gives\n",
    "\n",
    "\\begin{equation}\n",
    "\\log \\text{Pr}(\\mathbf{Z}|\\mathcal{M}_m) = \\log \\text{Pr}(\\mathbf{Z}|\\hat\\theta_m,\\mathcal{M}_m) - \\frac{d_m}{2} \\log N + O(1).\n",
    "\\end{equation}\n",
    "\n",
    "Here $\\hat\\theta_m$ is a MLE and $d_m$ is the number of free parameters in model $m$.\n",
    "\n",
    "If we define our loss function to be $-2\\log \\text{Pr}(\\mathbf{Z}|\\hat\\theta_m,\\mathcal{M})$, this is equivalent to the $\\text{BIC}$ criterion specified at the top.\n",
    "\n",
    "> Therefore, choosing the model with minimum $\\text{BIC}$ is equivalent to choosing the model with largest (approximate) posterior probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus\n",
    "\n",
    "This framework gives us more.\n",
    "\n",
    "If we compute the $\\text{BIC}$ criterion for a set of $M$ models, giving $\\text{BIC}_m$, then we can estimate the posterior probability of each model $\\mathcal{M}_m$ as\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\exp\\left( -\\frac{1}{2} \\text{BIC}_m \\right)}{\\sum_{l=1}^m \\exp\\left( -\\frac{1}{2} \\text{BIC}_l \\right)}.\n",
    "\\end{equation}\n",
    "\n",
    "Thus we can estimate not only the best model, but also assess the relative merits of the models considered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meaning and comparison\n",
    "\n",
    "For model selection purposes, there is no clear choice between $\\text{AIC}$ and $\\text{BIC}$.\n",
    "\n",
    "$\\text{BIC}$ is asymptotically consistent as a selection criterion. What this means is that given a family of models, including the true model, the probabilty that $\\text{BIC}$ will select the correct model approaches one as the sample size $N \\rightarrow \\infty$.\n",
    "\n",
    "This is not the case for $\\text{AIC}$, which tends to choose models which are too complex as $N \\rightarrow \\infty$. On the other hand, for finite samples, $\\text{BIC}$ often chooses models that are too simple, because of its heavy penalty on complexity."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
