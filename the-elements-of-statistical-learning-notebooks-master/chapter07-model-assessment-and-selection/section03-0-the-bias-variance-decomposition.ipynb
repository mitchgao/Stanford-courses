{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $\\S$ 7.3. The Bias-Variance Decomposition\n",
    "\n",
    "As in Chapter 2, if we assume that\n",
    "\n",
    "\\begin{equation}\n",
    "Y = f(X) + \\epsilon,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\text{E}(\\epsilon) = 0$ and $\\text{Var}(\\epsilon) = \\sigma_\\epsilon^2$, we can derive an expression for the expected prediction error of a regression fit $\\hat{f}(X)$ at an input point $X=x_0$, using squared-error loss:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Err}(x_0) &= \\text{E}\\left[ \\left( Y - \\hat{f}(x_0) \\right)^2 \\mid X=x_0 \\right] \\\\\n",
    "&= \\sigma_\\epsilon^2 + \\left( \\text{E}\\hat{f}(x_0) - f(x_0) \\right)^2 + \\text{E}\\left[ \\hat{f}(x_0) - \\text{E}\\hat{f}(x_0) \\right]^2\\\\\n",
    "&= \\sigma_\\epsilon^2 + \\text{Bias}^2\\left(\\hat{f}(x_0)\\right) + \\text{Var}\\left(\\hat{f}(x_0)\\right) \\\\\n",
    "&= \\text{Irreducible Error} + \\text{Bias}^2 + \\text{Variance}.\n",
    "\\end{align}\n",
    "\n",
    "* The first term is the variance of the target around its true mean $f(x_0)$, and cannot be avoided no matter how well we estimate $f(x_0)$, unless $\\sigma_\\epsilon^2 = 0$.\n",
    "* The second term is the squared bias, the amount by which the average of our estimate differs from the true mean.\n",
    "* The last term is the variance, the expected squared deviation of $\\hat{f}(x_0)$ around its mean.\n",
    "\n",
    "Typically the more complex we make the model $\\hat{f}$, the lower the (squared) bias but the higher the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 1: kNN fit\n",
    "\n",
    "For the kNN regression fit, these expressions have the simple form\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Err}(x_0) &= \\text{E}\\left[ \\left( Y - \\hat{f}_k(x_0) \\right)^2 \\mid X=x_0 \\right] \\\\\n",
    "&= \\sigma_\\epsilon^2 + \\left( f(x_0) - \\frac1k \\sum_{l=k}^k f(x_{(l)}) \\right)^2 + \\frac{\\sigma_\\epsilon^2}k.\n",
    "\\end{align}\n",
    "\n",
    "Here we assume for simplicity that training inputs $x_i$ are fixed, and the randomness arises from the $y_i$.\n",
    "\n",
    "The number of neighborhoods $k$ is inversely related to the model complexity: For small $k$, the estimate $\\hat{f}(x)$ can potentially adapt itself better to the underlying $f(x)$. As we increase $k$, the bias -- the squared difference between $f(x_0)$ and the average of $f(x)$ at the kNN -- will typically increase, while the variance decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: linear model fit\n",
    "\n",
    "For a linear model fit\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}_p(x) = x^T \\hat\\beta,\n",
    "\\end{equation}\n",
    "\n",
    "where the parameter vector $\\beta$ with $p$ components is fit by least squares, we have\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Err}(x_0) &= \\text{E}\\left[ \\left( Y - \\hat{f}_p(x_0) \\right)^2 \\mid X=x_0 \\right] \\\\\n",
    "&= \\sigma_\\epsilon^2 + \\left( f(x_0) - \\text{E}\\hat{f}_p(x_0) \\right)^2 + \\|\\mathbf{h}(x_0)\\|^2 \\sigma_\\epsilon^2.\n",
    "\\end{align}\n",
    "\n",
    "Here $\\mathbf{h}(x_0) = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} x_0$, the $N$-vector of linear weights that produce the fit\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{f}_p(x_0) = \\mathbf{h}(x_0)^T \\mathbf{y} = x_0^T(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y},\n",
    "\\end{equation}\n",
    "\n",
    "and hence\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac1N \\sum_{i=1}^N \\text{Err}(x_i) = \\sigma_\\epsilon^2 + \\frac1N \\sum_{i=1}^N \\left( f(x_i) - \\text{E}\\hat{f}(x_i) \\right)^2 + \\frac{p}N \\sigma_\\epsilon^2,\n",
    "\\end{equation}\n",
    "\n",
    "the _in-sample_ error. Here model complexity is directly related to the number of parameters $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2-1: Ridge regression fit\n",
    "\n",
    "The test error for a ridge regression fit $\\hat{f}_\\alpha(x)$ is identical in form to the one of linear regression fit\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Err}(x_0) &= \\text{E}\\left[ \\left( Y - \\hat{f}_\\alpha(x_0) \\right)^2 \\mid X=x_0 \\right] \\\\\n",
    "&= \\sigma_\\epsilon^2 + \\left( f(x_0) - \\text{E}\\hat{f}_\\alpha(x_0) \\right)^2 + \\|\\mathbf{h}(x_0)\\|^2 \\sigma_\\epsilon^2,\n",
    "\\end{align}\n",
    "\n",
    "except the linear weights in the variance term are different:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{h}(x_0) = \\mathbf{X}(\\mathbf{X}^T\\mathbf{X} + \\alpha\\mathbf{I})^{-1} x_0.\n",
    "\\end{equation}\n",
    "\n",
    "The bias term will also be different.\n",
    "\n",
    "For a linear model family such as ridge regression, we can break down the bias more finely.\n",
    "\n",
    "Let $\\beta_*$ denote the parameters of the best-fitting linear approximation to $f$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta_* = \\arg\\min_\\beta \\text{E}\\left[ f(X) - X^T\\beta \\right]^2,\n",
    "\\end{equation}\n",
    "\n",
    "where the expectation is taken w.r.t. the distribution of the input variable $X$. Then we can write the average squared bias as\n",
    "\n",
    "\\begin{align}\n",
    "\\text{E}_{x_0} \\left[ f(x_0) - \\text{E}\\hat{f}_\\alpha(x_0) \\right]^2 &= \\text{E}_{x_0} \\left[ f(x_0) - x_0^T\\beta_* \\right]^2 + \\text{E}_{x_0} \\left[ x_0^T\\beta_* - \\text{E} x_0^T\\hat\\beta_\\alpha \\right]^2 \\\\\n",
    "&= \\text{Ave}\\left[\\text{Model Bias}\\right]^2 + \\text{Ave}\\left[\\text{Estimation Bias}\\right]^2.\n",
    "\\end{align}\n",
    "\n",
    "* The first term is the average squared _model bias_, the error between the best-fitting linear approximation and the true function.\n",
    "* The second  term is the average squared _estimation bias_, the error between the average estimate $\\text{E}(x_0^T\\hat\\beta)$ and the best-fitting linear approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Linear model fit, more on bias\n",
    "\n",
    "* For linear models fit by ordinary least squares, the estimation bias is zero.\n",
    "* For restricted fits, such as ridge regression, it is positive, and we trade it off with the benefits of a reduced variance.\n",
    "* The model bias can only be reduced by enlarging the class of linear models to a richer collection of models, by including interactions and transformations of the variables in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review with schematic figure\n",
    "\n",
    "FIGURE 7.2 shows the bias-variance tradeoff schematically.\n",
    "\n",
    "![The model space is the set of all possible predictions from the model, with the \"closest\" fit labeled with a black dot. The model bias from the truth is shown, along with the  variance, indicated by the large yellow circle centered at the black dot labeled \"closest fit in population\". A shrunken or regularized fit is also shown, having additional estimation bias, but smaller prediction error due to its decreased variance.](./fig7-2.jpg)\n",
    "\n",
    "In the case of linear models,\n",
    "* the model space is the set of all linear predictions from $p$ inputs and\n",
    "* the black dot labeled \"closest fit\" is $x^T\\beta_*$.\n",
    "* The blue-shaded region indicates the error $\\sigma_\\epsilon$ with which we see the truth in the training sample.\n",
    "\n",
    "Also shown is the variance of the least squares fit, indicated by the large yellow circle centered at the black dot labeled \"closest fit in population\".\n",
    "\n",
    "Now if we were to fit a model with fewer predictors, or regularize the coefficients by shrinking them toward zero (say), we would get the \"shrunken fit\" shown in the figure. This fit has an additional estimation bias, due to the fact that it is not the closest fit in the model space. On the other hand, it has smaller variance.\n",
    "\n",
    "If the decrease in variance exceeds the increase in (squared) bias, then this is worthwhile."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
